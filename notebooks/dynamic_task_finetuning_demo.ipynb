{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continual Task Finetuning Demo\n",
    "\n",
    "This notebook demonstrates how to pre-train the flexible multi-task foundation model on non-PI polymers, then load the best checkpoint, freeze the shared encoder, and fine-tune newly added tasks for PI polymers using the dynamic task management utilities (`add_task` / `remove_tasks`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "- **Descriptors**: `data/amorphous_polymer_FFDescriptor_20250730.parquet`\n",
    "- **Non-PI properties**: `data/amorphous_polymer_non_PI_properties_20250730.parquet`\n",
    "- **PI properties**: `data/amorphous_polymer_PI_properties_20250730.parquet`\n",
    "- Target regression labels (normalized): density, Cp, Rg, linear_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-30 05:38:05.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__init__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mLoguru logger initialized for foundation_model package.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from foundation_model.data.datamodule import CompoundDataModule\n",
    "from foundation_model.models.flexible_multi_task_model import FlexibleMultiTaskModel\n",
    "from foundation_model.models.model_config import RegressionTaskConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "DESCRIPTOR_PATH = DATA_DIR / \"amorphous_polymer_FFDescriptor_20250730.parquet\"\n",
    "NON_PI_PATH = DATA_DIR / \"amorphous_polymer_non_PI_properties_20250730.parquet\"\n",
    "PI_PATH = DATA_DIR / \"amorphous_polymer_PI_properties_20250730.parquet\"\n",
    "\n",
    "TARGET_COLUMNS = {\n",
    "    \"density\": \"density (normalized)\",\n",
    "    \"Cp\": \"Cp (normalized)\",\n",
    "    \"Rg\": \"Rg (normalized)\",\n",
    "    \"linear_expansion\": \"linear_expansion (normalized)\",\n",
    "}\n",
    "\n",
    "SHARED_BLOCK_DIMS = [190, 256, 128]\n",
    "HEAD_HIDDEN = 64  # hidden width for regression heads\n",
    "ARTIFACT_ROOT = Path(\"artifacts/polymers_dynamic_tasks\")\n",
    "ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PRETRAIN_SAMPLE = 10_000  # subset for quick demonstration\n",
    "PI_SAMPLE = None  # use full PI set by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train feature tensor: (10000, 190)\n",
      "Pre-train targets: (10000, 4)\n",
      "Fine-tune feature tensor: (1083, 190)\n",
      "Fine-tune targets: (1083, 4)\n"
     ]
    }
   ],
   "source": [
    "descriptor_df = pd.read_parquet(DESCRIPTOR_PATH)\n",
    "non_pi_df = pd.read_parquet(NON_PI_PATH)\n",
    "pi_df = pd.read_parquet(PI_PATH)\n",
    "\n",
    "common_non_pi = descriptor_df.index.intersection(non_pi_df.index)\n",
    "pretrain_features = descriptor_df.loc[common_non_pi]\n",
    "pretrain_targets = non_pi_df.loc[common_non_pi, list(TARGET_COLUMNS.values())]\n",
    "\n",
    "if PRETRAIN_SAMPLE is not None and PRETRAIN_SAMPLE < len(pretrain_features):\n",
    "    pretrain_features = pretrain_features.sample(n=PRETRAIN_SAMPLE, random_state=42)\n",
    "    pretrain_targets = pretrain_targets.loc[pretrain_features.index]\n",
    "\n",
    "common_pi = descriptor_df.index.intersection(pi_df.index)\n",
    "pi_features = descriptor_df.loc[common_pi]\n",
    "pi_targets = pi_df.loc[common_pi, list(TARGET_COLUMNS.values())]\n",
    "\n",
    "if PI_SAMPLE is not None and PI_SAMPLE < len(pi_features):\n",
    "    pi_features = pi_features.sample(n=PI_SAMPLE, random_state=13)\n",
    "    pi_targets = pi_targets.loc[pi_features.index]\n",
    "\n",
    "print(f\"Pre-train feature tensor: {pretrain_features.shape}\")\n",
    "print(f\"Pre-train targets: {pretrain_targets.shape}\")\n",
    "print(f\"Fine-tune feature tensor: {pi_features.shape}\")\n",
    "print(f\"Fine-tune targets: {pi_targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Task Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain tasks: ['density', 'Cp', 'Rg', 'linear_expansion']\n",
      "PI tasks: ['density_pi', 'Cp_pi', 'Rg_pi', 'linear_expansion_pi']\n"
     ]
    }
   ],
   "source": [
    "def build_regression_task(name: str, column: str) -> RegressionTaskConfig:\n",
    "    return RegressionTaskConfig(\n",
    "        name=name,\n",
    "        data_column=column,\n",
    "        dims=[SHARED_BLOCK_DIMS[-1], HEAD_HIDDEN, 1],\n",
    "        norm=True,\n",
    "        residual=False,\n",
    "    )\n",
    "\n",
    "pretrain_task_configs = [build_regression_task(name, col) for name, col in TARGET_COLUMNS.items()]\n",
    "pi_task_configs = [build_regression_task(f\"{name}_pi\", col) for name, col in TARGET_COLUMNS.items()]\n",
    "\n",
    "print(\"Pretrain tasks:\", [cfg.name for cfg in pretrain_task_configs])\n",
    "print(\"PI tasks:\", [cfg.name for cfg in pi_task_configs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 — Pre-train on non-PI polymers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_datamodule = CompoundDataModule(\n",
    "    formula_desc_source=pretrain_features,\n",
    "    attributes_source=pretrain_targets,\n",
    "    task_configs=pretrain_task_configs,\n",
    "    batch_size=256,\n",
    "    num_workers=0,\n",
    "    # val_split=0.1,\n",
    "    # test_split=0.1,\n",
    ")\n",
    "\n",
    "pretrain_model = FlexibleMultiTaskModel(\n",
    "    shared_block_dims=SHARED_BLOCK_DIMS,\n",
    "    task_configs=pretrain_task_configs,\n",
    "    enable_learnable_loss_balancer=True,\n",
    ")\n",
    "\n",
    "pretrain_checkpoint_dir = ARTIFACT_ROOT / \"pretrain_checkpoints\"\n",
    "pretrain_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "pretrain_ckpt = ModelCheckpoint(\n",
    "    dirpath=pretrain_checkpoint_dir,\n",
    "    filename=\"pretrain-{epoch:02d}-{val_final_loss:.4f}\",\n",
    "    monitor=\"val_final_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "pretrain_logger = CSVLogger(save_dir=ARTIFACT_ROOT / \"logs\", name=\"pretrain\")\n",
    "\n",
    "pretrain_trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    callbacks=[pretrain_ckpt],\n",
    "    logger=pretrain_logger,\n",
    "    log_every_n_steps=10,\n",
    "    limit_train_batches=0.2,\n",
    "    limit_val_batches=0.5,\n",
    ")\n",
    "\n",
    "pretrain_trainer.fit(pretrain_model, datamodule=pretrain_datamodule)\n",
    "print(f\"Best checkpoint: {pretrain_ckpt.best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 — Fine-tune newly added PI tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_datamodule = CompoundDataModule(\n",
    "    formula_desc_source=pi_features,\n",
    "    attributes_source=pi_targets,\n",
    "    task_configs=pi_task_configs,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    val_split=0.2,\n",
    "    test_split=0.0,\n",
    ")\n",
    "\n",
    "finetune_model = FlexibleMultiTaskModel(\n",
    "    shared_block_dims=SHARED_BLOCK_DIMS,\n",
    "    task_configs=pretrain_task_configs,\n",
    "    enable_learnable_loss_balancer=True,\n",
    "    strict_loading=False,\n",
    ")\n",
    "\n",
    "best_ckpt_path = pretrain_ckpt.best_model_path\n",
    "if not best_ckpt_path:\n",
    "    raise RuntimeError(\"Pre-training did not produce a checkpoint. Check earlier cells for errors.\")\n",
    "\n",
    "state = torch.load(best_ckpt_path, map_location=\"cpu\")\n",
    "finetune_model.load_state_dict(state[\"state_dict\"], strict=False)\n",
    "\n",
    "for param in finetune_model.encoder.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "finetune_model.remove_tasks(*TARGET_COLUMNS.keys())\n",
    "finetune_model.add_task(*pi_task_configs)\n",
    "\n",
    "print(\"Trainable task heads:\", list(finetune_model.task_heads.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_checkpoint_dir = ARTIFACT_ROOT / \"finetune_checkpoints\"\n",
    "finetune_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "finetune_ckpt = ModelCheckpoint(\n",
    "    dirpath=finetune_checkpoint_dir,\n",
    "    filename=\"finetune-{epoch:02d}-{val_final_loss:.4f}\",\n",
    "    monitor=\"val_final_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "finetune_logger = CSVLogger(save_dir=ARTIFACT_ROOT / \"logs\", name=\"finetune\")\n",
    "\n",
    "finetune_trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"cpu\",\n",
    "    devices=1,\n",
    "    callbacks=[finetune_ckpt],\n",
    "    logger=finetune_logger,\n",
    "    log_every_n_steps=5,\n",
    "    limit_train_batches=1.0,\n",
    "    limit_val_batches=1.0,\n",
    ")\n",
    "\n",
    "finetune_trainer.fit(finetune_model, datamodule=pi_datamodule)\n",
    "print(f\"Best fine-tuning checkpoint: {finetune_ckpt.best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect fine-tuned predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_datamodule.setup(stage=\"validate\")\n",
    "val_loader = pi_datamodule.val_dataloader()\n",
    "example_batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = finetune_model(example_batch[0], example_batch[3])\n",
    "\n",
    "for name, tensor in outputs.items():\n",
    "    print(name, tensor[:5].squeeze().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
