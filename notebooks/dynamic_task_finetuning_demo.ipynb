{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Continual Task Finetuning Demo\n",
        "\n",
        "This notebook demonstrates how to pre-train the flexible multi-task foundation model on non-PI polymers, then load the best checkpoint, freeze the shared encoder, and fine-tune newly added tasks for PI polymers using the dynamic task management utilities (`add_task` / `remove_tasks`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Overview\n",
        "\n",
        "- **Descriptors**: `data/amorphous_polymer_FFDescriptor_20250730.parquet`\n",
        "- **Non-PI properties**: `data/amorphous_polymer_non_PI_properties_20250730.parquet`\n",
        "- **PI properties**: `data/amorphous_polymer_PI_properties_20250730.parquet`\n",
        "- Target regression labels (normalized): density, Cp, Rg, linear_expansion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "\n",
        "from foundation_model.data.datamodule import CompoundDataModule\n",
        "from foundation_model.models.flexible_multi_task_model import FlexibleMultiTaskModel\n",
        "from foundation_model.models.model_config import RegressionTaskConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = Path(\"data\")\n",
        "DESCRIPTOR_PATH = DATA_DIR / \"amorphous_polymer_FFDescriptor_20250730.parquet\"\n",
        "NON_PI_PATH = DATA_DIR / \"amorphous_polymer_non_PI_properties_20250730.parquet\"\n",
        "PI_PATH = DATA_DIR / \"amorphous_polymer_PI_properties_20250730.parquet\"\n",
        "\n",
        "TARGET_COLUMNS = {\n",
        "    \"density\": \"density (normalized)\",\n",
        "    \"Cp\": \"Cp (normalized)\",\n",
        "    \"Rg\": \"Rg (normalized)\",\n",
        "    \"linear_expansion\": \"linear_expansion (normalized)\",\n",
        "}\n",
        "\n",
        "SHARED_BLOCK_DIMS = [190, 256, 128]\n",
        "HEAD_HIDDEN = 64  # hidden width for regression heads\n",
        "ARTIFACT_ROOT = Path(\"notebooks/artifacts/polymers_dynamic_tasks\")\n",
        "ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "PRETRAIN_SAMPLE = 6000  # subset for quick demonstration\n",
        "PI_SAMPLE = None  # use full PI set by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "descriptor_df = pd.read_parquet(DESCRIPTOR_PATH)\n",
        "non_pi_df = pd.read_parquet(NON_PI_PATH)\n",
        "pi_df = pd.read_parquet(PI_PATH)\n",
        "\n",
        "common_non_pi = descriptor_df.index.intersection(non_pi_df.index)\n",
        "pretrain_features = descriptor_df.loc[common_non_pi]\n",
        "pretrain_targets = non_pi_df.loc[common_non_pi, list(TARGET_COLUMNS.values())]\n",
        "\n",
        "if PRETRAIN_SAMPLE is not None and PRETRAIN_SAMPLE < len(pretrain_features):\n",
        "    pretrain_features = pretrain_features.sample(n=PRETRAIN_SAMPLE, random_state=42)\n",
        "    pretrain_targets = pretrain_targets.loc[pretrain_features.index]\n",
        "\n",
        "common_pi = descriptor_df.index.intersection(pi_df.index)\n",
        "pi_features = descriptor_df.loc[common_pi]\n",
        "pi_targets = pi_df.loc[common_pi, list(TARGET_COLUMNS.values())]\n",
        "\n",
        "if PI_SAMPLE is not None and PI_SAMPLE < len(pi_features):\n",
        "    pi_features = pi_features.sample(n=PI_SAMPLE, random_state=13)\n",
        "    pi_targets = pi_targets.loc[pi_features.index]\n",
        "\n",
        "print(f\"Pre-train feature tensor: {pretrain_features.shape}\")\n",
        "print(f\"Pre-train targets: {pretrain_targets.shape}\")\n",
        "print(f\"Fine-tune feature tensor: {pi_features.shape}\")\n",
        "print(f\"Fine-tune targets: {pi_targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Task Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_regression_task(name: str, column: str) -> RegressionTaskConfig:\n",
        "    return RegressionTaskConfig(\n",
        "        name=name,\n",
        "        data_column=column,\n",
        "        dims=[SHARED_BLOCK_DIMS[-1], HEAD_HIDDEN, 1],\n",
        "        norm=True,\n",
        "        residual=False,\n",
        "    )\n",
        "\n",
        "pretrain_task_configs = [build_regression_task(name, col) for name, col in TARGET_COLUMNS.items()]\n",
        "pi_task_configs = [build_regression_task(f\"{name}_pi\", col) for name, col in TARGET_COLUMNS.items()]\n",
        "\n",
        "print(\"Pretrain tasks:\", [cfg.name for cfg in pretrain_task_configs])\n",
        "print(\"PI tasks:\", [cfg.name for cfg in pi_task_configs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 1 \u2014 Pre-train on non-PI polymers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrain_datamodule = CompoundDataModule(\n",
        "    formula_desc_source=pretrain_features,\n",
        "    attributes_source=pretrain_targets,\n",
        "    task_configs=pretrain_task_configs,\n",
        "    batch_size=256,\n",
        "    num_workers=0,\n",
        "    val_split=0.1,\n",
        "    test_split=0.1,\n",
        ")\n",
        "\n",
        "pretrain_model = FlexibleMultiTaskModel(\n",
        "    shared_block_dims=SHARED_BLOCK_DIMS,\n",
        "    task_configs=pretrain_task_configs,\n",
        "    enable_learnable_loss_balancer=True,\n",
        ")\n",
        "\n",
        "pretrain_checkpoint_dir = ARTIFACT_ROOT / \"pretrain_checkpoints\"\n",
        "pretrain_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "pretrain_ckpt = ModelCheckpoint(\n",
        "    dirpath=pretrain_checkpoint_dir,\n",
        "    filename=\"pretrain-{epoch:02d}-{val_final_loss:.4f}\",\n",
        "    monitor=\"val_final_loss\",\n",
        "    mode=\"min\",\n",
        "    save_top_k=1,\n",
        ")\n",
        "\n",
        "pretrain_logger = CSVLogger(save_dir=ARTIFACT_ROOT / \"logs\", name=\"pretrain\")\n",
        "\n",
        "pretrain_trainer = Trainer(\n",
        "    max_epochs=3,\n",
        "    accelerator=\"cpu\",\n",
        "    devices=1,\n",
        "    callbacks=[pretrain_ckpt],\n",
        "    logger=pretrain_logger,\n",
        "    log_every_n_steps=10,\n",
        "    limit_train_batches=0.2,\n",
        "    limit_val_batches=0.5,\n",
        ")\n",
        "\n",
        "pretrain_trainer.fit(pretrain_model, datamodule=pretrain_datamodule)\n",
        "print(f\"Best checkpoint: {pretrain_ckpt.best_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stage 2 \u2014 Fine-tune newly added PI tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pi_datamodule = CompoundDataModule(\n",
        "    formula_desc_source=pi_features,\n",
        "    attributes_source=pi_targets,\n",
        "    task_configs=pi_task_configs,\n",
        "    batch_size=64,\n",
        "    num_workers=0,\n",
        "    val_split=0.2,\n",
        "    test_split=0.0,\n",
        ")\n",
        "\n",
        "finetune_model = FlexibleMultiTaskModel(\n",
        "    shared_block_dims=SHARED_BLOCK_DIMS,\n",
        "    task_configs=pretrain_task_configs,\n",
        "    enable_learnable_loss_balancer=True,\n",
        "    strict_loading=False,\n",
        ")\n",
        "\n",
        "best_ckpt_path = pretrain_ckpt.best_model_path\n",
        "if not best_ckpt_path:\n",
        "    raise RuntimeError(\"Pre-training did not produce a checkpoint. Check earlier cells for errors.\")\n",
        "\n",
        "state = torch.load(best_ckpt_path, map_location=\"cpu\")\n",
        "finetune_model.load_state_dict(state[\"state_dict\"], strict=False)\n",
        "\n",
        "for param in finetune_model.encoder.parameters():\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "finetune_model.remove_tasks(*TARGET_COLUMNS.keys())\n",
        "for cfg in pi_task_configs:\n",
        "    finetune_model.add_task(cfg)\n",
        "\n",
        "print(\"Trainable task heads:\", list(finetune_model.task_heads.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "finetune_checkpoint_dir = ARTIFACT_ROOT / \"finetune_checkpoints\"\n",
        "finetune_checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "finetune_ckpt = ModelCheckpoint(\n",
        "    dirpath=finetune_checkpoint_dir,\n",
        "    filename=\"finetune-{epoch:02d}-{val_final_loss:.4f}\",\n",
        "    monitor=\"val_final_loss\",\n",
        "    mode=\"min\",\n",
        "    save_top_k=1,\n",
        ")\n",
        "\n",
        "finetune_logger = CSVLogger(save_dir=ARTIFACT_ROOT / \"logs\", name=\"finetune\")\n",
        "\n",
        "finetune_trainer = Trainer(\n",
        "    max_epochs=5,\n",
        "    accelerator=\"cpu\",\n",
        "    devices=1,\n",
        "    callbacks=[finetune_ckpt],\n",
        "    logger=finetune_logger,\n",
        "    log_every_n_steps=5,\n",
        "    limit_train_batches=1.0,\n",
        "    limit_val_batches=1.0,\n",
        ")\n",
        "\n",
        "finetune_trainer.fit(finetune_model, datamodule=pi_datamodule)\n",
        "print(f\"Best fine-tuning checkpoint: {finetune_ckpt.best_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect fine-tuned predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pi_datamodule.setup(stage=\"validate\")\n",
        "val_loader = pi_datamodule.val_dataloader()\n",
        "example_batch = next(iter(val_loader))\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = finetune_model(example_batch[0], example_batch[3])\n",
        "\n",
        "for name, tensor in outputs.items():\n",
        "    print(name, tensor[:5].squeeze().cpu().numpy())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}