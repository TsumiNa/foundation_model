{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Interactive Model Test and Debugging\n",
                "\n",
                "This notebook is designed to help test and debug the `FlexibleMultiTaskModel` and `CompoundDataModule` step by step. It loads configurations, initializes the data module and model, fetches a batch of data, and manually walks through the core logic of the `training_step`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-14 04:43:49,056 - INFO - Successfully imported project modules.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import pandas as pd\n",
                "from omegaconf import OmegaConf\n",
                "import yaml  # For loading the raw config if needed\n",
                "import pprint\n",
                "import logging\n",
                "\n",
                "# Configure basic logging for the notebook\n",
                "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "# Add project root to sys.path to allow imports from src\n",
                "# Assumes the notebook is in a subdirectory of the project root (e.g., 'notebooks/')\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
                "# if project_root not in sys.path:\n",
                "#     sys.path.insert(0, project_root)\n",
                "#     logger.info(f\"Added {project_root} to sys.path\")\n",
                "\n",
                "try:\n",
                "    from foundation_model.models.flexible_multi_task_model import FlexibleMultiTaskModel\n",
                "    from foundation_model.data.datamodule import CompoundDataModule\n",
                "    from foundation_model.configs.model_config import TaskType  # For potential inspection\n",
                "\n",
                "    logger.info(\"Successfully imported project modules.\")\n",
                "except ImportError as e:\n",
                "    logger.error(\n",
                "        f\"Error importing project modules: {e}. Ensure PYTHONPATH is set correctly or notebook is in the correct location.\"\n",
                "    )\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Configuration\n",
                "\n",
                "Load the model and data configurations from `samples/generated_configs/generated_model_config.yaml`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-14 04:51:03,812 - INFO - Loading configuration from: /data/foundation_model/samples/generated_configs/generated_model_config.yaml\n",
                        "2025-05-14 04:51:03,849 - INFO - Configuration loaded successfully.\n",
                        "2025-05-14 04:51:03,850 - INFO - Model Configuration:\n",
                        "2025-05-14 04:51:03,857 - INFO - Data Configuration:\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{ 'class_path': 'foundation_model.models.FlexibleMultiTaskModel',\n",
                        "  'init_args': { 'enable_self_supervised_training': False,\n",
                        "                 'loss_weights': { 'contrastive': 1.0,\n",
                        "                                   'cross_recon': 1.0,\n",
                        "                                   'mfm': 1.0},\n",
                        "                 'mask_ratio': 0.15,\n",
                        "                 'modality_dropout_p': 0.3,\n",
                        "                 'norm_shared': True,\n",
                        "                 'residual_shared': False,\n",
                        "                 'shared_block_dims': [256, 128, 64],\n",
                        "                 'shared_block_optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                             'eps': 1e-06,\n",
                        "                                             'factor': 0.1,\n",
                        "                                             'freeze_parameters': False,\n",
                        "                                             'lr': 0.001,\n",
                        "                                             'min_lr': 1e-06,\n",
                        "                                             'mode': 'min',\n",
                        "                                             'monitor': 'val_total_loss',\n",
                        "                                             'optimizer_type': 'AdamW',\n",
                        "                                             'patience': 20,\n",
                        "                                             'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                             'weight_decay': 0.01},\n",
                        "                 'task_configs': [ { 'dims': [64, 32, 1],\n",
                        "                                     'enabled': True,\n",
                        "                                     'name': 'regression_1',\n",
                        "                                     'norm': True,\n",
                        "                                     'optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                                    'eps': 1e-06,\n",
                        "                                                    'factor': 0.5,\n",
                        "                                                    'lr': 0.001,\n",
                        "                                                    'min_lr': 1e-05,\n",
                        "                                                    'mode': 'min',\n",
                        "                                                    'monitor': 'val_regression_1_loss',\n",
                        "                                                    'optimizer_type': 'AdamW',\n",
                        "                                                    'patience': 10,\n",
                        "                                                    'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                                    'weight_decay': 0.01},\n",
                        "                                     'residual': False,\n",
                        "                                     'type': 'REGRESSION',\n",
                        "                                     'weight': 1.0},\n",
                        "                                   { 'dims': [64, 32, 1],\n",
                        "                                     'enabled': True,\n",
                        "                                     'name': 'regression_2',\n",
                        "                                     'norm': True,\n",
                        "                                     'optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                                    'eps': 1e-06,\n",
                        "                                                    'factor': 0.5,\n",
                        "                                                    'lr': 0.001,\n",
                        "                                                    'min_lr': 1e-05,\n",
                        "                                                    'mode': 'min',\n",
                        "                                                    'monitor': 'val_regression_2_loss',\n",
                        "                                                    'optimizer_type': 'AdamW',\n",
                        "                                                    'patience': 10,\n",
                        "                                                    'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                                    'weight_decay': 0.01},\n",
                        "                                     'residual': False,\n",
                        "                                     'type': 'REGRESSION',\n",
                        "                                     'weight': 1.0}],\n",
                        "                 'temperature': 0.07,\n",
                        "                 'with_structure': False}}\n",
                        "{ 'class_path': 'foundation_model.data.CompoundDataModule',\n",
                        "  'init_args': { 'attributes_source': '/data/foundation_model/samples/fake_data/attributes.csv',\n",
                        "                 'batch_size': 64,\n",
                        "                 'formula_desc_source': '/data/foundation_model/samples/fake_data/formula_features.csv',\n",
                        "                 'num_workers': 0,\n",
                        "                 'task_configs': [ { 'dims': [64, 32, 1],\n",
                        "                                     'enabled': True,\n",
                        "                                     'name': 'regression_1',\n",
                        "                                     'norm': True,\n",
                        "                                     'optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                                    'eps': 1e-06,\n",
                        "                                                    'factor': 0.5,\n",
                        "                                                    'lr': 0.001,\n",
                        "                                                    'min_lr': 1e-05,\n",
                        "                                                    'mode': 'min',\n",
                        "                                                    'monitor': 'val_regression_1_loss',\n",
                        "                                                    'optimizer_type': 'AdamW',\n",
                        "                                                    'patience': 10,\n",
                        "                                                    'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                                    'weight_decay': 0.01},\n",
                        "                                     'residual': False,\n",
                        "                                     'type': 'REGRESSION',\n",
                        "                                     'weight': 1.0},\n",
                        "                                   { 'dims': [64, 32, 1],\n",
                        "                                     'enabled': True,\n",
                        "                                     'name': 'regression_2',\n",
                        "                                     'norm': True,\n",
                        "                                     'optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                                    'eps': 1e-06,\n",
                        "                                                    'factor': 0.5,\n",
                        "                                                    'lr': 0.001,\n",
                        "                                                    'min_lr': 1e-05,\n",
                        "                                                    'mode': 'min',\n",
                        "                                                    'monitor': 'val_regression_2_loss',\n",
                        "                                                    'optimizer_type': 'AdamW',\n",
                        "                                                    'patience': 10,\n",
                        "                                                    'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                                    'weight_decay': 0.01},\n",
                        "                                     'residual': False,\n",
                        "                                     'type': 'REGRESSION',\n",
                        "                                     'weight': 1.0}],\n",
                        "                 'test_random_seed': 24,\n",
                        "                 'test_split': 0.1,\n",
                        "                 'train_random_seed': 42,\n",
                        "                 'val_split': 0.1}}\n"
                    ]
                }
            ],
            "source": [
                "config_path = os.path.join(project_root, \"samples/generated_configs/generated_model_config.yaml\")\n",
                "logger.info(f\"Loading configuration from: {config_path}\")\n",
                "\n",
                "try:\n",
                "    cfg = OmegaConf.load(config_path)\n",
                "    logger.info(\"Configuration loaded successfully.\")\n",
                "    # Pretty print the loaded configuration (optional)\n",
                "    # logger.info(OmegaConf.to_yaml(cfg))\n",
                "except FileNotFoundError:\n",
                "    logger.error(f\"Configuration file not found at {config_path}\")\n",
                "    raise\n",
                "except Exception as e:\n",
                "    logger.error(f\"Error loading OmegaConf configuration: {e}\")\n",
                "    raise\n",
                "\n",
                "# Extract model and data specific configurations\n",
                "\n",
                "model_cfg = cfg.model\n",
                "data_cfg = cfg.data\n",
                "\n",
                "pp = pprint.PrettyPrinter(indent=2)\n",
                "logger.info(\"Model Configuration:\")\n",
                "pp.pprint(OmegaConf.to_container(model_cfg, resolve=True))  # resolve=True to see interpolated values\n",
                "logger.info(\"Data Configuration:\")\n",
                "pp.pprint(OmegaConf.to_container(data_cfg, resolve=True))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize DataModule\n",
                "\n",
                "Instantiate `CompoundDataModule`, prepare data, and set up for the 'fit' stage to get training and validation dataloaders."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(\"Initializing CompoundDataModule...\")\n",
                "# The data_cfg.init_args.task_configs uses OmegaConf interpolation ${model.init_args.task_configs}\n",
                "# We need to pass the resolved model task_configs to the datamodule if not already resolved by OmegaConf access.\n",
                "# However, OmegaConf usually resolves this when accessing data_cfg.init_args\n",
                "\n",
                "datamodule_args = OmegaConf.to_container(data_cfg.init_args, resolve=True)\n",
                "\n",
                "# Ensure task_configs are passed correctly (OmegaConf should handle the interpolation)\n",
                "if \"task_configs\" not in datamodule_args or datamodule_args[\"task_configs\"] is None:\n",
                "    logger.info(\"Manually assigning task_configs to datamodule_args from model_cfg\")\n",
                "    datamodule_args[\"task_configs\"] = OmegaConf.to_container(model_cfg.init_args.task_configs, resolve=True)\n",
                "\n",
                "datamodule = CompoundDataModule(**datamodule_args)\n",
                "\n",
                "logger.info(\"Preparing data...\")\n",
                "datamodule.prepare_data()  # Downloads or verifies data, typically no-op if local\n",
                "\n",
                "logger.info(\"Setting up DataModule for 'fit' stage...\")\n",
                "datamodule.setup(stage=\"fit\")\n",
                "\n",
                "train_dataloader = datamodule.train_dataloader()\n",
                "val_dataloader = datamodule.val_dataloader()\n",
                "\n",
                "logger.info(f\"Number of training batches: {len(train_dataloader)}\")\n",
                "logger.info(f\"Number of validation batches: {len(val_dataloader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Initialize Model\n",
                "\n",
                "Instantiate `FlexibleMultiTaskModel` using the loaded model configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(\"Initializing FlexibleMultiTaskModel...\")\n",
                "model_init_args = OmegaConf.to_container(model_cfg.init_args, resolve=True)\n",
                "model = FlexibleMultiTaskModel(**model_init_args)\n",
                "logger.info(\"Model initialized successfully.\")\n",
                "logger.info(f\"Model structure:\\n{model}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Fetch a Batch of Data\n",
                "\n",
                "Get one batch from the training dataloader to simulate what the model receives during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(\"Fetching one batch from train_dataloader...\")\n",
                "batch = next(iter(train_dataloader))\n",
                "batch_idx = 0  # For simulation\n",
                "\n",
                "x, y_dict_batch, task_masks_batch, task_sequence_data_batch = batch\n",
                "\n",
                "logger.info(\"Batch details:\")\n",
                "if isinstance(x, tuple):\n",
                "    logger.info(\n",
                "        f\"  x_formula shape: {x[0].shape if x[0] is not None else 'None'}, dtype: {x[0].dtype if x[0] is not None else 'None'}\"\n",
                "    )\n",
                "    logger.info(\n",
                "        f\"  x_struct shape: {x[1].shape if x[1] is not None else 'None'}, dtype: {x[1].dtype if x[1] is not None else 'None'}\"\n",
                "    )\n",
                "else:\n",
                "    logger.info(f\"  x shape: {x.shape}, dtype: {x.dtype}\")\n",
                "\n",
                "logger.info(\"  y_dict_batch keys: \" + str(list(y_dict_batch.keys())))\n",
                "for task_name, tensor in y_dict_batch.items():\n",
                "    logger.info(f\"    {task_name} target shape: {tensor.shape}, dtype: {tensor.dtype}\")\n",
                "\n",
                "logger.info(\"  task_masks_batch keys: \" + str(list(task_masks_batch.keys())))\n",
                "for task_name, tensor in task_masks_batch.items():\n",
                "    logger.info(f\"    {task_name} mask shape: {tensor.shape}, dtype: {tensor.dtype}\")\n",
                "\n",
                "logger.info(\"  task_sequence_data_batch keys: \" + str(list(task_sequence_data_batch.keys())))\n",
                "for task_name, tensor in task_sequence_data_batch.items():\n",
                "    logger.info(f\"    {task_name} sequence data shape: {tensor.shape}, dtype: {tensor.dtype}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Manual `training_step` Walkthrough\n",
                "\n",
                "This section replicates the logic inside the model's `training_step` method cell by cell to inspect intermediate values and the final loss."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1 Unpack Batch and Determine Input Modalities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logs = {}\n",
                "\n",
                "# Determine input modalities (simplified from model's training_step)\n",
                "x_formula = None\n",
                "original_x_struct = None  # Keep original structure input for potential cross-recon target\n",
                "\n",
                "if model.with_structure and isinstance(x, (list, tuple)):\n",
                "    x_formula, original_x_struct = x\n",
                "    if x_formula is None:\n",
                "        raise ValueError(\"Formula input (x_formula) cannot be None in multi-modal mode.\")\n",
                "elif not model.with_structure and isinstance(x, torch.Tensor):\n",
                "    x_formula = x\n",
                "elif model.with_structure and isinstance(x, torch.Tensor):\n",
                "    x_formula = x\n",
                "    # original_x_struct remains None\n",
                "else:\n",
                "    raise TypeError(f\"Unexpected input type/combination. with_structure={model.with_structure}, type(x)={type(x)}\")\n",
                "\n",
                "logger.info(f\"x_formula device: {x_formula.device if x_formula is not None else 'N/A'}\")\n",
                "total_loss = torch.tensor(0.0, device=x_formula.device if x_formula is not None else \"cpu\")\n",
                "logger.info(f\"Initial total_loss: {total_loss}, requires_grad: {total_loss.requires_grad}, device: {total_loss.device}\")\n",
                "\n",
                "# Modality Dropout (skipped as SSL is disabled in current config)\n",
                "x_struct_for_processing = original_x_struct\n",
                "if model.enable_self_supervised_training and model.with_structure and original_x_struct is not None:\n",
                "    logger.info(\"SSL and structure are enabled, modality dropout would be considered here.\")\n",
                "    # Placeholder for modality dropout logic if it were active\n",
                "    pass\n",
                "\n",
                "# Self-Supervised Learning (SSL) Calculations (skipped as SSL is disabled)\n",
                "if model.enable_self_supervised_training:\n",
                "    logger.info(\"SSL is enabled, SSL losses would be calculated here.\")\n",
                "    # Placeholder for SSL loss calculations\n",
                "    pass\n",
                "else:\n",
                "    logger.info(\"SSL is disabled, skipping SSL loss calculations.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Supervised Task Calculations: Forward Pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare input for the standard forward pass\n",
                "if model.with_structure:\n",
                "    forward_input = (x_formula, x_struct_for_processing)\n",
                "else:\n",
                "    forward_input = x_formula\n",
                "\n",
                "logger.info(\"Performing forward pass...\")\n",
                "# Call the model's forward method directly\n",
                "# Ensure model is in training mode if it has dropout/batchnorm layers that behave differently\n",
                "model.train()\n",
                "preds = model(forward_input, task_sequence_data_batch)\n",
                "\n",
                "logger.info(\"Predictions (preds) keys: \" + str(list(preds.keys())))\n",
                "for task_name, pred_tensor in preds.items():\n",
                "    logger.info(\n",
                "        f\"  {task_name} prediction shape: {pred_tensor.shape}, dtype: {pred_tensor.dtype}, requires_grad: {pred_tensor.requires_grad}\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3 Supervised Task Calculations: Loss Computation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(\"Calculating supervised task losses...\")\n",
                "for name, pred_tensor in preds.items():\n",
                "    if name not in y_dict_batch or not model.task_configs_map[name].enabled:\n",
                "        logger.info(f\"Skipping loss for task {name} (no target or disabled).\")\n",
                "        continue\n",
                "\n",
                "    head = model.task_heads[name]\n",
                "    target = y_dict_batch[name]\n",
                "    sample_mask = task_masks_batch.get(name)\n",
                "\n",
                "    if sample_mask is None:\n",
                "        logger.warning(f\"Mask not found for task {name}. Assuming all samples are valid.\")\n",
                "        sample_mask = torch.ones_like(target, dtype=torch.bool, device=target.device)\n",
                "\n",
                "    loss, _ = head.compute_loss(pred_tensor, target, sample_mask)\n",
                "    task_weight = model.w.get(name, 1.0)\n",
                "    weighted_loss = task_weight * loss\n",
                "\n",
                "    logger.info(f\"Task: {name}\")\n",
                "    logger.info(f\"  Raw loss: {loss.item()}, requires_grad: {loss.requires_grad}\")\n",
                "    logger.info(f\"  Task weight: {task_weight}\")\n",
                "    logger.info(f\"  Weighted loss: {weighted_loss.item()}, requires_grad: {weighted_loss.requires_grad}\")\n",
                "\n",
                "    total_loss += weighted_loss\n",
                "\n",
                "    logs[f\"train_{name}_loss\"] = loss.detach()\n",
                "    logs[f\"train_{name}_loss_weighted\"] = weighted_loss.detach()\n",
                "\n",
                "logger.info(\n",
                "    f\"Final total_loss: {total_loss.item()}, requires_grad: {total_loss.requires_grad}, grad_fn: {total_loss.grad_fn}\"\n",
                ")\n",
                "logs[\"train_total_loss\"] = total_loss.detach()\n",
                "\n",
                "# At this point, you can inspect logs or total_loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.4 Manual Backward Pass and Optimizer Step (Conceptual)\n",
                "\n",
                "This demonstrates how the backward pass and optimizer steps would be called. Note that `FlexibleMultiTaskModel` uses manual optimization and can have multiple optimizers. For simplicity, we'll just show the call to `manual_backward`. The actual optimizer configuration and stepping would involve iterating through `model.optimizers()`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if total_loss.requires_grad:\n",
                "    logger.info(\"total_loss requires grad. Proceeding with conceptual backward pass.\")\n",
                "    # In a real scenario with a Lightning Trainer, trainer.strategy.backward would be called via model.manual_backward()\n",
                "    # For this notebook, we can try to call backward directly on the loss if no trainer is involved.\n",
                "    # However, model.manual_backward(total_loss) is the correct way if simulating the model's own logic.\n",
                "\n",
                "    # To simulate model's internal call if it were part of a Trainer:\n",
                "    # model.manual_backward(total_loss) # This would require a trainer instance to be set on the model.\n",
                "\n",
                "    # Direct backward call for demonstration (if no trainer context):\n",
                "    # This will populate .grad attributes of tensors that were part of the computation graph and require grad.\n",
                "    try:\n",
                "        total_loss.backward()  # Computes gradients\n",
                "        logger.info(\"total_loss.backward() called successfully.\")\n",
                "\n",
                "        # Conceptual optimizer step (actual model has multiple optimizers)\n",
                "        # optimizers = model.configure_optimizers() # This returns a list of optimizers/schedulers\n",
                "        # For example, taking the first optimizer if it exists and is a plain optimizer:\n",
                "        # if optimizers:\n",
                "        #     opt0_config = optimizers[0]\n",
                "        #     if isinstance(opt0_config, torch.optim.Optimizer):\n",
                "        #         opt0_config.step()\n",
                "        #         opt0_config.zero_grad(set_to_none=True)\n",
                "        #         logger.info(\"Conceptual step and zero_grad for the first optimizer.\")\n",
                "        #     elif isinstance(opt0_config, dict) and 'optimizer' in opt0_config:\n",
                "        #         opt0_config['optimizer'].step()\n",
                "        #         opt0_config['optimizer'].zero_grad(set_to_none=True)\n",
                "        #         logger.info(\"Conceptual step and zero_grad for the first optimizer from dict.\")\n",
                "        logger.info(\"Gradients would now be populated. Optimizer step would follow.\")\n",
                "        # Example: check grad of a parameter from the first layer of the shared encoder\n",
                "        if hasattr(model, \"shared\") and hasattr(model.shared, \"0\") and hasattr(model.shared[0], \"weight\"):\n",
                "            logger.info(f\"Gradient of model.shared[0].weight: {model.shared[0].weight.grad}\")\n",
                "        else:\n",
                "            logger.info(\"Could not access model.shared[0].weight.grad for inspection.\")\n",
                "\n",
                "    except RuntimeError as e:\n",
                "        logger.error(f\"RuntimeError during backward pass: {e}\")\n",
                "        logger.error(\"This likely means an issue with the computation graph or requires_grad status.\")\n",
                "else:\n",
                "    logger.warning(\n",
                "        \"total_loss does not require grad and has no grad_fn. Skipping backward pass. \"\n",
                "        \"This might indicate all parameters are frozen or loss contributions are zero.\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Prediction Step Walkthrough (Optional)\n",
                "\n",
                "Demonstrate how to use the model for prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(\"Setting up DataModule for 'predict' stage...\")\n",
                "datamodule.setup(stage=\"predict\")  # Re-setup for the predict dataloader\n",
                "predict_dataloader = datamodule.predict_dataloader()\n",
                "\n",
                "if len(predict_dataloader) > 0:\n",
                "    logger.info(\"Fetching one batch from predict_dataloader...\")\n",
                "    predict_batch = next(iter(predict_dataloader))\n",
                "    predict_batch_idx = 0\n",
                "\n",
                "    # The predict_step in the model expects batch[0] to be x_formula\n",
                "    # and batch[3] to be task_sequence_data_batch (if present)\n",
                "    # The CompoundDataset for predict_set=True yields: (model_input_x, sample_y_dict, sample_task_masks_dict, sample_task_sequence_data_dict)\n",
                "    # where model_input_x is x_formula (or (x_formula, None) if with_structure)\n",
                "\n",
                "    logger.info(\"Performing prediction_step...\")\n",
                "    model.eval()  # Set model to evaluation mode\n",
                "    with torch.no_grad():  # Ensure no gradients are computed\n",
                "        predictions = model.predict_step(batch=predict_batch, batch_idx=predict_batch_idx, additional_output=True)\n",
                "\n",
                "    logger.info(\"Predictions output:\")\n",
                "    pp.pprint(predictions)\n",
                "else:\n",
                "    logger.info(\"Predict dataloader is empty, skipping prediction step walkthrough.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## End of Notebook\n",
                "\n",
                "This notebook provides a basic framework for interactively testing the model. You can expand on this by:\n",
                "- Modifying configurations.\n",
                "- Testing specific parts of the model (e.g., individual task heads, encoder blocks).\n",
                "- Visualizing weights, activations, or gradients."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
