{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Interactive Model Test and Debugging\n",
                "\n",
                "This notebook is designed to help test and debug the `FlexibleMultiTaskModel` and `CompoundDataModule` step by step. It loads configurations, initializes the data module and model, fetches a batch of data, and manually walks through the core logic of the `training_step`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-14 22:57:03,462 - INFO - Successfully imported project modules.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import torch\n",
                "import pandas as pd\n",
                "from omegaconf import OmegaConf\n",
                "import yaml  # For loading the raw config if needed\n",
                "import pprint\n",
                "import logging\n",
                "\n",
                "# Configure basic logging for the notebook\n",
                "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "# Add project root to sys.path to allow imports from src\n",
                "# Assumes the notebook is in a subdirectory of the project root (e.g., 'notebooks/')\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
                "# if project_root not in sys.path:\n",
                "#     sys.path.insert(0, project_root)\n",
                "#     logger.info(f\"Added {project_root} to sys.path\")\n",
                "\n",
                "try:\n",
                "    from foundation_model.models.flexible_multi_task_model import FlexibleMultiTaskModel\n",
                "    from foundation_model.data.datamodule import CompoundDataModule\n",
                "    from foundation_model.configs.model_config import TaskType  # For potential inspection\n",
                "\n",
                "    logger.info(\"Successfully imported project modules.\")\n",
                "except ImportError as e:\n",
                "    logger.error(\n",
                "        f\"Error importing project modules: {e}. Ensure PYTHONPATH is set correctly or notebook is in the correct location.\"\n",
                "    )\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Configuration\n",
                "\n",
                "Load the model and data configurations from `samples/generated_configs/generated_model_config.yaml`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-14 22:57:10,344 - INFO - Loading configuration from: /data/foundation_model/samples/generated_configs/generated_model_config.yaml\n",
                        "2025-05-14 22:57:10,381 - INFO - Configuration loaded successfully.\n",
                        "2025-05-14 22:57:10,382 - INFO - Model Configuration:\n",
                        "2025-05-14 22:57:10,389 - INFO - Data Configuration:\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{ 'class_path': 'foundation_model.models.FlexibleMultiTaskModel',\n",
                        "  'init_args': { 'enable_self_supervised_training': False,\n",
                        "                 'loss_weights': { 'contrastive': 1.0,\n",
                        "                                   'cross_recon': 1.0,\n",
                        "                                   'mfm': 1.0},\n",
                        "                 'mask_ratio': 0.15,\n",
                        "                 'modality_dropout_p': 0.3,\n",
                        "                 'norm_shared': True,\n",
                        "                 'residual_shared': False,\n",
                        "                 'shared_block_dims': [256, 128, 64],\n",
                        "                 'shared_block_optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                             'eps': 1e-06,\n",
                        "                                             'factor': 0.1,\n",
                        "                                             'freeze_parameters': False,\n",
                        "                                             'lr': 0.001,\n",
                        "                                             'min_lr': 1e-06,\n",
                        "                                             'mode': 'min',\n",
                        "                                             'monitor': 'val_total_loss',\n",
                        "                                             'optimizer_type': 'AdamW',\n",
                        "                                             'patience': 20,\n",
                        "                                             'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                             'weight_decay': 0.01},\n",
                        "                 'task_configs': [ { 'dims': [64, 32, 1],\n",
                        "                                     'enabled': True,\n",
                        "                                     'name': 'regression_1',\n",
                        "                                     'norm': True,\n",
                        "                                     'optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                                    'eps': 1e-06,\n",
                        "                                                    'factor': 0.5,\n",
                        "                                                    'lr': 0.001,\n",
                        "                                                    'min_lr': 1e-05,\n",
                        "                                                    'mode': 'min',\n",
                        "                                                    'monitor': 'val_regression_1_loss',\n",
                        "                                                    'optimizer_type': 'AdamW',\n",
                        "                                                    'patience': 10,\n",
                        "                                                    'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                                    'weight_decay': 0.01},\n",
                        "                                     'residual': False,\n",
                        "                                     'type': 'REGRESSION',\n",
                        "                                     'weight': 1.0},\n",
                        "                                   { 'dims': [64, 32, 1],\n",
                        "                                     'enabled': True,\n",
                        "                                     'name': 'regression_2',\n",
                        "                                     'norm': True,\n",
                        "                                     'optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                                    'eps': 1e-06,\n",
                        "                                                    'factor': 0.5,\n",
                        "                                                    'lr': 0.001,\n",
                        "                                                    'min_lr': 1e-05,\n",
                        "                                                    'mode': 'min',\n",
                        "                                                    'monitor': 'val_regression_2_loss',\n",
                        "                                                    'optimizer_type': 'AdamW',\n",
                        "                                                    'patience': 10,\n",
                        "                                                    'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                                    'weight_decay': 0.01},\n",
                        "                                     'residual': False,\n",
                        "                                     'type': 'REGRESSION',\n",
                        "                                     'weight': 1.0}],\n",
                        "                 'temperature': 0.07,\n",
                        "                 'with_structure': False}}\n",
                        "{ 'class_path': 'foundation_model.data.CompoundDataModule',\n",
                        "  'init_args': { 'attributes_source': '/data/foundation_model/samples/fake_data/attributes.csv',\n",
                        "                 'batch_size': 64,\n",
                        "                 'formula_desc_source': '/data/foundation_model/samples/fake_data/formula_features.csv',\n",
                        "                 'num_workers': 0,\n",
                        "                 'task_configs': [ { 'dims': [64, 32, 1],\n",
                        "                                     'enabled': True,\n",
                        "                                     'name': 'regression_1',\n",
                        "                                     'norm': True,\n",
                        "                                     'optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                                    'eps': 1e-06,\n",
                        "                                                    'factor': 0.5,\n",
                        "                                                    'lr': 0.001,\n",
                        "                                                    'min_lr': 1e-05,\n",
                        "                                                    'mode': 'min',\n",
                        "                                                    'monitor': 'val_regression_1_loss',\n",
                        "                                                    'optimizer_type': 'AdamW',\n",
                        "                                                    'patience': 10,\n",
                        "                                                    'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                                    'weight_decay': 0.01},\n",
                        "                                     'residual': False,\n",
                        "                                     'type': 'REGRESSION',\n",
                        "                                     'weight': 1.0},\n",
                        "                                   { 'dims': [64, 32, 1],\n",
                        "                                     'enabled': True,\n",
                        "                                     'name': 'regression_2',\n",
                        "                                     'norm': True,\n",
                        "                                     'optimizer': { 'betas': [0.9, 0.999],\n",
                        "                                                    'eps': 1e-06,\n",
                        "                                                    'factor': 0.5,\n",
                        "                                                    'lr': 0.001,\n",
                        "                                                    'min_lr': 1e-05,\n",
                        "                                                    'mode': 'min',\n",
                        "                                                    'monitor': 'val_regression_2_loss',\n",
                        "                                                    'optimizer_type': 'AdamW',\n",
                        "                                                    'patience': 10,\n",
                        "                                                    'scheduler_type': 'ReduceLROnPlateau',\n",
                        "                                                    'weight_decay': 0.01},\n",
                        "                                     'residual': False,\n",
                        "                                     'type': 'REGRESSION',\n",
                        "                                     'weight': 1.0}],\n",
                        "                 'test_random_seed': 24,\n",
                        "                 'test_split': 0.1,\n",
                        "                 'train_random_seed': 42,\n",
                        "                 'val_split': 0.1}}\n"
                    ]
                }
            ],
            "source": [
                "config_path = os.path.join(project_root, \"samples/generated_configs/generated_model_config.yaml\")\n",
                "logger.info(f\"Loading configuration from: {config_path}\")\n",
                "\n",
                "try:\n",
                "    cfg = OmegaConf.load(config_path)\n",
                "    logger.info(\"Configuration loaded successfully.\")\n",
                "    # Pretty print the loaded configuration (optional)\n",
                "    # logger.info(OmegaConf.to_yaml(cfg))\n",
                "except FileNotFoundError:\n",
                "    logger.error(f\"Configuration file not found at {config_path}\")\n",
                "    raise\n",
                "except Exception as e:\n",
                "    logger.error(f\"Error loading OmegaConf configuration: {e}\")\n",
                "    raise\n",
                "\n",
                "# Extract model and data specific configurations\n",
                "\n",
                "model_cfg = cfg.model\n",
                "data_cfg = cfg.data\n",
                "\n",
                "pp = pprint.PrettyPrinter(indent=2)\n",
                "logger.info(\"Model Configuration:\")\n",
                "pp.pprint(OmegaConf.to_container(model_cfg, resolve=True))  # resolve=True to see interpolated values\n",
                "logger.info(\"Data Configuration:\")\n",
                "pp.pprint(OmegaConf.to_container(data_cfg, resolve=True))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialize DataModule\n",
                "\n",
                "Instantiate `CompoundDataModule`, prepare data, and set up for the 'fit' stage to get training and validation dataloaders."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "b3732e83",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-14 22:57:58,600 - INFO - Initializing CompoundDataModule...\n",
                        "2025-05-14 22:57:58,604 - INFO - Initializing CompoundDataModule...\n",
                        "2025-05-14 22:57:58,605 - INFO - --- Loading Data ---\n",
                        "2025-05-14 22:57:58,606 - INFO - Loading 'formula_desc' data from path: /data/foundation_model/samples/fake_data/formula_features.csv\n",
                        "2025-05-14 22:57:58,675 - INFO - Successfully loaded 'formula_desc'. Shape: (1000, 256)\n",
                        "2025-05-14 22:57:58,676 - INFO - Initial loaded formula_df length: 1000\n",
                        "2025-05-14 22:57:58,677 - INFO - Formula_df length after initial dropna: 1000. This index is now the master reference.\n",
                        "2025-05-14 22:57:58,678 - INFO - Loading 'attributes' data from path: /data/foundation_model/samples/fake_data/attributes.csv\n",
                        "2025-05-14 22:57:58,681 - INFO - Successfully loaded 'attributes'. Shape: (1000, 6)\n",
                        "2025-05-14 22:57:58,681 - INFO - Initial loaded attributes_df length: 1000\n",
                        "2025-05-14 22:57:58,682 - INFO - --- Aligning DataFrames by formula_df index (master_index length: 1000) ---\n",
                        "2025-05-14 22:57:58,684 - INFO - Length after aligning formula_df and attributes_df: 1000\n",
                        "2025-05-14 22:57:58,684 - INFO - with_structure is False. Structure data will not be loaded or used.\n",
                        "2025-05-14 22:57:58,685 - INFO - Final aligned formula_df length: 1000\n",
                        "2025-05-14 22:57:58,685 - INFO - Final aligned attributes_df length: 1000\n",
                        "2025-05-14 22:57:58,686 - INFO - Final aligned structure_df is None.\n",
                        "2025-05-14 22:57:58,686 - INFO - Final actual_with_structure status: False\n",
                        "2025-05-14 22:57:58,686 - INFO - DataModule initialized with 2 task configurations.\n",
                        "2025-05-14 22:57:58,687 - INFO - Preparing data...\n",
                        "2025-05-14 22:57:58,687 - INFO - Setting up DataModule for 'fit' stage...\n",
                        "2025-05-14 22:57:58,688 - INFO - --- Setting up DataModule for stage: fit ---\n",
                        "2025-05-14 22:57:58,688 - INFO - Total samples available before splitting (from attributes_df index): 1000\n",
                        "2025-05-14 22:57:58,689 - INFO - Data split strategy: Using 'split' column from attributes_df.\n",
                        "2025-05-14 22:57:58,690 - INFO - Value counts in 'split' column: {'train': 692, 'test': 163, 'val': 145}\n",
                        "2025-05-14 22:57:58,691 - INFO - Final dataset sizes after splitting: Train=692, Validation=145, Test=163\n",
                        "2025-05-14 22:57:58,692 - INFO - Passing use_structure_for_this_dataset=False to CompoundDataset instances.\n",
                        "2025-05-14 22:57:58,692 - INFO - --- Creating 'fit' stage datasets (train/val) ---\n",
                        "2025-05-14 22:57:58,692 - INFO - Creating train_dataset with 692 samples.\n",
                        "2025-05-14 22:57:58,696 - INFO - [train_dataset] Initializing CompoundDataset...\n",
                        "2025-05-14 22:57:58,696 - INFO - [train_dataset] is_predict_set: False, use_structure: False\n",
                        "2025-05-14 22:57:58,697 - INFO - [train_dataset] Received formula_desc with shape: (692, 256)\n",
                        "2025-05-14 22:57:58,697 - INFO - [train_dataset] Received attributes with shape: (692, 6)\n",
                        "2025-05-14 22:57:58,710 - INFO - [train_dataset] Final x_formula shape: torch.Size([692, 256])\n",
                        "2025-05-14 22:57:58,711 - INFO - [train_dataset] Processing 2 provided task configurations.\n",
                        "2025-05-14 22:57:58,712 - INFO - [train_dataset] CompoundDataset initialization complete. Processed 0 enabled tasks.\n",
                        "2025-05-14 22:57:58,712 - INFO - Creating val_dataset with 145 samples.\n",
                        "2025-05-14 22:57:58,713 - INFO - [val_dataset] Initializing CompoundDataset...\n",
                        "2025-05-14 22:57:58,713 - INFO - [val_dataset] is_predict_set: False, use_structure: False\n",
                        "2025-05-14 22:57:58,714 - INFO - [val_dataset] Received formula_desc with shape: (145, 256)\n",
                        "2025-05-14 22:57:58,714 - INFO - [val_dataset] Received attributes with shape: (145, 6)\n",
                        "2025-05-14 22:57:58,715 - INFO - [val_dataset] Final x_formula shape: torch.Size([145, 256])\n",
                        "2025-05-14 22:57:58,715 - INFO - [val_dataset] Processing 2 provided task configurations.\n",
                        "2025-05-14 22:57:58,716 - INFO - [val_dataset] CompoundDataset initialization complete. Processed 0 enabled tasks.\n",
                        "2025-05-14 22:57:58,716 - INFO - --- DataModule setup for stage 'fit' complete ---\n",
                        "2025-05-14 22:57:58,717 - INFO - Number of training batches: 11\n",
                        "2025-05-14 22:57:58,717 - INFO - Number of validation batches: 3\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- CompoundDataModule: train_dataloader() called. Train dataset length: 692\n"
                    ]
                }
            ],
            "source": [
                "logger.info(\"Initializing CompoundDataModule...\")\n",
                "# The data_cfg.init_args.task_configs uses OmegaConf interpolation ${model.init_args.task_configs}\n",
                "# We need to pass the resolved model task_configs to the datamodule if not already resolved by OmegaConf access.\n",
                "# However, OmegaConf usually resolves this when accessing data_cfg.init_args\n",
                "\n",
                "datamodule_args = OmegaConf.to_container(data_cfg.init_args, resolve=True)\n",
                "\n",
                "# Ensure task_configs are passed correctly (OmegaConf should handle the interpolation)\n",
                "if \"task_configs\" not in datamodule_args or datamodule_args[\"task_configs\"] is None:\n",
                "    logger.info(\"Manually assigning task_configs to datamodule_args from model_cfg\")\n",
                "    datamodule_args[\"task_configs\"] = OmegaConf.to_container(model_cfg.init_args.task_configs, resolve=True)\n",
                "\n",
                "datamodule = CompoundDataModule(**datamodule_args)\n",
                "\n",
                "logger.info(\"Preparing data...\")\n",
                "datamodule.prepare_data()  # Downloads or verifies data, typically no-op if local\n",
                "\n",
                "logger.info(\"Setting up DataModule for 'fit' stage...\")\n",
                "datamodule.setup(stage=\"fit\")\n",
                "\n",
                "train_dataloader = datamodule.train_dataloader()\n",
                "val_dataloader = datamodule.val_dataloader()\n",
                "\n",
                "logger.info(f\"Number of training batches: {len(train_dataloader)}\")\n",
                "logger.info(f\"Number of validation batches: {len(val_dataloader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d53d65d9",
            "metadata": {},
            "source": [
                "## 4. Initialize Model\n",
                "\n",
                "Instantiate `FlexibleMultiTaskModel` using the loaded model configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "cff4e2e9",
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "At least one task configuration must be provided",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfoundation_model\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfigs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OptimizerConfig, RegressionTaskConfig\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mFlexibleMultiTaskModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshared_block_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# RegressionTaskConfig(\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     name=\"regression_task\",  # Example task name\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     dims=[64, 32, 1],\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     norm=True,\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     residual=False,\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     weight=1.0,\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     enabled=True,\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     optimizer=OptimizerConfig(\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         optimizer_type=\"AdamW\",\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         lr=0.001,\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         weight_decay=0.01,\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         eps=1.0e-06,\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         betas=(0.9, 0.999),\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         scheduler_type=\"ReduceLROnPlateau\",\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         mode=\"min\",\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         factor=0.5,\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         patience=10,\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         min_lr=1.0e-05,\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#         monitor=\"val_regression_task_loss\",\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     ),\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# )\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshared_block_optimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOptimizerConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAdamW\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0e-06\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReduceLROnPlateau\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0e-05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval_regression_task_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/data/foundation_model/src/foundation_model/models/flexible_multi_task_model.py:159\u001b[39m, in \u001b[36mFlexibleMultiTaskModel.__init__\u001b[39m\u001b[34m(self, shared_block_dims, task_configs, norm_shared, residual_shared, shared_block_optimizer, with_structure, struct_block_dims, modality_dropout_p, enable_self_supervised_training, loss_weights, mask_ratio, temperature)\u001b[39m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mshared_block_dims must have at least 2 elements\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task_configs:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one task configuration must be provided\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# Store configuration parameters\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mself\u001b[39m.shared_block_dims = shared_block_dims\n",
                        "\u001b[31mValueError\u001b[39m: At least one task configuration must be provided"
                    ]
                }
            ],
            "source": [
                "from foundation_model.configs.model_config import OptimizerConfig, RegressionTaskConfig\n",
                "\n",
                "\n",
                "FlexibleMultiTaskModel(\n",
                "    shared_block_dims=[256, 128, 64],\n",
                "    task_configs=[\n",
                "        # RegressionTaskConfig(\n",
                "        #     name=\"regression_task\",  # Example task name\n",
                "        #     dims=[64, 32, 1],\n",
                "        #     norm=True,\n",
                "        #     residual=False,\n",
                "        #     weight=1.0,\n",
                "        #     enabled=True,\n",
                "        #     optimizer=OptimizerConfig(\n",
                "        #         optimizer_type=\"AdamW\",\n",
                "        #         lr=0.001,\n",
                "        #         weight_decay=0.01,\n",
                "        #         eps=1.0e-06,\n",
                "        #         betas=(0.9, 0.999),\n",
                "        #         scheduler_type=\"ReduceLROnPlateau\",\n",
                "        #         mode=\"min\",\n",
                "        #         factor=0.5,\n",
                "        #         patience=10,\n",
                "        #         min_lr=1.0e-05,\n",
                "        #         monitor=\"val_regression_task_loss\",\n",
                "        #     ),\n",
                "        # )\n",
                "    ],\n",
                "    shared_block_optimizer=OptimizerConfig(\n",
                "        optimizer_type=\"AdamW\",\n",
                "        lr=0.01,\n",
                "        weight_decay=0.01,\n",
                "        eps=1.0e-06,\n",
                "        betas=(0.9, 0.999),\n",
                "        scheduler_type=\"ReduceLROnPlateau\",\n",
                "        mode=\"min\",\n",
                "        factor=0.5,\n",
                "        patience=10,\n",
                "        min_lr=1.0e-05,\n",
                "        monitor=\"val_regression_task_loss\",\n",
                "    ),\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "b624b9e5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-05-14 23:05:35,488 - INFO - Initializing FlexibleMultiTaskModel...\n"
                    ]
                },
                {
                    "ename": "AssertionError",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mInitializing FlexibleMultiTaskModel...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m model_init_args = OmegaConf.to_container(model_cfg.init_args, resolve=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mFlexibleMultiTaskModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mModel initialized successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel structure:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/data/foundation_model/src/foundation_model/models/flexible_multi_task_model.py:188\u001b[39m, in \u001b[36mFlexibleMultiTaskModel.__init__\u001b[39m\u001b[34m(self, shared_block_dims, task_configs, norm_shared, residual_shared, shared_block_optimizer, with_structure, struct_block_dims, modality_dropout_p, enable_self_supervised_training, loss_weights, mask_ratio, temperature)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Initialize model components\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;28mself\u001b[39m._init_foundation_encoder()\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_task_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m enable_self_supervised_training:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_self_supervised_module()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/data/foundation_model/src/foundation_model/models/flexible_multi_task_model.py:282\u001b[39m, in \u001b[36mFlexibleMultiTaskModel._init_task_heads\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_task_heads\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    281\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Initialize task heads based on configurations.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28mself\u001b[39m.task_heads = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_task_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# Apply optimizer freeze_parameters to task heads\u001b[39;00m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task_heads.items():\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m/data/foundation_model/src/foundation_model/models/flexible_multi_task_model.py:270\u001b[39m, in \u001b[36mFlexibleMultiTaskModel._build_task_heads\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_item.type == TaskType.REGRESSION:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config_item, RegressionTaskConfig)\n\u001b[32m    271\u001b[39m     task_heads_dict[config_item.name] = RegressionHead(config=config_item)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m config_item.type == TaskType.CLASSIFICATION:\n",
                        "\u001b[31mAssertionError\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "logger.info(\"Initializing FlexibleMultiTaskModel...\")\n",
                "model_init_args = OmegaConf.to_container(model_cfg.init_args, resolve=True)\n",
                "model = FlexibleMultiTaskModel(**model_cfg.init_args)\n",
                "logger.info(\"Model initialized successfully.\")\n",
                "logger.info(f\"Model structure:\\n{model}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Fetch a Batch of Data\n",
                "\n",
                "Get one batch from the training dataloader to simulate what the model receives during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(\"Fetching one batch from train_dataloader...\")\n",
                "batch = next(iter(train_dataloader))\n",
                "batch_idx = 0  # For simulation\n",
                "\n",
                "x, y_dict_batch, task_masks_batch, task_sequence_data_batch = batch\n",
                "\n",
                "logger.info(\"Batch details:\")\n",
                "if isinstance(x, tuple):\n",
                "    logger.info(\n",
                "        f\"  x_formula shape: {x[0].shape if x[0] is not None else 'None'}, dtype: {x[0].dtype if x[0] is not None else 'None'}\"\n",
                "    )\n",
                "    logger.info(\n",
                "        f\"  x_struct shape: {x[1].shape if x[1] is not None else 'None'}, dtype: {x[1].dtype if x[1] is not None else 'None'}\"\n",
                "    )\n",
                "else:\n",
                "    logger.info(f\"  x shape: {x.shape}, dtype: {x.dtype}\")\n",
                "\n",
                "logger.info(\"  y_dict_batch keys: \" + str(list(y_dict_batch.keys())))\n",
                "for task_name, tensor in y_dict_batch.items():\n",
                "    logger.info(f\"    {task_name} target shape: {tensor.shape}, dtype: {tensor.dtype}\")\n",
                "\n",
                "logger.info(\"  task_masks_batch keys: \" + str(list(task_masks_batch.keys())))\n",
                "for task_name, tensor in task_masks_batch.items():\n",
                "    logger.info(f\"    {task_name} mask shape: {tensor.shape}, dtype: {tensor.dtype}\")\n",
                "\n",
                "logger.info(\"  task_sequence_data_batch keys: \" + str(list(task_sequence_data_batch.keys())))\n",
                "for task_name, tensor in task_sequence_data_batch.items():\n",
                "    logger.info(f\"    {task_name} sequence data shape: {tensor.shape}, dtype: {tensor.dtype}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Manual `training_step` Walkthrough\n",
                "\n",
                "This section replicates the logic inside the model's `training_step` method cell by cell to inspect intermediate values and the final loss."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.1 Unpack Batch and Determine Input Modalities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logs = {}\n",
                "\n",
                "# Determine input modalities (simplified from model's training_step)\n",
                "x_formula = None\n",
                "original_x_struct = None  # Keep original structure input for potential cross-recon target\n",
                "\n",
                "if model.with_structure and isinstance(x, (list, tuple)):\n",
                "    x_formula, original_x_struct = x\n",
                "    if x_formula is None:\n",
                "        raise ValueError(\"Formula input (x_formula) cannot be None in multi-modal mode.\")\n",
                "elif not model.with_structure and isinstance(x, torch.Tensor):\n",
                "    x_formula = x\n",
                "elif model.with_structure and isinstance(x, torch.Tensor):\n",
                "    x_formula = x\n",
                "    # original_x_struct remains None\n",
                "else:\n",
                "    raise TypeError(f\"Unexpected input type/combination. with_structure={model.with_structure}, type(x)={type(x)}\")\n",
                "\n",
                "logger.info(f\"x_formula device: {x_formula.device if x_formula is not None else 'N/A'}\")\n",
                "total_loss = torch.tensor(0.0, device=x_formula.device if x_formula is not None else \"cpu\")\n",
                "logger.info(f\"Initial total_loss: {total_loss}, requires_grad: {total_loss.requires_grad}, device: {total_loss.device}\")\n",
                "\n",
                "# Modality Dropout (skipped as SSL is disabled in current config)\n",
                "x_struct_for_processing = original_x_struct\n",
                "if model.enable_self_supervised_training and model.with_structure and original_x_struct is not None:\n",
                "    logger.info(\"SSL and structure are enabled, modality dropout would be considered here.\")\n",
                "    # Placeholder for modality dropout logic if it were active\n",
                "    pass\n",
                "\n",
                "# Self-Supervised Learning (SSL) Calculations (skipped as SSL is disabled)\n",
                "if model.enable_self_supervised_training:\n",
                "    logger.info(\"SSL is enabled, SSL losses would be calculated here.\")\n",
                "    # Placeholder for SSL loss calculations\n",
                "    pass\n",
                "else:\n",
                "    logger.info(\"SSL is disabled, skipping SSL loss calculations.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Supervised Task Calculations: Forward Pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare input for the standard forward pass\n",
                "if model.with_structure:\n",
                "    forward_input = (x_formula, x_struct_for_processing)\n",
                "else:\n",
                "    forward_input = x_formula\n",
                "\n",
                "logger.info(\"Performing forward pass...\")\n",
                "# Call the model's forward method directly\n",
                "# Ensure model is in training mode if it has dropout/batchnorm layers that behave differently\n",
                "model.train()\n",
                "preds = model(forward_input, task_sequence_data_batch)\n",
                "\n",
                "logger.info(\"Predictions (preds) keys: \" + str(list(preds.keys())))\n",
                "for task_name, pred_tensor in preds.items():\n",
                "    logger.info(\n",
                "        f\"  {task_name} prediction shape: {pred_tensor.shape}, dtype: {pred_tensor.dtype}, requires_grad: {pred_tensor.requires_grad}\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3 Supervised Task Calculations: Loss Computation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(\"Calculating supervised task losses...\")\n",
                "for name, pred_tensor in preds.items():\n",
                "    if name not in y_dict_batch or not model.task_configs_map[name].enabled:\n",
                "        logger.info(f\"Skipping loss for task {name} (no target or disabled).\")\n",
                "        continue\n",
                "\n",
                "    head = model.task_heads[name]\n",
                "    target = y_dict_batch[name]\n",
                "    sample_mask = task_masks_batch.get(name)\n",
                "\n",
                "    if sample_mask is None:\n",
                "        logger.warning(f\"Mask not found for task {name}. Assuming all samples are valid.\")\n",
                "        sample_mask = torch.ones_like(target, dtype=torch.bool, device=target.device)\n",
                "\n",
                "    loss, _ = head.compute_loss(pred_tensor, target, sample_mask)\n",
                "    task_weight = model.w.get(name, 1.0)\n",
                "    weighted_loss = task_weight * loss\n",
                "\n",
                "    logger.info(f\"Task: {name}\")\n",
                "    logger.info(f\"  Raw loss: {loss.item()}, requires_grad: {loss.requires_grad}\")\n",
                "    logger.info(f\"  Task weight: {task_weight}\")\n",
                "    logger.info(f\"  Weighted loss: {weighted_loss.item()}, requires_grad: {weighted_loss.requires_grad}\")\n",
                "\n",
                "    total_loss += weighted_loss\n",
                "\n",
                "    logs[f\"train_{name}_loss\"] = loss.detach()\n",
                "    logs[f\"train_{name}_loss_weighted\"] = weighted_loss.detach()\n",
                "\n",
                "logger.info(\n",
                "    f\"Final total_loss: {total_loss.item()}, requires_grad: {total_loss.requires_grad}, grad_fn: {total_loss.grad_fn}\"\n",
                ")\n",
                "logs[\"train_total_loss\"] = total_loss.detach()\n",
                "\n",
                "# At this point, you can inspect logs or total_loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.4 Manual Backward Pass and Optimizer Step (Conceptual)\n",
                "\n",
                "This demonstrates how the backward pass and optimizer steps would be called. Note that `FlexibleMultiTaskModel` uses manual optimization and can have multiple optimizers. For simplicity, we'll just show the call to `manual_backward`. The actual optimizer configuration and stepping would involve iterating through `model.optimizers()`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if total_loss.requires_grad:\n",
                "    logger.info(\"total_loss requires grad. Proceeding with conceptual backward pass.\")\n",
                "    # In a real scenario with a Lightning Trainer, trainer.strategy.backward would be called via model.manual_backward()\n",
                "    # For this notebook, we can try to call backward directly on the loss if no trainer is involved.\n",
                "    # However, model.manual_backward(total_loss) is the correct way if simulating the model's own logic.\n",
                "\n",
                "    # To simulate model's internal call if it were part of a Trainer:\n",
                "    # model.manual_backward(total_loss) # This would require a trainer instance to be set on the model.\n",
                "\n",
                "    # Direct backward call for demonstration (if no trainer context):\n",
                "    # This will populate .grad attributes of tensors that were part of the computation graph and require grad.\n",
                "    try:\n",
                "        total_loss.backward()  # Computes gradients\n",
                "        logger.info(\"total_loss.backward() called successfully.\")\n",
                "\n",
                "        # Conceptual optimizer step (actual model has multiple optimizers)\n",
                "        # optimizers = model.configure_optimizers() # This returns a list of optimizers/schedulers\n",
                "        # For example, taking the first optimizer if it exists and is a plain optimizer:\n",
                "        # if optimizers:\n",
                "        #     opt0_config = optimizers[0]\n",
                "        #     if isinstance(opt0_config, torch.optim.Optimizer):\n",
                "        #         opt0_config.step()\n",
                "        #         opt0_config.zero_grad(set_to_none=True)\n",
                "        #         logger.info(\"Conceptual step and zero_grad for the first optimizer.\")\n",
                "        #     elif isinstance(opt0_config, dict) and 'optimizer' in opt0_config:\n",
                "        #         opt0_config['optimizer'].step()\n",
                "        #         opt0_config['optimizer'].zero_grad(set_to_none=True)\n",
                "        #         logger.info(\"Conceptual step and zero_grad for the first optimizer from dict.\")\n",
                "        logger.info(\"Gradients would now be populated. Optimizer step would follow.\")\n",
                "        # Example: check grad of a parameter from the first layer of the shared encoder\n",
                "        if hasattr(model, \"shared\") and hasattr(model.shared, \"0\") and hasattr(model.shared[0], \"weight\"):\n",
                "            logger.info(f\"Gradient of model.shared[0].weight: {model.shared[0].weight.grad}\")\n",
                "        else:\n",
                "            logger.info(\"Could not access model.shared[0].weight.grad for inspection.\")\n",
                "\n",
                "    except RuntimeError as e:\n",
                "        logger.error(f\"RuntimeError during backward pass: {e}\")\n",
                "        logger.error(\"This likely means an issue with the computation graph or requires_grad status.\")\n",
                "else:\n",
                "    logger.warning(\n",
                "        \"total_loss does not require grad and has no grad_fn. Skipping backward pass. \"\n",
                "        \"This might indicate all parameters are frozen or loss contributions are zero.\"\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Prediction Step Walkthrough (Optional)\n",
                "\n",
                "Demonstrate how to use the model for prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logger.info(\"Setting up DataModule for 'predict' stage...\")\n",
                "datamodule.setup(stage=\"predict\")  # Re-setup for the predict dataloader\n",
                "predict_dataloader = datamodule.predict_dataloader()\n",
                "\n",
                "if len(predict_dataloader) > 0:\n",
                "    logger.info(\"Fetching one batch from predict_dataloader...\")\n",
                "    predict_batch = next(iter(predict_dataloader))\n",
                "    predict_batch_idx = 0\n",
                "\n",
                "    # The predict_step in the model expects batch[0] to be x_formula\n",
                "    # and batch[3] to be task_sequence_data_batch (if present)\n",
                "    # The CompoundDataset for predict_set=True yields: (model_input_x, sample_y_dict, sample_task_masks_dict, sample_task_sequence_data_dict)\n",
                "    # where model_input_x is x_formula (or (x_formula, None) if with_structure)\n",
                "\n",
                "    logger.info(\"Performing prediction_step...\")\n",
                "    model.eval()  # Set model to evaluation mode\n",
                "    with torch.no_grad():  # Ensure no gradients are computed\n",
                "        predictions = model.predict_step(batch=predict_batch, batch_idx=predict_batch_idx, additional_output=True)\n",
                "\n",
                "    logger.info(\"Predictions output:\")\n",
                "    pp.pprint(predictions)\n",
                "else:\n",
                "    logger.info(\"Predict dataloader is empty, skipping prediction step walkthrough.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## End of Notebook\n",
                "\n",
                "This notebook provides a basic framework for interactively testing the model. You can expand on this by:\n",
                "- Modifying configurations.\n",
                "- Testing specific parts of the model (e.g., individual task heads, encoder blocks).\n",
                "- Visualizing weights, activations, or gradients."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
