{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3104328d",
   "metadata": {},
   "source": [
    "# Progressive Pretrain → Classification: Results Analysis\n",
    "\n",
    "训练完成后，从磁盘加载各 run/stage 的结果文件，汇总并绘制：\n",
    "1. **Accuracy vs Pretrain Stages** — 分类准确率随 pretrain source task 数量的变化\n",
    "2. **Per-class Metrics** (ALL test) — 5 classes × 3 metrics (precision, recall, f1)\n",
    "3. **Per-class Metrics** (n_elements ≥ 4) — 仅含组分元素数 ≥ 4 的 test 样本\n",
    "\n",
    "支持多台 machine 上分别训练各 run，最终将 `runXX/` 目录复制到同一 `output_dir` 下后运行本 notebook。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e21933",
   "metadata": {},
   "source": [
    "## 1. 设置路径\n",
    "\n",
    "将 `OUTPUT_DIR` 指向训练脚本产出的根目录（包含 `run01/`, `run02/`, ... 子目录）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 修改此处指向你的训练输出目录 ===\n",
    "OUTPUT_DIR = Path(\"../logs/multi_task_suite/0211_0355\")\n",
    "\n",
    "# 自动从 experiment_records.json 获取 class_names（如果存在）\n",
    "# 否则手动指定\n",
    "CLASS_NAMES = [\"DAC\", \"DQC\", \"IAC\", \"IQC\", \"others\"]\n",
    "\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Exists: {OUTPUT_DIR.exists()}\")\n",
    "if OUTPUT_DIR.exists():\n",
    "    runs = sorted([d.name for d in OUTPUT_DIR.iterdir() if d.is_dir() and d.name.startswith(\"run\")])\n",
    "    print(f\"Found {len(runs)} run(s): {runs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb000e7",
   "metadata": {},
   "source": [
    "## 2. 加载工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clf_reports_from_disk(root_dir: Path, report_name: str = \"clf_report_all.json\"):\n",
    "    \"\"\"\n",
    "    扫描 root_dir 下所有 run/stage 的 clf report JSON。\n",
    "    返回 dict: {run_label: {stage_idx: report_dict}}\n",
    "    目录结构: root_dir / runXX / pretrain_stageNN_xxx / finetune / material_type / <report_name>\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for p in sorted(root_dir.glob(f\"run*/pretrain_stage*/finetune/material_type/{report_name}\")):\n",
    "        parts = p.relative_to(root_dir).parts\n",
    "        run_label = parts[0]\n",
    "        stage_str = parts[1]  # e.g. 'pretrain_stage01_efermi'\n",
    "        stage_idx = int(stage_str.split(\"_\")[1].replace(\"stage\", \"\"))\n",
    "        with open(p) as f:\n",
    "            report = json.load(f)\n",
    "        results.setdefault(run_label, {})[stage_idx] = report\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_metrics_from_disk(root_dir: Path):\n",
    "    \"\"\"\n",
    "    扫描 root_dir 下所有 run/stage 的 metrics.json。\n",
    "    返回 dict: {run_label: {stage_idx: metrics_dict}}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for p in sorted(root_dir.glob(\"run*/pretrain_stage*/finetune/material_type/metrics.json\")):\n",
    "        parts = p.relative_to(root_dir).parts\n",
    "        run_label = parts[0]\n",
    "        stage_str = parts[1]\n",
    "        stage_idx = int(stage_str.split(\"_\")[1].replace(\"stage\", \"\"))\n",
    "        with open(p) as f:\n",
    "            metrics = json.load(f)\n",
    "        results.setdefault(run_label, {})[stage_idx] = metrics\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_experiment_records(root_dir: Path):\n",
    "    \"\"\"Load experiment_records.json if available.\"\"\"\n",
    "    path = root_dir / \"experiment_records.json\"\n",
    "    if path.exists():\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac8d18",
   "metadata": {},
   "source": [
    "## 3. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f57738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "metrics_by_run = load_metrics_from_disk(OUTPUT_DIR)\n",
    "print(f\"Found {len(metrics_by_run)} run(s) with metrics\")\n",
    "\n",
    "# Determine n_stages from data\n",
    "n_stages = 0\n",
    "for run_label, stages in metrics_by_run.items():\n",
    "    n_stages = max(n_stages, max(stages.keys()))\n",
    "    print(f\"  {run_label}: stages {sorted(stages.keys())}\")\n",
    "\n",
    "print(f\"\\nTotal stages: {n_stages}\")\n",
    "\n",
    "# Build accuracy matrix\n",
    "accuracy_matrix = []\n",
    "run_labels = sorted(metrics_by_run.keys())\n",
    "for run_label in run_labels:\n",
    "    stages = metrics_by_run[run_label]\n",
    "    accs = []\n",
    "    for s in range(1, n_stages + 1):\n",
    "        if s in stages:\n",
    "            accs.append(stages[s].get(\"test_accuracy\"))\n",
    "        else:\n",
    "            accs.append(None)\n",
    "    accuracy_matrix.append(accs)\n",
    "\n",
    "# Load experiment records for task sequence info\n",
    "experiment_records = load_experiment_records(OUTPUT_DIR)\n",
    "if experiment_records:\n",
    "    print(f\"\\nExperiment records: {len(experiment_records)} run(s)\")\n",
    "else:\n",
    "    print(\"\\nNo experiment_records.json found (optional).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2991734f",
   "metadata": {},
   "source": [
    "## 4. Accuracy vs Pretrain Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac572ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5), dpi=150)\n",
    "\n",
    "for i, (run_label, run_accs) in enumerate(zip(run_labels, accuracy_matrix)):\n",
    "    stages_x = list(range(1, len(run_accs) + 1))\n",
    "    ax.plot(stages_x, run_accs, \"o-\", alpha=0.5, label=run_label)\n",
    "\n",
    "# Mean ± Std\n",
    "acc_array = np.array([\n",
    "    [a if a is not None else np.nan for a in row]\n",
    "    for row in accuracy_matrix\n",
    "])\n",
    "mean_acc = np.nanmean(acc_array, axis=0)\n",
    "std_acc = np.nanstd(acc_array, axis=0)\n",
    "stages_x = np.arange(1, n_stages + 1)\n",
    "\n",
    "ax.errorbar(stages_x, mean_acc, yerr=std_acc, fmt=\"s-\", color=\"black\",\n",
    "            linewidth=2, markersize=8, capsize=4, label=\"Mean ± Std\")\n",
    "\n",
    "ax.set_xlabel(\"Number of Pretrain Source Tasks\", fontsize=13)\n",
    "ax.set_ylabel(\"Test Classification Accuracy\", fontsize=13)\n",
    "ax.set_title(\"Material Type Classification vs. Progressive Pretraining\", fontsize=14)\n",
    "ax.set_xticks(stages_x)\n",
    "ax.legend(fontsize=9, loc=\"best\", ncol=max(1, len(run_labels) // 5))\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.savefig(OUTPUT_DIR / \"accuracy_vs_stages.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Accuracy Summary ===\")\n",
    "for s in range(n_stages):\n",
    "    print(f\"  Stage {s+1}: mean={mean_acc[s]:.4f} ± {std_acc[s]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aefff23",
   "metadata": {},
   "source": [
    "## 5. Per-Run Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5cb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_records:\n",
    "    print(\"=== Per-Run Summary ===\")\n",
    "    for record in experiment_records:\n",
    "        run = record[\"run\"]\n",
    "        seq = record[\"task_sequence\"]\n",
    "        # Get final accuracy from finetune records\n",
    "        ft_records = record.get(\"finetune\", [])\n",
    "        final_acc = ft_records[-1].get(\"test_accuracy\") if ft_records else None\n",
    "        if final_acc is not None:\n",
    "            print(f\"  {run}: order={seq}, final_acc={final_acc:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {run}: order={seq}, no result\")\n",
    "else:\n",
    "    print(\"=== Per-Run Summary (from metrics.json) ===\")\n",
    "    for run_label, accs in zip(run_labels, accuracy_matrix):\n",
    "        final_acc = accs[-1] if accs else None\n",
    "        if final_acc is not None:\n",
    "            print(f\"  {run_label}: final_acc={final_acc:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {run_label}: no result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d76c79a",
   "metadata": {},
   "source": [
    "## 6. Per-class Classification Metrics vs Pretrain Stages\n",
    "\n",
    "从磁盘加载各 run/stage 的 `clf_report_all.json` 和 `clf_report_ge4.json`，  \n",
    "汇总后绘制 5 classes × 3 metrics 的 subplot（支持不同 machine 上的 run 结果合并）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_class_metrics(\n",
    "    reports_by_run,\n",
    "    class_names,\n",
    "    n_stages,\n",
    "    title_suffix=\"\",\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    绘制 n_classes rows × 3 cols (precision/recall/f1) 的 subplot。\n",
    "    reports_by_run: {run_label: {stage_idx: clf_report_dict}}\n",
    "    \"\"\"\n",
    "    metrics_of_interest = [\"precision\", \"recall\", \"f1-score\"]\n",
    "    n_classes = len(class_names)\n",
    "    n_metrics = len(metrics_of_interest)\n",
    "    n_runs = len(reports_by_run)\n",
    "\n",
    "    # shape [n_runs, n_stages, n_classes, n_metrics]\n",
    "    arr = np.full((n_runs, n_stages, n_classes, n_metrics), np.nan)\n",
    "\n",
    "    for r_idx, (run_label, stages_dict) in enumerate(sorted(reports_by_run.items())):\n",
    "        for s_idx_1based, report in stages_dict.items():\n",
    "            s_idx = s_idx_1based - 1  # 0-based\n",
    "            if not report:\n",
    "                continue\n",
    "            for c_idx, cname in enumerate(class_names):\n",
    "                if cname in report:\n",
    "                    for m_idx, metric in enumerate(metrics_of_interest):\n",
    "                        arr[r_idx, s_idx, c_idx, m_idx] = report[cname].get(metric, np.nan)\n",
    "\n",
    "    mean_vals = np.nanmean(arr, axis=0)\n",
    "    std_vals = np.nanstd(arr, axis=0)\n",
    "    stages_x = np.arange(1, n_stages + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        n_classes, n_metrics,\n",
    "        figsize=(4 * n_metrics, 3.2 * n_classes),\n",
    "        sharex=True, squeeze=False,\n",
    "    )\n",
    "    colors = plt.cm.tab10.colors\n",
    "\n",
    "    for c_idx, cname in enumerate(class_names):\n",
    "        for m_idx, metric in enumerate(metrics_of_interest):\n",
    "            ax = axes[c_idx, m_idx]\n",
    "            mu = mean_vals[:, c_idx, m_idx]\n",
    "            sd = std_vals[:, c_idx, m_idx]\n",
    "\n",
    "            ax.plot(stages_x, mu, \"o-\", color=colors[c_idx], linewidth=1.5, markersize=4)\n",
    "            ax.fill_between(stages_x, mu - sd, mu + sd, alpha=0.2, color=colors[c_idx])\n",
    "\n",
    "            ax.set_ylim(-0.05, 1.05)\n",
    "            ax.set_xticks(stages_x)\n",
    "\n",
    "            if c_idx == 0:\n",
    "                ax.set_title(metric.capitalize(), fontsize=12, fontweight=\"bold\")\n",
    "            if m_idx == 0:\n",
    "                ax.set_ylabel(cname, fontsize=11, fontweight=\"bold\")\n",
    "            if c_idx == n_classes - 1:\n",
    "                ax.set_xlabel(\"# pretrain source tasks\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"Per-class Metrics vs Progressive Pretrain Stages{title_suffix}\",\n",
    "        fontsize=14, fontweight=\"bold\", y=1.01,\n",
    "    )\n",
    "    fig.tight_layout()\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    plt.show()\n",
    "    return arr\n",
    "\n",
    "\n",
    "print(\"plot_per_class_metrics() defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading reports from: {OUTPUT_DIR}\\n\")\n",
    "\n",
    "# 1) ALL test\n",
    "reports_all = load_clf_reports_from_disk(OUTPUT_DIR, \"clf_report_all.json\")\n",
    "print(f\"Found {len(reports_all)} run(s) with clf_report_all\")\n",
    "arr_all = plot_per_class_metrics(\n",
    "    reports_all, CLASS_NAMES, n_stages,\n",
    "    title_suffix=\" (ALL test)\",\n",
    "    save_path=OUTPUT_DIR / \"per_class_metrics_all_test.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e035331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) n_elements >= 4\n",
    "reports_ge4 = load_clf_reports_from_disk(OUTPUT_DIR, \"clf_report_ge4.json\")\n",
    "print(f\"Found {len(reports_ge4)} run(s) with clf_report_ge4\")\n",
    "arr_ge4 = plot_per_class_metrics(\n",
    "    reports_ge4, CLASS_NAMES, n_stages,\n",
    "    title_suffix=\" (n_elements ≥ 4)\",\n",
    "    save_path=OUTPUT_DIR / \"per_class_metrics_ge4_test.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526365c",
   "metadata": {},
   "source": [
    "## 7. 数值总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74466aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_of_interest = [\"precision\", \"recall\", \"f1-score\"]\n",
    "\n",
    "print(\"=== Per-class Metrics Summary (ALL test) ===\")\n",
    "for c_idx, cname in enumerate(CLASS_NAMES):\n",
    "    print(f\"\\n  {cname}:\")\n",
    "    for m_idx, metric in enumerate(metrics_of_interest):\n",
    "        vals = arr_all[:, :, c_idx, m_idx]  # [n_runs, n_stages]\n",
    "        mean_v = np.nanmean(vals, axis=0)\n",
    "        std_v = np.nanstd(vals, axis=0)\n",
    "        line = \"    \" + metric.ljust(12)\n",
    "        for s in range(n_stages):\n",
    "            line += f\"  S{s+1}: {mean_v[s]:.3f}±{std_v[s]:.3f}\"\n",
    "        print(line)\n",
    "\n",
    "print(\"\\n\\n=== Per-class Metrics Summary (n_elements ≥ 4) ===\")\n",
    "for c_idx, cname in enumerate(CLASS_NAMES):\n",
    "    print(f\"\\n  {cname}:\")\n",
    "    for m_idx, metric in enumerate(metrics_of_interest):\n",
    "        vals = arr_ge4[:, :, c_idx, m_idx]\n",
    "        mean_v = np.nanmean(vals, axis=0)\n",
    "        std_v = np.nanstd(vals, axis=0)\n",
    "        line = \"    \" + metric.ljust(12)\n",
    "        for s in range(n_stages):\n",
    "            line += f\"  S{s+1}: {mean_v[s]:.3f}±{std_v[s]:.3f}\"\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
