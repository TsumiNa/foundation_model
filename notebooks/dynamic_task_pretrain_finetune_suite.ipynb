{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain & PI Finetuning Suite\n",
    "\n",
    "This notebook orchestrates 20 randomized incremental pretrain runs on non-PI polymers followed by PI-property finetuning with a frozen shared encoder. It combines the continual-task recipes from `dynamic_task_finetuning_demo.ipynb` and `dynamic_task_incremental_finetuning.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "- **Descriptors**: `data/amorphous_polymer_FFDescriptor_20250730.parquet`\n",
    "- **Non-PI properties**: `data/amorphous_polymer_non_PI_properties_20250730.parquet`\n",
    "- **PI properties**: `data/amorphous_polymer_PI_properties_20250730.parquet`\n",
    "- Pretrain tasks: 15 properties (density through thermal_diffusivity) sampled in random order per run\n",
    "- PI finetune tasks: density, Rg, r2, self-diffusion, Cp, Cv, linear_expansion, refractive_index, tg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-31 10:10:53.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__init__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mLoguru logger initialized for foundation_model package.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger\n",
    "from loguru import logger as fm_logger\n",
    "\n",
    "from foundation_model.data.datamodule import CompoundDataModule\n",
    "from foundation_model.models.flexible_multi_task_model import FlexibleMultiTaskModel\n",
    "from foundation_model.models.model_config import OptimizerConfig, RegressionTaskConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "DESCRIPTOR_PATH = DATA_DIR / \"amorphous_polymer_FFDescriptor_20250730.parquet\"\n",
    "NON_PI_PATH = DATA_DIR / \"amorphous_polymer_non_PI_properties_20250730.parquet\"\n",
    "PI_PATH = DATA_DIR / \"amorphous_polymer_PI_properties_20250730.parquet\"\n",
    "SCALER_PATH = DATA_DIR / \"amorphous_polymer_properties_scaler_20250730.pkl.z\"\n",
    "\n",
    "USE_NORMALIZED_TARGETS = False\n",
    "FINETUNE_FREEZE_SHARED = True\n",
    "QUIET_MODEL_LOGGING = True\n",
    "\n",
    "PRETRAIN_TASK_NAMES = [\n",
    "    \"density\",\n",
    "    \"Rg\",\n",
    "    \"r2\",\n",
    "    # \"self-diffusion\",\n",
    "    # \"Cp\",\n",
    "    # \"Cv\",\n",
    "    # \"bulk_modulus\",\n",
    "    # \"volume_expansion\",\n",
    "    # \"linear_expansion\",\n",
    "    # \"static_dielectric_const\",\n",
    "    # \"dielectric_const_dc\",\n",
    "    # \"refractive_index\",\n",
    "    # \"tg\",\n",
    "    # \"thermal_conductivity\",\n",
    "    # \"thermal_diffusivity\",\n",
    "]\n",
    "FINETUNE_TASK_NAMES = [\n",
    "    # \"density\",\n",
    "    # \"Rg\",\n",
    "    # \"r2\",\n",
    "    # \"self-diffusion\",\n",
    "    # \"Cp\",\n",
    "    # \"Cv\",\n",
    "    \"linear_expansion\",\n",
    "    \"refractive_index\",\n",
    "    \"tg\",\n",
    "]\n",
    "\n",
    "LOWER_CASE_PROPERTIES = sorted(set(PRETRAIN_TASK_NAMES) | set(FINETUNE_TASK_NAMES))\n",
    "\n",
    "def target_column(property_name: str) -> str:\n",
    "    return f\"{property_name}{'(normalized)' if USE_NORMALIZED_TARGETS else ''}\"\n",
    "\n",
    "TARGET_COLUMNS = {name: target_column(name) for name in LOWER_CASE_PROPERTIES}\n",
    "PRETRAIN_TARGET_COLUMNS = {name: TARGET_COLUMNS[name] for name in PRETRAIN_TASK_NAMES}\n",
    "FINETUNE_TARGET_COLUMNS = {name: TARGET_COLUMNS[name] for name in FINETUNE_TASK_NAMES}\n",
    "\n",
    "SHARED_BLOCK_DIMS = [190, 256, 128]\n",
    "HEAD_HIDDEN = 64\n",
    "ARTIFACT_ROOT = Path(\"../artifacts/polymers_pretrain_finetune_runs\")\n",
    "ARTIFACT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# NUM_PRETRAIN_RUNS = 20\n",
    "# PRETRAIN_MAX_EPOCHS = 100\n",
    "# FINETUNE_MAX_EPOCHS = 60\n",
    "NUM_PRETRAIN_RUNS = 2\n",
    "PRETRAIN_MAX_EPOCHS = 10\n",
    "FINETUNE_MAX_EPOCHS = 10\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 0\n",
    "LOG_EVERY_N_STEPS = 20\n",
    "RANDOM_SEED_BASE = 1729\n",
    "\n",
    "PRETRAIN_SAMPLE = None  # Set to an int for smoke tests\n",
    "PI_SAMPLE = None  # Set to an int for smoke tests\n",
    "\n",
    "PROPERTY_SCALERS: dict[str, Any] = {}\n",
    "\n",
    "if QUIET_MODEL_LOGGING:\n",
    "    fm_logger.disable(\"foundation_model\")\n",
    "else:\n",
    "    fm_logger.enable(\"foundation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65832084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain feature matrix: (71725, 190)\n",
      "Pretrain target matrix: (71725, 3)\n",
      "PI feature matrix: (1083, 190)\n",
      "PI target matrix: (1083, 3)\n"
     ]
    }
   ],
   "source": [
    "descriptor_df = pd.read_parquet(DESCRIPTOR_PATH)\n",
    "non_pi_df = pd.read_parquet(NON_PI_PATH)\n",
    "pi_df = pd.read_parquet(PI_PATH)\n",
    "\n",
    "if USE_NORMALIZED_TARGETS:\n",
    "    if not SCALER_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Missing scaler file: {SCALER_PATH}\")\n",
    "    PROPERTY_SCALERS = joblib.load(SCALER_PATH)\n",
    "    missing_scalers = [name for name in LOWER_CASE_PROPERTIES if name not in PROPERTY_SCALERS]\n",
    "    if missing_scalers:\n",
    "        raise KeyError(f\"Scaler missing entries for: {missing_scalers}\")\n",
    "else:\n",
    "    PROPERTY_SCALERS = {}\n",
    "\n",
    "missing_pretrain = [PRETRAIN_TARGET_COLUMNS[name] for name in PRETRAIN_TASK_NAMES if PRETRAIN_TARGET_COLUMNS[name] not in non_pi_df.columns]\n",
    "if missing_pretrain:\n",
    "    raise KeyError(f\"Non-PI table missing columns: {missing_pretrain}\")\n",
    "\n",
    "missing_finetune = [name for name in FINETUNE_TASK_NAMES if FINETUNE_TARGET_COLUMNS[name] not in pi_df.columns]\n",
    "if missing_finetune:\n",
    "    print(f\"Warning: PI table missing columns for tasks: {missing_finetune}. They will be skipped.\")\n",
    "available_finetune_tasks = [name for name in FINETUNE_TASK_NAMES if name not in missing_finetune]\n",
    "if not available_finetune_tasks:\n",
    "    raise ValueError(\"No PI finetune tasks remain after filtering missing columns.\")\n",
    "original_finetune_columns = FINETUNE_TARGET_COLUMNS\n",
    "FINETUNE_TASK_NAMES = available_finetune_tasks\n",
    "FINETUNE_TARGET_COLUMNS = {name: original_finetune_columns[name] for name in FINETUNE_TASK_NAMES}\n",
    "\n",
    "common_non_pi_index = descriptor_df.index.intersection(non_pi_df.index)\n",
    "pretrain_features = descriptor_df.loc[common_non_pi_index]\n",
    "pretrain_targets = non_pi_df.loc[common_non_pi_index, [PRETRAIN_TARGET_COLUMNS[name] for name in PRETRAIN_TASK_NAMES]]\n",
    "\n",
    "if PRETRAIN_SAMPLE is not None and PRETRAIN_SAMPLE < len(pretrain_features):\n",
    "    pretrain_features = pretrain_features.sample(n=PRETRAIN_SAMPLE, random_state=42)\n",
    "    pretrain_targets = pretrain_targets.loc[pretrain_features.index]\n",
    "\n",
    "common_pi_index = descriptor_df.index.intersection(pi_df.index)\n",
    "pi_features = descriptor_df.loc[common_pi_index]\n",
    "pi_targets = pi_df.loc[common_pi_index, [FINETUNE_TARGET_COLUMNS[name] for name in FINETUNE_TASK_NAMES]]\n",
    "\n",
    "if PI_SAMPLE is not None and PI_SAMPLE < len(pi_features):\n",
    "    pi_features = pi_features.sample(n=PI_SAMPLE, random_state=13)\n",
    "    pi_targets = pi_targets.loc[pi_features.index]\n",
    "\n",
    "print(f\"Pretrain feature matrix: {pretrain_features.shape}\")\n",
    "print(f\"Pretrain target matrix: {pretrain_targets.shape}\")\n",
    "print(f\"PI feature matrix: {pi_features.shape}\")\n",
    "print(f\"PI target matrix: {pi_targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c5945",
   "metadata": {},
   "source": [
    "## Helper Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f265d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_slug(name: str) -> str:\n",
    "    slug = re.sub(r\"[^a-z0-9]+\", \"_\", name.lower()).strip(\"_\")\n",
    "    return slug or \"task\"\n",
    "\n",
    "def maybe_inverse_transform(property_name: str, values: np.ndarray) -> np.ndarray:\n",
    "    if not USE_NORMALIZED_TARGETS:\n",
    "        return values\n",
    "    scaler = PROPERTY_SCALERS.get(property_name)\n",
    "    if scaler is None:\n",
    "        raise KeyError(f\"Scaler not found for property '{property_name}'\")\n",
    "    reshaped = values.reshape(-1, 1)\n",
    "    restored = scaler.inverse_transform(reshaped)\n",
    "    return np.asarray(restored).reshape(-1)\n",
    "\n",
    "def build_regression_task(name: str, column: str) -> RegressionTaskConfig:\n",
    "    return RegressionTaskConfig(\n",
    "        name=name,\n",
    "        data_column=column,\n",
    "        dims=[SHARED_BLOCK_DIMS[-1], HEAD_HIDDEN, 1],\n",
    "        norm=True,\n",
    "        residual=False,\n",
    "    )\n",
    "\n",
    "def make_pretrain_task_configs(task_names: list[str]) -> list[RegressionTaskConfig]:\n",
    "    return [build_regression_task(name, PRETRAIN_TARGET_COLUMNS[name]) for name in task_names]\n",
    "\n",
    "def make_finetune_task_config(task_name: str) -> RegressionTaskConfig:\n",
    "    return build_regression_task(task_name, FINETUNE_TARGET_COLUMNS[task_name])\n",
    "\n",
    "def build_pretrain_datamodule(task_names: list[str], *, batch_size: int = BATCH_SIZE) -> CompoundDataModule:\n",
    "    stage_targets = pretrain_targets.loc[:, [PRETRAIN_TARGET_COLUMNS[name] for name in task_names]]\n",
    "    return CompoundDataModule(\n",
    "        formula_desc_source=pretrain_features,\n",
    "        attributes_source=stage_targets,\n",
    "        task_configs=make_pretrain_task_configs(task_names),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "def build_finetune_datamodule(task_name: str, *, batch_size: int = BATCH_SIZE) -> CompoundDataModule:\n",
    "    target_frame = pi_targets.loc[:, [FINETUNE_TARGET_COLUMNS[task_name]]]\n",
    "    task_config = make_finetune_task_config(task_name)\n",
    "    return CompoundDataModule(\n",
    "        formula_desc_source=pi_features,\n",
    "        attributes_source=target_frame,\n",
    "        task_configs=[task_config],\n",
    "        batch_size=batch_size,\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "def plot_test_predictions(\n",
    "    *,\n",
    "    model: FlexibleMultiTaskModel,\n",
    "    datamodule: CompoundDataModule,\n",
    "    phase: str,\n",
    "    run_id: int,\n",
    "    stage_num: int | None,\n",
    "    stage_tasks: list[str],\n",
    "    new_task_name: str,\n",
    "    output_dir: Path | str,\n",
    ") -> None:\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    metrics_path = output_dir / \"metrics.json\"\n",
    "    predictions_path = output_dir / \"predictions.parquet\"\n",
    "    task_order_path = output_dir / \"tasks.txt\"\n",
    "    task_order_path.write_text(\" -> \".join(stage_tasks) + \"\", encoding=\"utf-8\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    datamodule.setup(stage=\"test\")\n",
    "    test_loader = datamodule.test_dataloader()\n",
    "    if test_loader is None:\n",
    "        raise RuntimeError(f\"{phase} stage {stage_num} has no test dataloader\")\n",
    "\n",
    "    original_device = next(model.parameters()).device\n",
    "    was_training = model.training\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    aggregated: dict[str, dict[str, list[np.ndarray]]] = {}\n",
    "    prediction_rows: list[dict[str, float | int | str | None]] = []\n",
    "    per_task_counts: dict[str, int] = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, y_dict, mask_dict, t_sequences = batch\n",
    "            x = x.to(device)\n",
    "            preds = model(x, t_sequences)\n",
    "\n",
    "            for name, pred_tensor in preds.items():\n",
    "                if name not in y_dict:\n",
    "                    continue\n",
    "\n",
    "                target_tensor = y_dict[name]\n",
    "                mask_tensor = mask_dict.get(name)\n",
    "\n",
    "                if isinstance(target_tensor, list):\n",
    "                    target_flat = torch.cat([t.detach().cpu().reshape(-1) for t in target_tensor])\n",
    "                else:\n",
    "                    target_flat = target_tensor.detach().cpu().reshape(-1)\n",
    "\n",
    "                pred_flat = pred_tensor.detach().cpu().reshape(-1)\n",
    "\n",
    "                if mask_tensor is not None:\n",
    "                    if isinstance(mask_tensor, list):\n",
    "                        mask_flat = torch.cat([m.detach().cpu().reshape(-1) for m in mask_tensor])\n",
    "                    else:\n",
    "                        mask_flat = mask_tensor.detach().cpu().reshape(-1)\n",
    "                    mask_flat = mask_flat.bool()\n",
    "                    target_flat = target_flat[mask_flat]\n",
    "                    pred_flat = pred_flat[mask_flat]\n",
    "\n",
    "                if target_flat.numel() == 0:\n",
    "                    continue\n",
    "\n",
    "                target_np = target_flat.numpy()\n",
    "                pred_np = pred_flat.numpy()\n",
    "                target_np = maybe_inverse_transform(name, target_np)\n",
    "                pred_np = maybe_inverse_transform(name, pred_np)\n",
    "\n",
    "                entry = aggregated.setdefault(name, {\"preds\": [], \"targets\": []})\n",
    "                entry[\"preds\"].append(pred_np)\n",
    "                entry[\"targets\"].append(target_np)\n",
    "\n",
    "                start_idx = per_task_counts.get(name, 0)\n",
    "                for offset, (actual_val, pred_val) in enumerate(zip(target_np.tolist(), pred_np.tolist())):\n",
    "                    prediction_rows.append(\n",
    "                        {\n",
    "                            \"run\": run_id,\n",
    "                            \"phase\": phase,\n",
    "                            \"stage\": stage_num,\n",
    "                            \"task\": name,\n",
    "                            \"sample_index\": start_idx + offset,\n",
    "                            \"actual\": actual_val,\n",
    "                            \"predicted\": pred_val,\n",
    "                        }\n",
    "                    )\n",
    "                per_task_counts[name] = start_idx + len(target_np)\n",
    "\n",
    "    if not aggregated:\n",
    "        print(f\"No predictions to log for run {run_id} stage {stage_num} ({phase}).\")\n",
    "        model.to(original_device)\n",
    "        if was_training:\n",
    "            model.train()\n",
    "        return\n",
    "\n",
    "    metrics: dict[str, dict[str, float | int | None]] = {}\n",
    "\n",
    "    for name in stage_tasks:\n",
    "        if name not in aggregated:\n",
    "            continue\n",
    "        preds = np.concatenate(aggregated[name][\"preds\"])\n",
    "        targets = np.concatenate(aggregated[name][\"targets\"])\n",
    "        diff = preds - targets\n",
    "        mae = float(np.mean(np.abs(diff)))\n",
    "        mse = float(np.mean(diff ** 2))\n",
    "        rmse = float(np.sqrt(np.mean(diff ** 2)))\n",
    "        ss_tot = float(np.sum((targets - np.mean(targets)) ** 2))\n",
    "        ss_res = float(np.sum(diff ** 2))\n",
    "        r2_value = 1.0 - ss_res / ss_tot if ss_tot > 0 else None\n",
    "        metrics[name] = {\n",
    "            \"samples\": int(targets.size),\n",
    "            \"mae\": mae,\n",
    "            \"mse\": mse,\n",
    "            \"rmse\": rmse,\n",
    "            \"r2\": r2_value,\n",
    "        }\n",
    "\n",
    "        lo = float(min(preds.min(), targets.min()))\n",
    "        hi = float(max(preds.max(), targets.max()))\n",
    "        buffer = 0.05 * (hi - lo) if hi > lo else 0.1\n",
    "        lo -= buffer\n",
    "        hi += buffer\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(9, 9))\n",
    "        ax.scatter(targets, preds, s=14, alpha=0.6, edgecolors=\"none\")\n",
    "        ax.plot([lo, hi], [lo, hi], \"--\", color=\"tab:red\", linewidth=1.5)\n",
    "        annotation_lines = [\n",
    "            f\"MAE: {mae:.3f}\",\n",
    "            rf\"$R^2$: {r2_value:.3f}\" if r2_value is not None else r\"$R^2$: N/A\",\n",
    "            f\"Samples: {int(targets.size):,}\",\n",
    "        ]\n",
    "        ax.text(\n",
    "            0.05,\n",
    "            0.95,\n",
    "            \"\\n\".join(annotation_lines),\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=13,\n",
    "            verticalalignment=\"top\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"white\", alpha=0.7),\n",
    "        )\n",
    "        ax.set_xlim(lo, hi)\n",
    "        ax.set_ylim(lo, hi)\n",
    "        ax.set_xlabel(\"Actual\")\n",
    "        ax.set_ylabel(\"Predicted\")\n",
    "        if phase == \"pretrain\" and stage_num is not None:\n",
    "            title_prefix = f\"Pretrain Stage {stage_num}\"\n",
    "        else:\n",
    "            title_prefix = \"Finetune\"\n",
    "        ax.set_title(f\"{title_prefix}: {name}\")\n",
    "        ax.grid(alpha=0.25)\n",
    "        ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(output_dir / f\"{safe_slug(name)}_pred.png\", dpi=100)\n",
    "        plt.close(fig)\n",
    "\n",
    "    metrics_payload = {\n",
    "        \"run_id\": run_id,\n",
    "        \"phase\": phase,\n",
    "        \"stage\": stage_num,\n",
    "        \"new_task\": new_task_name,\n",
    "        \"task_sequence\": list(stage_tasks),\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "\n",
    "    if prediction_rows:\n",
    "        pd.DataFrame(prediction_rows).to_parquet(predictions_path, index=False)\n",
    "        print(f\"Saved predictions to {predictions_path}\")\n",
    "\n",
    "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics_payload, f, indent=2)\n",
    "    print(f\"Saved metrics to {metrics_path}\")\n",
    "\n",
    "    model.to(original_device)\n",
    "    if was_training:\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c312b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.serialization.add_safe_globals([RegressionTaskConfig, TaskType, OptimizerConfig])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef7ae0",
   "metadata": {},
   "source": [
    "## Pretrain & Finetune Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e909ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "Starting run01\n",
      "Task order: ['density', 'Rg', 'r2']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 1      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 8.4 K  | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "107 K     Trainable params\n",
      "0         Non-trainable params\n",
      "107 K     Total params\n",
      "0.430     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:01<00:00, 119.32it/s, v_num=0, train_final_loss_step=-2.43, val_final_loss=-2.43, train_final_loss_epoch=-2.38] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:01<00:00, 118.09it/s, v_num=0, train_final_loss_step=-2.43, val_final_loss=-2.43, train_final_loss_epoch=-2.38]\n",
      "Run run01 stage 1: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run01/pretrain_stage01_density/checkpoints/density-epoch=09-val_final_loss=-2.4284.ckpt\n",
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run01/pretrain_stage01_density/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run01/pretrain_stage01_density/prediction/metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 2      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 16.9 K | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "115 K     Trainable params\n",
      "0         Non-trainable params\n",
      "115 K     Total params\n",
      "0.464     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:02<00:00, 79.25it/s, v_num=0, train_final_loss_step=8.010, val_final_loss=1.330, train_final_loss_epoch=1.340]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:02<00:00, 79.12it/s, v_num=0, train_final_loss_step=8.010, val_final_loss=1.330, train_final_loss_epoch=1.340]\n",
      "Run run01 stage 2: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run01/pretrain_stage02_rg/checkpoints/rg-epoch=08-val_final_loss=1.3182.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run01/pretrain_stage02_rg/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run01/pretrain_stage02_rg/prediction/metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 3      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 25.3 K | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "124 K     Trainable params\n",
      "0         Non-trainable params\n",
      "124 K     Total params\n",
      "0.498     Total estimated model params size (MB)\n",
      "46        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:03<00:00, 61.00it/s, v_num=0, train_final_loss_step=135.0, val_final_loss=314.0, train_final_loss_epoch=301.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:03<00:00, 60.49it/s, v_num=0, train_final_loss_step=135.0, val_final_loss=314.0, train_final_loss_epoch=301.0]\n",
      "Run run01 stage 3: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run01/pretrain_stage03_r2/checkpoints/r2-epoch=09-val_final_loss=314.0172.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 1      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 8.4 K  | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "8.4 K     Trainable params\n",
      "99.1 K    Non-trainable params\n",
      "107 K     Total params\n",
      "0.430     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run01/pretrain_stage03_r2/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run01/pretrain_stage03_r2/prediction/metrics.json\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=20). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 92.74it/s, v_num=0, train_final_loss_step=-0.39, val_final_loss=-0.40, train_final_loss_epoch=-0.372]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 69.33it/s, v_num=0, train_final_loss_step=-0.39, val_final_loss=-0.40, train_final_loss_epoch=-0.372]\n",
      "Run run01 finetune linear_expansion: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run01/finetune_linear_expansion/checkpoints/linear_expansion-epoch=09-val_final_loss=-0.4002.ckpt\n",
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run01/finetune_linear_expansion/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run01/finetune_linear_expansion/prediction/metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 1      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 8.4 K  | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "8.4 K     Trainable params\n",
      "99.1 K    Non-trainable params\n",
      "107 K     Total params\n",
      "0.430     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=20). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 96.01it/s, v_num=0, train_final_loss_step=-0.122, val_final_loss=-0.141, train_final_loss_epoch=-0.109]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 72.54it/s, v_num=0, train_final_loss_step=-0.122, val_final_loss=-0.141, train_final_loss_epoch=-0.109]\n",
      "Run run01 finetune refractive_index: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run01/finetune_refractive_index/checkpoints/refractive_index-epoch=09-val_final_loss=-0.1414.ckpt\n",
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run01/finetune_refractive_index/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run01/finetune_refractive_index/prediction/metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 1      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 8.4 K  | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "8.4 K     Trainable params\n",
      "99.1 K    Non-trainable params\n",
      "107 K     Total params\n",
      "0.430     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=20). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 102.72it/s, v_num=0, train_final_loss_step=1.57e+5, val_final_loss=8.84e+5, train_final_loss_epoch=3.44e+6]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 74.72it/s, v_num=0, train_final_loss_step=1.57e+5, val_final_loss=8.84e+5, train_final_loss_epoch=3.44e+6] \n",
      "Run run01 finetune tg: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run01/finetune_tg/checkpoints/tg-epoch=09-val_final_loss=883993.6250.ckpt\n",
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run01/finetune_tg/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run01/finetune_tg/prediction/metrics.json\n",
      "\n",
      "====================\n",
      "Starting run02\n",
      "Task order: ['Rg', 'density', 'r2']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 1      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 8.4 K  | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "107 K     Trainable params\n",
      "0         Non-trainable params\n",
      "107 K     Total params\n",
      "0.430     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:01<00:00, 116.20it/s, v_num=0, train_final_loss_step=1.730, val_final_loss=3.510, train_final_loss_epoch=3.420]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:01<00:00, 114.99it/s, v_num=0, train_final_loss_step=1.730, val_final_loss=3.510, train_final_loss_epoch=3.420]\n",
      "Run run02 stage 1: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run02/pretrain_stage01_rg/checkpoints/rg-epoch=09-val_final_loss=3.5054.ckpt\n",
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run02/pretrain_stage01_rg/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run02/pretrain_stage01_rg/prediction/metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 2      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 16.9 K | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "115 K     Trainable params\n",
      "0         Non-trainable params\n",
      "115 K     Total params\n",
      "0.464     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:02<00:00, 79.41it/s, v_num=0, train_final_loss_step=1.220, val_final_loss=1.860, train_final_loss_epoch=1.820]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:02<00:00, 78.67it/s, v_num=0, train_final_loss_step=1.220, val_final_loss=1.860, train_final_loss_epoch=1.820]\n",
      "Run run02 stage 2: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run02/pretrain_stage02_density/checkpoints/density-epoch=09-val_final_loss=1.8555.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run02/pretrain_stage02_density/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run02/pretrain_stage02_density/prediction/metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 3      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 25.3 K | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "124 K     Trainable params\n",
      "0         Non-trainable params\n",
      "124 K     Total params\n",
      "0.498     Total estimated model params size (MB)\n",
      "46        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:03<00:00, 58.78it/s, v_num=0, train_final_loss_step=130.0, val_final_loss=316.0, train_final_loss_epoch=297.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 225/225 [00:03<00:00, 58.29it/s, v_num=0, train_final_loss_step=130.0, val_final_loss=316.0, train_final_loss_epoch=297.0]\n",
      "Run run02 stage 3: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run02/pretrain_stage03_r2/checkpoints/r2-epoch=09-val_final_loss=316.0619.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 1      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 8.4 K  | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "8.4 K     Trainable params\n",
      "99.1 K    Non-trainable params\n",
      "107 K     Total params\n",
      "0.430     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run02/pretrain_stage03_r2/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run02/pretrain_stage03_r2/prediction/metrics.json\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=20). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 102.73it/s, v_num=0, train_final_loss_step=-0.391, val_final_loss=-0.40, train_final_loss_epoch=-0.373]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 75.13it/s, v_num=0, train_final_loss_step=-0.391, val_final_loss=-0.40, train_final_loss_epoch=-0.373] \n",
      "Run run02 finetune linear_expansion: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run02/finetune_linear_expansion/checkpoints/linear_expansion-epoch=09-val_final_loss=-0.4005.ckpt\n",
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run02/finetune_linear_expansion/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run02/finetune_linear_expansion/prediction/metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 1      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 8.4 K  | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "8.4 K     Trainable params\n",
      "99.1 K    Non-trainable params\n",
      "107 K     Total params\n",
      "0.430     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=20). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 91.72it/s, v_num=0, train_final_loss_step=-0.271, val_final_loss=-0.244, train_final_loss_epoch=-0.257]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 67.19it/s, v_num=0, train_final_loss_step=-0.271, val_final_loss=-0.244, train_final_loss_epoch=-0.257]\n",
      "Run run02 finetune refractive_index: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run02/finetune_refractive_index/checkpoints/refractive_index-epoch=09-val_final_loss=-0.2442.ckpt\n",
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run02/finetune_refractive_index/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run02/finetune_refractive_index/prediction/metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "\n",
      "  | Name                | Type              | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | task_log_sigmas     | ParameterDict     | 1      | train\n",
      "1 | encoder             | FoundationEncoder | 99.1 K | train\n",
      "2 | shared              | LinearBlock       | 82.6 K | train\n",
      "3 | deposit             | Sequential        | 16.5 K | train\n",
      "4 | task_heads          | ModuleDict        | 8.4 K  | train\n",
      "5 | disabled_task_heads | ModuleDict        | 0      | train\n",
      "------------------------------------------------------------------\n",
      "8.4 K     Trainable params\n",
      "99.1 K    Non-trainable params\n",
      "107 K     Total params\n",
      "0.430     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/liuchang/projects/foundation_model/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=20). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 97.29it/s, v_num=0, train_final_loss_step=1.98e+5, val_final_loss=9.3e+5, train_final_loss_epoch=3.62e+6]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 70.24it/s, v_num=0, train_final_loss_step=1.98e+5, val_final_loss=9.3e+5, train_final_loss_epoch=3.62e+6]\n",
      "Run run02 finetune tg: best checkpoint -> /Users/liuchang/projects/foundation_model/artifacts/polymers_pretrain_finetune_runs/run02/finetune_tg/checkpoints/tg-epoch=09-val_final_loss=930090.2500.ckpt\n",
      "Saved predictions to ../artifacts/polymers_pretrain_finetune_runs/run02/finetune_tg/prediction/predictions.parquet\n",
      "Saved metrics to ../artifacts/polymers_pretrain_finetune_runs/run02/finetune_tg/prediction/metrics.json\n",
      "Completed all pretrain + finetune runs.\n"
     ]
    }
   ],
   "source": [
    "experiment_records: list[dict] = []\n",
    "\n",
    "for run_idx in range(1, NUM_PRETRAIN_RUNS + 1):\n",
    "    rng = random.Random(RANDOM_SEED_BASE + run_idx)\n",
    "    task_sequence = rng.sample(PRETRAIN_TASK_NAMES, k=len(PRETRAIN_TASK_NAMES))\n",
    "    run_label = f\"run{run_idx:02d}\"\n",
    "    print(f\"\"\"\n",
    "====================\n",
    "Starting {run_label}\n",
    "Task order: {task_sequence}\n",
    "====================\"\"\"\n",
    ")\n",
    "\n",
    "    run_root = ARTIFACT_ROOT / run_label\n",
    "    run_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    previous_checkpoint: str | None = None\n",
    "    pretrain_stage_records: list[dict] = []\n",
    "\n",
    "    for stage_idx, task_name in enumerate(task_sequence, start=1):\n",
    "        stage_tasks = task_sequence[:stage_idx]\n",
    "        datamodule = build_pretrain_datamodule(stage_tasks)\n",
    "        task_configs = make_pretrain_task_configs(stage_tasks)\n",
    "\n",
    "        if previous_checkpoint is None:\n",
    "            model = FlexibleMultiTaskModel(\n",
    "                shared_block_dims=SHARED_BLOCK_DIMS,\n",
    "                task_configs=task_configs,\n",
    "                enable_learnable_loss_balancer=True,\n",
    "                shared_block_optimizer=OptimizerConfig(lr=1e-2),\n",
    "            )\n",
    "        else:\n",
    "            model = FlexibleMultiTaskModel.load_from_checkpoint(\n",
    "                checkpoint_path=previous_checkpoint,\n",
    "                strict=False,\n",
    "                enable_learnable_loss_balancer=True,\n",
    "            )\n",
    "            existing = set(model.task_heads.keys())\n",
    "            new_configs = [cfg for cfg in task_configs if cfg.name not in existing]\n",
    "            if new_configs:\n",
    "                model.add_task(*new_configs)\n",
    "\n",
    "        stage_dir = run_root / f\"pretrain_stage{stage_idx:02d}_{safe_slug(task_name)}\"\n",
    "        stage_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        checkpoint_cb = ModelCheckpoint(\n",
    "            dirpath=stage_dir / \"checkpoints\",\n",
    "            filename=f\"{safe_slug(task_name)}-{{epoch:02d}}-{{val_final_loss:.4f}}\",\n",
    "            monitor=\"val_final_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "        )\n",
    "        early_stopping = EarlyStopping(monitor=\"val_final_loss\", mode=\"min\", patience=10)\n",
    "        csv_logger = CSVLogger(save_dir=stage_dir / \"logs\", name=\"csv\")\n",
    "        tensorboard_logger = TensorBoardLogger(save_dir=stage_dir / \"logs\", name=\"tensorboard\")\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=PRETRAIN_MAX_EPOCHS,\n",
    "            accelerator=\"auto\",\n",
    "            devices=\"auto\",\n",
    "            callbacks=[checkpoint_cb, early_stopping],\n",
    "            logger=[csv_logger, tensorboard_logger],\n",
    "            log_every_n_steps=LOG_EVERY_N_STEPS,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "        best_model_path = checkpoint_cb.best_model_path\n",
    "        print(f\"Run {run_label} stage {stage_idx}: best checkpoint -> {best_model_path}\")\n",
    "\n",
    "        if best_model_path:\n",
    "            state = torch.load(best_model_path, map_location=\"cpu\", weights_only=True)\n",
    "            state_dict = state.get(\"state_dict\", state)\n",
    "            model.load_state_dict(state_dict)\n",
    "            previous_checkpoint = best_model_path\n",
    "        else:\n",
    "            print(\"Warning: no best checkpoint captured; using current weights.\")\n",
    "\n",
    "        prediction_dir = stage_dir / \"prediction\"\n",
    "        plot_test_predictions(\n",
    "            model=model,\n",
    "            datamodule=datamodule,\n",
    "            phase=\"pretrain\",\n",
    "            run_id=run_idx,\n",
    "            stage_num=stage_idx,\n",
    "            stage_tasks=stage_tasks,\n",
    "            new_task_name=task_name,\n",
    "            output_dir=prediction_dir,\n",
    "        )\n",
    "\n",
    "        pretrain_stage_records.append(\n",
    "            {\n",
    "                \"stage\": stage_idx,\n",
    "                \"task_name\": task_name,\n",
    "                \"task_sequence\": list(stage_tasks),\n",
    "                \"checkpoint\": best_model_path,\n",
    "                \"stage_dir\": stage_dir,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if previous_checkpoint is None:\n",
    "        raise RuntimeError(f\"Run {run_label} produced no pretrain checkpoint; cannot finetune.\")\n",
    "\n",
    "    finetune_records: list[dict] = []\n",
    "    for task_name in FINETUNE_TASK_NAMES:\n",
    "        finetune_model = FlexibleMultiTaskModel.load_from_checkpoint(\n",
    "            checkpoint_path=previous_checkpoint,\n",
    "            strict=False,\n",
    "            enable_learnable_loss_balancer=True,\n",
    "            freeze_shared_encoder=FINETUNE_FREEZE_SHARED,\n",
    "        )\n",
    "        active_tasks = list(finetune_model.task_heads.keys())\n",
    "        if active_tasks:\n",
    "            finetune_model.remove_tasks(*active_tasks)\n",
    "\n",
    "        task_config = make_finetune_task_config(task_name)\n",
    "        finetune_model.add_task(task_config)\n",
    "\n",
    "        datamodule = build_finetune_datamodule(task_name)\n",
    "\n",
    "        stage_dir = run_root / f\"finetune_{safe_slug(task_name)}\"\n",
    "        stage_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        checkpoint_cb = ModelCheckpoint(\n",
    "            dirpath=stage_dir / \"checkpoints\",\n",
    "            filename=f\"{safe_slug(task_name)}-{{epoch:02d}}-{{val_final_loss:.4f}}\",\n",
    "            monitor=\"val_final_loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "        )\n",
    "        early_stopping = EarlyStopping(monitor=\"val_final_loss\", mode=\"min\", patience=10)\n",
    "        csv_logger = CSVLogger(save_dir=stage_dir / \"logs\", name=\"csv\")\n",
    "        tensorboard_logger = TensorBoardLogger(save_dir=stage_dir / \"logs\", name=\"tensorboard\")\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=FINETUNE_MAX_EPOCHS,\n",
    "            accelerator=\"auto\",\n",
    "            devices=\"auto\",\n",
    "            callbacks=[checkpoint_cb, early_stopping],\n",
    "            logger=[csv_logger, tensorboard_logger],\n",
    "            log_every_n_steps=LOG_EVERY_N_STEPS,\n",
    "        )\n",
    "\n",
    "        trainer.fit(finetune_model, datamodule=datamodule)\n",
    "        best_model_path = checkpoint_cb.best_model_path\n",
    "        print(f\"Run {run_label} finetune {task_name}: best checkpoint -> {best_model_path}\")\n",
    "\n",
    "        if best_model_path:\n",
    "            state = torch.load(best_model_path, map_location=\"cpu\", weights_only=True)\n",
    "            state_dict = state.get(\"state_dict\", state)\n",
    "            finetune_model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            print(\"Warning: finetune stage missing checkpoint; using current weights.\")\n",
    "\n",
    "        prediction_dir = stage_dir / \"prediction\"\n",
    "        plot_test_predictions(\n",
    "            model=finetune_model,\n",
    "            datamodule=datamodule,\n",
    "            phase=\"finetune\",\n",
    "            run_id=run_idx,\n",
    "            stage_num=None,\n",
    "            stage_tasks=[task_name],\n",
    "            new_task_name=task_name,\n",
    "            output_dir=prediction_dir,\n",
    "        )\n",
    "\n",
    "        finetune_records.append(\n",
    "            {\n",
    "                \"task_name\": task_name,\n",
    "                \"checkpoint\": best_model_path,\n",
    "                \"stage_dir\": stage_dir,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    experiment_records.append(\n",
    "        {\n",
    "            \"run\": run_label,\n",
    "            \"task_sequence\": task_sequence,\n",
    "            \"pretrain\": pretrain_stage_records,\n",
    "            \"pretrain_checkpoint\": previous_checkpoint,\n",
    "            \"finetune\": finetune_records,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Completed all pretrain + finetune runs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recorded 2 runs.\n",
      "run01 pretrain stages: 3 finetune stages: 3\n",
      "run02 pretrain stages: 3 finetune stages: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recorded {len(experiment_records)} runs.\")\n",
    "for record in experiment_records:\n",
    "    print(record[\"run\"], \"pretrain stages:\", len(record[\"pretrain\"]), \"finetune stages:\", len(record[\"finetune\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f78c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
