#!/bin/bash
# Copyright 2025 TsumiNa.
# SPDX-License-Identifier: Apache-2.0

# --- CLI Example 2: Training with Config File and CLI Overrides ---
#
# This script demonstrates how to use a base model configuration file
# and override specific parameters directly from the command line.
# This is useful for quick experiments or hyperparameter tuning.
#
# Prerequisites:
# 1. Fake data must be generated:
#    python samples/helper_tools/fake_data_generator.py
# 2. A model configuration must be generated (this will be our base):
#    python samples/helper_tools/config_generator.py \
#      --attributes_csv samples/fake_data/attributes.csv \
#      --formula_features_csv samples/fake_data/formula_features.csv \
#      --output_config samples/generated_configs/default_config.yaml
#
# Ensure your Python environment and the foundation_model package are active.

# --- Configuration ---
# Path to the base model configuration file
BASE_CONFIG_FILE="samples/generated_configs/default_config.yaml"

# Directory to store logs for this specific run
LOG_DIR_BASE="samples/example_logs/override_run"
EXPERIMENT_NAME="override_experiment_$(date +%Y%m%d_%H%M%S)"
LOG_DIR="${LOG_DIR_BASE}/${EXPERIMENT_NAME}"

# --- Script Execution ---
echo "--------------------------------------------------"
echo "Starting Training Run with CLI Overrides"
echo "--------------------------------------------------"
echo "Using Base Config File: ${BASE_CONFIG_FILE}"
echo "Logging to: ${LOG_DIR}"

# Check if base config file exists
if [ ! -f "$BASE_CONFIG_FILE" ]; then
    echo "Error: Base configuration file '$BASE_CONFIG_FILE' not found."
    echo "Please run the prerequisite generation scripts first."
    exit 1
fi

# Create log directory
mkdir -p "$LOG_DIR"

# Execute the training script with overrides
# The syntax for overriding parameters depends on the CLI parser (e.g., jsonargparse).
# Common convention is dot-notation for nested parameters and list indices.
#
# Example Overrides (these assume tasks exist in the generated config):
# - Change learning rate for the first task's optimizer.
# - Change the weight for the second task.
# - Reduce the maximum number of training epochs.
#
# IMPORTANT: The indices for `model.task_configs[0]` and `model.task_configs[1]`
# depend on the order of tasks generated by `config_generator.py` based on
# your `attributes.csv` columns. Adjust if necessary.
# If `default_config.yaml` has tasks named 'regression_1', 'regression_2', etc.,
# you might be able to use `model.task_configs[name=regression_1].optimizer.lr=...`
# if your LightningCLI/jsonargparse setup supports named access in lists.
# For simplicity, we use indices here.

python -m foundation_model.scripts.train fit \
    --config "$BASE_CONFIG_FILE" \
    model.task_configs[0].optimizer.lr=0.0005 \
    model.task_configs[1].weight=0.75 \
    trainer.max_epochs=5 \
    trainer.logger.save_dir="$LOG_DIR" \
    trainer.logger.name="" \
    trainer.callbacks[0].dirpath="$LOG_DIR/checkpoints/" \
    experiment_name="$EXPERIMENT_NAME"

echo "--------------------------------------------------"
echo "Override Training Run Command Executed."
echo "Check logs and results in: ${LOG_DIR}"
echo "Review the hparams.yaml in the log directory to see the applied overrides."
echo "--------------------------------------------------"

# Note:
# - The specific override paths (e.g., `model.task_configs[0].optimizer.lr`) must match
#   the structure of your `default_config.yaml`.
# - If a task doesn't exist at the specified index, the CLI might error or ignore the override.
# - This example sets `trainer.max_epochs=5` for a quick run. Adjust as needed.
