# experiment_name: base_experiment # This can be overridden by CLI or defaults if not set
# log_dir: results/logs/${experiment_name} # Replaced by trainer.default_root_dir
seed_everything: 42
model:
  class_path: foundation_model.models.FlexibleMultiTaskModel
  init_args:
    enable_learnable_loss_balancer: true
    shared_block_dims: [290, 128]
    task_configs:
      - name: density
        type: REGRESSION
        data_column: Density (normalized)
        dims: [128, 64, 32, 1]
        norm: true
        residual: false
        weight: 1.0
        enabled: true
        optimizer:
          optimizer_type: AdamW
          lr: 0.01
          weight_decay: 0.01
          eps: 1.0e-06
          betas: [0.9, 0.999]
          scheduler_type: ReduceLROnPlateau
          mode: min
          factor: 0.5
          patience: 10
          min_lr: 1.0e-05

      - name: formation_energy # Note: Duplicate task name, this might be an issue or intended for some reason.
        type: REGRESSION
        data_column: Formation energy per atom (normalized) # Assuming same column for this duplicate
        dims: [128, 64, 32, 1]
        norm: true
        residual: false
        weight: 1.0
        enabled: true
        optimizer:
          optimizer_type: AdamW
          lr: 0.01
          weight_decay: 0.01
          eps: 1.0e-06
          betas: [0.9, 0.999]
          scheduler_type: ReduceLROnPlateau
          mode: min
          factor: 0.5
          patience: 10
          min_lr: 1.0e-05

      - name: volume # Note: Duplicate task name, this might be an issue or intended for some reason.
        type: REGRESSION
        data_column: Volume (normalized) # Assuming same column for this duplicate
        dims: [128, 64, 32, 1]
        norm: true
        residual: false
        weight: 1.0
        enabled: true
        optimizer:
          optimizer_type: AdamW
          lr: 0.01
          weight_decay: 0.01
          eps: 1.0e-06
          betas: [0.9, 0.999]
          scheduler_type: ReduceLROnPlateau
          mode: min
          factor: 0.5
          patience: 10
          min_lr: 1.0e-05

      # - name: dielectric_constant # Note: Duplicate task name, this might be an issue or intended for some reason.
      #   type: REGRESSION
      #   data_column: Dielectric constant (normalized) # Assuming same column for this duplicate
      #   dims: [128, 64, 32, 1]
      #   norm: true
      #   residual: false
      #   weight: 1.0
      #   enabled: true
      #   optimizer:
      #     optimizer_type: AdamW
      #     lr: 0.01
      #     weight_decay: 0.01
      #     eps: 1.0e-06
      #     betas: [0.9, 0.999]
      #     scheduler_type: ReduceLROnPlateau
      #     mode: min
      #     factor: 0.5
      #     patience: 10
      #     min_lr: 1.0e-05

      # - name: hypermaterial_type # Note: Duplicate task name, this might be an issue or intended for some reason.
      #   type: CLASSIFICATION
      #   data_column: hypermaterial_type_label # Assuming same column for this duplicate
      #   dims: [128, 64, 32, 5]
      #   num_classes: 5
      #   norm: true
      #   residual: false
      #   weight: 1.0
      #   enabled: true
      #   optimizer:
      #     optimizer_type: AdamW
      #     lr: 0.01
      #     weight_decay: 0.01
      #     eps: 1.0e-06
      #     betas: [0.9, 0.999]
      #     scheduler_type: ReduceLROnPlateau
      #     mode: min
      #     factor: 0.5
      #     patience: 10
      #     min_lr: 1.0e-05

      # - name: space_group # Note: Duplicate task name, this might be an issue or intended for some reason.
      #   type: CLASSIFICATION
      #   data_column: space_group_label # Assuming same column for this duplicate
      #   dims: [128, 128, 214]
      #   num_classes: 214
      #   norm: true
      #   residual: false
      #   weight: 1.0
      #   enabled: true
      #   optimizer:
      #     optimizer_type: AdamW
      #     lr: 0.01
      #     weight_decay: 0.01
      #     eps: 1.0e-06
      #     betas: [0.9, 0.999]
      #     scheduler_type: ReduceLROnPlateau
      #     mode: min
      #     factor: 0.5
      #     patience: 10
      #     min_lr: 1.0e-05

      - name: dos # Note: Duplicate task name, this might be an issue or intended for some reason.
        type: ExtendRegression
        data_column: DOS density (normalized) # Assuming same column for this duplicate
        t_column: DOS energy
        x_dim: [128, 32, 16]
        t_dim: [32, 16]
        t_encoding_method: fourier
        # t_encoding_method: fc
        norm: true
        residual: false
        weight: 1.0
        enabled: true
        optimizer:
          optimizer_type: AdamW
          lr: 1.0e-3
          weight_decay: 0.01
          eps: 1.0e-06
          betas: [0.9, 0.999]
          scheduler_type: ReduceLROnPlateau
          mode: min
          factor: 0.5
          patience: 50
          min_lr: 1.0e-05

    shared_block_optimizer:
      optimizer_type: AdamW
      lr: 0.1
      weight_decay: 0.01
      eps: 1.0e-06
      betas: [0.9, 0.999]
      scheduler_type: ReduceLROnPlateau
      mode: min
      factor: 0.1
      patience: 50
      min_lr: 1.0e-03
      monitor: val_final_loss
    with_structure: false
    modality_dropout_p: 0.3
    norm_shared: true
    residual_shared: false
    enable_self_supervised_training: false
    mask_ratio: 0.15
    temperature: 0.07
    loss_weights:
      mfm: 1.0
      contrastive: 1.0
      cross_recon: 1.0
    # lora_rank and lora_alpha are per-task configurations

data:
  class_path: foundation_model.data.CompoundDataModule
  init_args:
    formula_desc_source: /data/foundation_model/data/qc_ac_te_mp_dos_composition_desc_trans_20250615.pd.parquet
    attributes_source: /data/foundation_model/data/qc_ac_te_mp_dos_reformat_20250615.pd.parquet
    # formula_desc_source: /Users/liuchang/projects/foundation_model/data/qc_ac_te_mp_dos_composition_desc_trans_20250615.pd.parquet
    # attributes_source: /Users/liuchang/projects/foundation_model/data/qc_ac_te_mp_dos_reformat_20250615.pd.parquet
    task_configs: ${model.init_args.task_configs}
    batch_size: 256
    num_workers: 0
    predict_idx: "all"
    val_split: 0.1
    test_split: 0.1
    train_random_seed: 42
    test_random_seed: 24

trainer:
  default_root_dir: ${oc.env:LOG_DIR,results/logs/default_experiment}
  max_epochs: 10
  # devices: auto
  accelerator: auto
  devices: 1
  enable_progress_bar: true
  fast_dev_run: false
  logger: false
  callbacks:
    - class_path: foundation_model.scripts.callbacks.PredictionDataFrameWriter
      init_args:
        output_path: ${oc.env:LOG_DIR,results/logs/default_experiment}
        write_interval: "epoch"
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: ${oc.env:LOG_DIR,results/logs/default_experiment}
        filename: model-{epoch:02d}-{val_final_loss:.4f}-{step}
        monitor: val_final_loss
        mode: min
        every_n_train_steps: 0
        every_n_epochs: 1
        train_time_interval: null
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_final_loss
        mode: min
