# è‡ªåŠ¨å¾®åˆ†ä¼˜åŒ–ç®—æ³•æ ¸å¿ƒå®ç°è¯¦è§£

## ğŸ¯ æ ¸å¿ƒç®—æ³•ï¼šLatent Space Gradient Optimization

### æ•°å­¦åŸç†

**é—®é¢˜å®šä¹‰**ï¼š
```
ç»™å®šè®­ç»ƒå¥½çš„æ¨¡å‹: x â†’ Encoder(x) = z â†’ TaskHead(z) = y

ä¼˜åŒ–ç›®æ ‡: æ‰¾åˆ° z* ä½¿å¾— y = TaskHead(z) è¾¾åˆ°æå€¼

é‡æ„: x* = Decoder(z*)
```

**ä¼˜åŒ–ç®—æ³•**ï¼š
```python
for step in range(max_steps):
    # 1. å‰å‘ä¼ æ’­
    y = TaskHead(z)

    # 2. è®¡ç®—æŸå¤±ï¼ˆæœ€å¤§åŒ–æ—¶å–è´Ÿï¼‰
    loss = -y  # for maximization

    # 3. åå‘ä¼ æ’­ï¼ˆåªæ›´æ–° zï¼Œæ¨¡å‹å‚æ•°å›ºå®šï¼‰
    loss.backward()

    # 4. ä¼˜åŒ–å™¨æ›´æ–° z
    optimizer.step(z)
```

---

## ğŸ’» æ ¸å¿ƒä»£ç å®ç°ï¼ˆå¸¦è¯¦ç»†æ³¨é‡Šï¼‰

### Part 1: åˆå§‹åŒ–ä¼˜åŒ–å˜é‡

```python
# ============================================================================
# æ­¥éª¤ 1: åˆå§‹åŒ– latent representation
# ============================================================================

# æ–¹å¼ 1: ä»è¾“å…¥ç¼–ç å¾—åˆ°åˆå§‹ latent
if initial_input is not None:
    with torch.no_grad():
        _, initial_latent = self.encoder(initial_input)
        # initial_latent.shape: (1, latent_dim)

# æ–¹å¼ 2: ä»éšæœºå‘é‡å¼€å§‹
else:
    initial_latent = torch.randn(1, latent_dim, device=device)
    # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ N(0,1) é‡‡æ ·

# å¯é€‰: æ·»åŠ æ‰°åŠ¨æ¢ç´¢ä¸åŒèµ·ç‚¹
if perturbation_std > 0:
    noise = torch.randn_like(initial_latent) * perturbation_std
    initial_latent = initial_latent + noise
    # æ·»åŠ é«˜æ–¯å™ªå£°: z_perturbed ~ N(z_init, ÏƒÂ²I)

# ============================================================================
# æ­¥éª¤ 2: åˆ›å»ºå¯ä¼˜åŒ–çš„ latentï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
# ============================================================================

# ä»è®¡ç®—å›¾ä¸­åˆ†ç¦»å¹¶è®¾ç½® requires_grad=True
latent = initial_latent.detach().clone().requires_grad_(True)
#        ^^^^^^^^^^^^^^^^        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
#        ä»æ¨¡å‹è®¡ç®—å›¾åˆ†ç¦»          ä½¿å…¶æˆä¸ºä¼˜åŒ–å˜é‡
#
# è¿™æ · latent ä¸å—æ¨¡å‹å‚æ•°å½±å“ï¼Œåªèƒ½é€šè¿‡æ¢¯åº¦ä¸‹é™æ›´æ–°

# ============================================================================
# æ­¥éª¤ 3: åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ– latentï¼Œä¸ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼‰
# ============================================================================

optimizer = optim.Adam([latent], lr=lr)
#                      ^^^^^^^^
#                      åªä¼ å…¥ latent ä½œä¸ºä¼˜åŒ–å‚æ•°
#                      æ¨¡å‹å‚æ•°ï¼ˆencoder, task_headï¼‰ä¸ä¼šè¢«æ›´æ–°
```

---

### Part 2: ä¼˜åŒ–å¾ªç¯ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰

```python
# ============================================================================
# ä¼˜åŒ–å¾ªç¯: é€šè¿‡è‡ªåŠ¨å¾®åˆ†æ›´æ–° latent
# ============================================================================

sign = 1.0 if mode == "max" else -1.0
# æœ€å¤§åŒ–: ä¼˜åŒ– -loss (æ¢¯åº¦ä¸Šå‡)
# æœ€å°åŒ–: ä¼˜åŒ– +loss (æ¢¯åº¦ä¸‹é™)

for step in range(steps):
    # ------------------------------------------------------------------------
    # æ­¥éª¤ 1: æ¸…é›¶æ¢¯åº¦
    # ------------------------------------------------------------------------
    optimizer.zero_grad()

    # ------------------------------------------------------------------------
    # æ­¥éª¤ 2: å‰å‘ä¼ æ’­ï¼ˆåªé€šè¿‡ task headï¼‰
    # ------------------------------------------------------------------------
    pred = self.task_heads[task_name](latent)
    #      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    #      latent â†’ task_head â†’ prediction
    #
    #      å…³é”®: åªä½¿ç”¨ task_headï¼Œä¸ä½¿ç”¨ encoder
    #      å› ä¸ºæˆ‘ä»¬ç›´æ¥ä¼˜åŒ– latentï¼Œä¸éœ€è¦ä»è¾“å…¥ç¼–ç 

    # pred.shape: (1, output_dim)  ä¾‹å¦‚: (1, 1) for scalar property

    # ------------------------------------------------------------------------
    # æ­¥éª¤ 3: è®¡ç®—æŸå¤±
    # ------------------------------------------------------------------------
    loss = -sign * pred.sum()
    #      ^^^^^^^^^^^^^^^^^^^^
    #      æœ€å¤§åŒ–: loss = -pred  (æ¢¯åº¦ä¸‹é™ = æ¢¯åº¦ä¸Šå‡è´Ÿå€¼)
    #      æœ€å°åŒ–: loss = +pred  (æ ‡å‡†æ¢¯åº¦ä¸‹é™)

    # ------------------------------------------------------------------------
    # æ­¥éª¤ 4: åå‘ä¼ æ’­ï¼ˆè®¡ç®— âˆ‚loss/âˆ‚latentï¼‰
    # ------------------------------------------------------------------------
    loss.backward()
    #
    # PyTorch è‡ªåŠ¨è®¡ç®—:
    #   âˆ‚loss/âˆ‚latent = âˆ‚loss/âˆ‚pred Â· âˆ‚pred/âˆ‚latent
    #
    # æ¢¯åº¦å­˜å‚¨åœ¨: latent.grad

    # ------------------------------------------------------------------------
    # æ­¥éª¤ 5: æ›´æ–° latentï¼ˆæ¢¯åº¦ä¸‹é™æ­¥ï¼‰
    # ------------------------------------------------------------------------
    optimizer.step()
    #
    # Adam æ›´æ–°è§„åˆ™:
    #   m_t = Î²â‚ Â· m_{t-1} + (1-Î²â‚) Â· âˆ‡loss        # ä¸€é˜¶çŸ©ä¼°è®¡
    #   v_t = Î²â‚‚ Â· v_{t-1} + (1-Î²â‚‚) Â· (âˆ‡loss)Â²    # äºŒé˜¶çŸ©ä¼°è®¡
    #   latent_new = latent_old - Î± Â· m_t / (âˆšv_t + Îµ)
    #
    # å…¶ä¸­:
    #   Î± = learning rate
    #   Î²â‚ = 0.9 (é»˜è®¤)
    #   Î²â‚‚ = 0.999 (é»˜è®¤)

    # ------------------------------------------------------------------------
    # æ­¥éª¤ 6: è®°å½•ä¼˜åŒ–å†å²ï¼ˆå¯é€‰ï¼‰
    # ------------------------------------------------------------------------
    with torch.no_grad():
        score = pred.item()
        history.append(score)
```

---

### Part 3: é‡æ„ä¼˜åŒ–åçš„ descriptor

```python
# ============================================================================
# ä½¿ç”¨ AutoEncoder é‡æ„ descriptor
# ============================================================================

with torch.no_grad():
    # ------------------------------------------------------------------------
    # è·å–æœ€ç»ˆä¼˜åŒ–çš„ latent
    # ------------------------------------------------------------------------
    optimized_latent = latent.detach()
    # shape: (1, latent_dim)

    # ------------------------------------------------------------------------
    # è®¡ç®—æœ€ç»ˆä»»åŠ¡åˆ†æ•°
    # ------------------------------------------------------------------------
    optimized_score = self.task_heads[task_name](optimized_latent)
    # shape: (1, 1)

    # ------------------------------------------------------------------------
    # é€šè¿‡ AutoEncoder decoder é‡æ„è¾“å…¥
    # ------------------------------------------------------------------------
    if ae_task_name is not None:
        reconstructed_input = self.task_heads[ae_task_name](optimized_latent)
        #                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        #                     z* â†’ Decoder â†’ x*
        #
        #                     AutoEncoderHead å°±æ˜¯ decoder
        #
        # reconstructed_input.shape: (1, input_dim)
        #
        # è¿™å°±æ˜¯ä¼˜åŒ–åçš„ææ–™æè¿°ç¬¦ï¼
```

---

## ğŸ” å…³é”®æŠ€æœ¯ç»†èŠ‚

### 1. ä¸ºä»€ä¹ˆç”¨ `detach().clone()`ï¼Ÿ

```python
latent = initial_latent.detach().clone().requires_grad_(True)
```

**åŸå› **ï¼š
- `detach()`: ä»åŸå§‹è®¡ç®—å›¾åˆ†ç¦»ï¼Œé¿å…åå‘ä¼ æ’­åˆ° encoder
- `clone()`: åˆ›å»ºå‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
- `requires_grad_(True)`: ä½¿å…¶æˆä¸ºä¼˜åŒ–å˜é‡

**å¦‚æœä¸ detach()**ï¼š
```python
# âŒ é”™è¯¯åšæ³•
latent = initial_latent.requires_grad_(True)

# é—®é¢˜: backward() ä¼šå°è¯•æ›´æ–° encoder çš„å‚æ•°
# å¯¼è‡´:
# 1. ä¼˜åŒ–ç›®æ ‡æ··ä¹±ï¼ˆæ—¢ä¼˜åŒ– latent åˆä¼˜åŒ– encoderï¼‰
# 2. å¯èƒ½ç ´åè®­ç»ƒå¥½çš„ encoder
```

### 2. ä¸ºä»€ä¹ˆåªä¼  `[latent]` ç»™ä¼˜åŒ–å™¨ï¼Ÿ

```python
optimizer = optim.Adam([latent], lr=lr)
```

**åŸå› **ï¼š
- åªæœ‰ `latent` éœ€è¦è¢«ä¼˜åŒ–
- æ¨¡å‹å‚æ•°ï¼ˆencoder, task_headï¼‰ä¿æŒå›ºå®š
- è¿™æ˜¯ **å˜é‡ä¼˜åŒ–** è€Œé **å‚æ•°ä¼˜åŒ–**

**å¯¹æ¯”è®­ç»ƒæ—¶çš„ä¼˜åŒ–å™¨**ï¼š
```python
# è®­ç»ƒæ—¶: ä¼˜åŒ–æ¨¡å‹å‚æ•°
optimizer = optim.Adam(model.parameters(), lr=lr)

# ä¼˜åŒ– latent æ—¶: åªä¼˜åŒ– latent å˜é‡
optimizer = optim.Adam([latent], lr=lr)
```

### 3. ä¸ºä»€ä¹ˆç”¨ `loss = -sign * pred.sum()`ï¼Ÿ

```python
sign = 1.0 if mode == "max" else -1.0
loss = -sign * pred.sum()
```

**æ•°å­¦è§£é‡Š**ï¼š

| ç›®æ ‡ | sign | loss | æ¢¯åº¦ä¸‹é™æ•ˆæœ |
|------|------|------|--------------|
| æœ€å¤§åŒ– y | +1 | -y | min(-y) = max(y) âœ“ |
| æœ€å°åŒ– y | -1 | +y | min(+y) = min(y) âœ“ |

**ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨æ¢¯åº¦ä¸Šå‡**ï¼Ÿ
```python
# ä¹Ÿå¯ä»¥è¿™æ ·å®ç°
if mode == "max":
    # æ¢¯åº¦ä¸Šå‡
    pred = self.task_heads[task_name](latent)
    (-pred).backward()  # è´Ÿæ¢¯åº¦æ–¹å‘
    optimizer.step()
else:
    # æ¢¯åº¦ä¸‹é™
    pred = self.task_heads[task_name](latent)
    pred.backward()
    optimizer.step()

# ä½†ç”¨ loss = -sign * pred æ›´ç®€æ´ç»Ÿä¸€
```

### 4. è®¡ç®—å›¾ç¤ºä¾‹

```
åˆå§‹åŒ–é˜¶æ®µ:
    initial_input â†’ Encoder â†’ initial_latent
                    ^^^^^^^^
                    requires_grad=False (å›ºå®šå‚æ•°)
                                â†“
                           detach().clone()
                                â†“
                            latent (requires_grad=True)

ä¼˜åŒ–é˜¶æ®µ (æ¯ä¸€æ­¥):
    latent â†’ TaskHead â†’ pred â†’ loss
    ^^^^^^   ^^^^^^^^^
    å¯ä¼˜åŒ–     å›ºå®šå‚æ•°

    backward():
    latent â† âˆ‚loss/âˆ‚latent
    ^^^^^
    æ›´æ–°è¿™ä¸ª

é‡æ„é˜¶æ®µ:
    optimized_latent â†’ AutoEncoderHead â†’ reconstructed_input
                       ^^^^^^^^^^^^^^^^
                       å°±æ˜¯ decoder
```

---

## ğŸ“Š ä¸æ ‡å‡†ä¼˜åŒ–é—®é¢˜çš„å¯¹æ¯”

### æ ‡å‡†ç¥ç»ç½‘ç»œè®­ç»ƒ
```python
# ä¼˜åŒ–ç›®æ ‡: æ¨¡å‹å‚æ•° Î¸
# å›ºå®š: è¾“å…¥æ•°æ® x, æ ‡ç­¾ y

for epoch in range(epochs):
    pred = model(x; Î¸)  # Î¸ æ˜¯å‚æ•°
    loss = criterion(pred, y)
    loss.backward()  # è®¡ç®— âˆ‚loss/âˆ‚Î¸
    optimizer.step()  # æ›´æ–° Î¸
```

### Latent ä¼˜åŒ–ï¼ˆæœ¬ç®—æ³•ï¼‰
```python
# ä¼˜åŒ–ç›®æ ‡: latent z
# å›ºå®š: æ¨¡å‹å‚æ•° Î¸

for step in range(steps):
    pred = task_head(z; Î¸)  # Î¸ å›ºå®š
    loss = -pred  # æœ€å¤§åŒ– pred
    loss.backward()  # è®¡ç®— âˆ‚loss/âˆ‚z
    optimizer.step()  # æ›´æ–° z
```

**å…³é”®åŒºåˆ«**ï¼š
- è®­ç»ƒ: ä¼˜åŒ–å‚æ•°ï¼Œå›ºå®šæ•°æ®
- Latentä¼˜åŒ–: ä¼˜åŒ–æ•°æ®è¡¨ç¤ºï¼Œå›ºå®šå‚æ•°

---

## ğŸ¨ å¯è§†åŒ–ç†è§£

### ä¼˜åŒ–è½¨è¿¹
```
Latent Space (2D visualization):

åˆå§‹ç‚¹ zâ‚€ â€¢
           â†˜
            â€¢ zâ‚
             â†˜
              â€¢ zâ‚‚    â† æ¢¯åº¦æ–¹å‘
               â†˜
                â€¢ zâ‚ƒ
                 â†˜
                  â€¢ z* (æœ€ä¼˜ç‚¹)

Property value:
f(zâ‚€) = 1.2
f(zâ‚) = 1.5
f(zâ‚‚) = 1.8
f(zâ‚ƒ) = 2.1
f(z*) = 2.3  â† æœ€å¤§å€¼
```

### å¤šé‡å¯åŠ¨æ•ˆæœ
```
Latent Space with multiple restarts:

Restart 1: â€¢â”€â”€â”€â”€â†’ â€¢ (local max, score=2.1)
Restart 2: â€¢â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â€¢ (global max, score=2.8) âœ“
Restart 3: â€¢â”€â”€â”€â†’ â€¢ (local max, score=1.9)
Restart 4: â€¢â”€â”€â”€â”€â”€â”€â”€â”€â†’ â€¢ (local max, score=2.3)
Restart 5: â€¢â”€â”€â”€â†’ â€¢ (local max, score=2.0)

é€‰æ‹©: Restart 2 çš„ç»“æœ
```

---

## ğŸ”¬ å®Œæ•´ä»£ç æµç¨‹æ€»ç»“

```python
def optimize_latent(self, task_name, initial_input, mode, ...):

    # ========================================
    # Phase 1: åˆå§‹åŒ–
    # ========================================

    # 1.1 è·å–æˆ–ç”Ÿæˆåˆå§‹ latent
    if initial_input is not None:
        _, initial_latent = self.encoder(initial_input)
    else:
        initial_latent = torch.randn(1, latent_dim)

    # 1.2 æ·»åŠ æ‰°åŠ¨ï¼ˆå¯é€‰ï¼‰
    if perturbation_std > 0:
        initial_latent += torch.randn_like(initial_latent) * perturbation_std

    # 1.3 åˆ›å»ºå¯ä¼˜åŒ–å˜é‡
    latent = initial_latent.detach().clone().requires_grad_(True)

    # 1.4 åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam([latent], lr=lr)

    # ========================================
    # Phase 2: ä¼˜åŒ–å¾ªç¯
    # ========================================

    for step in range(steps):
        # 2.1 æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()

        # 2.2 å‰å‘ä¼ æ’­
        pred = self.task_heads[task_name](latent)

        # 2.3 è®¡ç®—æŸå¤±
        loss = -sign * pred.sum()

        # 2.4 åå‘ä¼ æ’­
        loss.backward()  # è®¡ç®— âˆ‚loss/âˆ‚latent

        # 2.5 æ›´æ–° latent
        optimizer.step()  # latent â† latent - lr * âˆ‡loss

    # ========================================
    # Phase 3: é‡æ„
    # ========================================

    with torch.no_grad():
        # 3.1 è·å–ä¼˜åŒ–åçš„ latent
        optimized_latent = latent.detach()

        # 3.2 è®¡ç®—æœ€ç»ˆåˆ†æ•°
        optimized_score = self.task_heads[task_name](optimized_latent)

        # 3.3 é‡æ„ descriptor
        if ae_task_name:
            reconstructed_input = self.task_heads[ae_task_name](optimized_latent)
            # AutoEncoderHead: latent â†’ decoder â†’ input_space

    return {
        'optimized_latent': optimized_latent,
        'optimized_score': optimized_score,
        'reconstructed_input': reconstructed_input,
    }
```

---

## ğŸ§® æ•°å€¼ä¾‹å­

å‡è®¾æˆ‘ä»¬ä¼˜åŒ–å¯†åº¦ï¼ˆdensityï¼‰ï¼š

```python
# åˆå§‹çŠ¶æ€
initial_input: [0.1, 0.3, -0.2, ...]  # 190ç»´
     â†“ encoder
initial_latent: [0.5, -0.1, 0.3, ...]  # 128ç»´
     â†“ task_head
initial_density: 1.23 g/cmÂ³

# ä¼˜åŒ–è¿‡ç¨‹ï¼ˆæ¯ä¸€æ­¥ï¼‰
Step 1: latent = [0.5, -0.1, 0.3, ...] â†’ density = 1.23
        âˆ‡density/âˆ‡latent = [0.02, 0.05, -0.01, ...]
        latent â† latent + lr * âˆ‡ = [0.51, -0.05, 0.29, ...]

Step 2: latent = [0.51, -0.05, 0.29, ...] â†’ density = 1.28
        âˆ‡density/âˆ‡latent = [0.03, 0.04, -0.02, ...]
        latent â† [0.525, 0.0, 0.27, ...]

...

Step 200: latent = [0.8, 0.2, 0.1, ...] â†’ density = 2.15 g/cmÂ³

# é‡æ„
optimized_latent: [0.8, 0.2, 0.1, ...]  # 128ç»´
     â†“ autoencoder decoder
reconstructed_input: [0.2, 0.5, 0.1, ...]  # 190ç»´
     â†“ è¿™å°±æ˜¯æ–°ææ–™çš„æè¿°ç¬¦ï¼

# éªŒè¯
reconstructed_input â†’ encoder â†’ latent' â‰ˆ optimized_latent
latent' â†’ task_head â†’ density' â‰ˆ 2.15 g/cmÂ³ âœ“
```

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

1. **è‡ªåŠ¨å¾®åˆ†çš„åŠ›é‡**
   - ä¸éœ€è¦æ‰‹åŠ¨æ¨å¯¼ âˆ‚property/âˆ‚latent
   - PyTorch è‡ªåŠ¨è®¡ç®—æ‰€æœ‰æ¢¯åº¦
   - æ”¯æŒä»»æ„å¤æ‚çš„ç¥ç»ç½‘ç»œ

2. **å˜é‡ä¼˜åŒ– vs å‚æ•°ä¼˜åŒ–**
   - è®­ç»ƒ: å›ºå®šæ•°æ®ï¼Œä¼˜åŒ–å‚æ•°
   - Latentä¼˜åŒ–: å›ºå®šå‚æ•°ï¼Œä¼˜åŒ–è¡¨ç¤º

3. **ä¸ºä»€ä¹ˆéœ€è¦ AutoEncoder**
   - Latent ç©ºé—´æ˜¯æŠ½è±¡çš„ï¼Œæ— æ³•ç›´æ¥è§£é‡Š
   - AutoEncoder å°†å…¶æ˜ å°„å›å¯è§£é‡Šçš„ descriptor ç©ºé—´
   - Descriptor å¯ä»¥æŒ‡å¯¼å®é™…ææ–™åˆæˆ

4. **å¤šé‡å¯åŠ¨çš„é‡è¦æ€§**
   - Latent ç©ºé—´å¯èƒ½æœ‰å¤šä¸ªå±€éƒ¨æœ€ä¼˜
   - å¤šèµ·ç‚¹æ¢ç´¢å¢åŠ æ‰¾åˆ°å…¨å±€æœ€ä¼˜çš„æ¦‚ç‡
   - ç±»ä¼¼äºé—ä¼ ç®—æ³•çš„ç§ç¾¤æ€æƒ³

---

## ğŸ“š ç›¸å…³æŠ€æœ¯

è¿™ä¸ªç®—æ³•ä¸ä»¥ä¸‹æŠ€æœ¯ç›¸å…³ï¼š

1. **å¯¹æŠ—æ ·æœ¬ç”Ÿæˆï¼ˆAdversarial Examplesï¼‰**
   - ä¹Ÿæ˜¯ä¼˜åŒ–è¾“å…¥æ¥æ¬ºéª—æ¨¡å‹
   - åŒºåˆ«: æˆ‘ä»¬æ˜¯å¯»æ‰¾æœ‰æ„ä¹‰çš„æå€¼ï¼Œä¸æ˜¯æ¬ºéª—

2. **DeepDream / Neural Style Transfer**
   - ä¹Ÿæ˜¯ä¼˜åŒ–è¾“å…¥æ¥æœ€å¤§åŒ–æŸäº›æ¿€æ´»
   - åŒºåˆ«: æˆ‘ä»¬ä¼˜åŒ– latentï¼Œç„¶åé‡æ„

3. **Bayesian Optimization**
   - ä¹Ÿç”¨äºé»‘ç›’ä¼˜åŒ–
   - åŒºåˆ«: æˆ‘ä»¬åˆ©ç”¨äº†æ¢¯åº¦ä¿¡æ¯ï¼ˆæ›´é«˜æ•ˆï¼‰

4. **GAN ä¸­çš„ latent space manipulation**
   - ä¹Ÿæ˜¯åœ¨ latent space ä¸­å¯»æ‰¾ç‰¹å®šå±æ€§
   - åŒºåˆ«: æˆ‘ä»¬ç”¨æ¢¯åº¦ä¸‹é™ï¼ŒGANç”¨é‡‡æ ·

---

è¿™å°±æ˜¯æ ¸å¿ƒç®—æ³•ï¼ç®€å•ä½†å¼ºå¤§ ğŸš€
