# ğŸ¯ Latent Space ä¼˜åŒ–æ ¸å¿ƒç®—æ³•è¯´æ˜

## ğŸ“š æ ¸å¿ƒæ–‡æ¡£

æˆ‘å·²ç»åˆ›å»ºäº†è¯¦ç»†çš„ç®—æ³•è¯´æ˜æ–‡æ¡£ï¼Œè¯·æŒ‰é¡ºåºé˜…è¯»ï¼š

### 1. **å®Œæ•´ç®—æ³•è¯¦è§£**
ğŸ“„ [OPTIMIZATION_ALGORITHM_CORE.md](OPTIMIZATION_ALGORITHM_CORE.md)

**å†…å®¹**ï¼š
- æ•°å­¦åŸç†å’Œå…¬å¼æ¨å¯¼
- å®Œæ•´çš„ä»£ç å®ç°ï¼ˆå¸¦è¯¦ç»†æ³¨é‡Šï¼‰
- PyTorch è‡ªåŠ¨å¾®åˆ†æœºåˆ¶è§£é‡Š
- å…³é”®æŠ€æœ¯ç»†èŠ‚ï¼ˆä¸ºä»€ä¹ˆç”¨ detachã€clone ç­‰ï¼‰
- ä¸æ ‡å‡†ä¼˜åŒ–é—®é¢˜çš„å¯¹æ¯”
- å¯è§†åŒ–ç¤ºä¾‹

**é€‚åˆ**ï¼šæƒ³æ·±å…¥ç†è§£ç®—æ³•åŸç†çš„è¯»è€…

### 2. **æœ€å°åŒ–å¯è¿è¡Œç¤ºä¾‹**
ğŸ“„ [examples/minimal_latent_optimization.py](examples/minimal_latent_optimization.py)

**å†…å®¹**ï¼š
- ç‹¬ç«‹çš„æœ€å°åŒ–å®ç°ï¼ˆä¸ä¾èµ–å®Œæ•´æ¨¡å‹ï¼‰
- æ ¸å¿ƒç®—æ³•çš„ç²¾ç®€ç‰ˆ
- æ¢¯åº¦è®¡ç®—æ¼”ç¤º
- å¤šé‡å¯åŠ¨ç¤ºä¾‹
- å¯è§†åŒ–åŠŸèƒ½

**è¿è¡Œ**ï¼š
```bash
python examples/minimal_latent_optimization.py
```

**é€‚åˆ**ï¼šæƒ³å¿«é€Ÿçœ‹åˆ°è¿è¡Œç»“æœå’Œç†è§£æ ¸å¿ƒé€»è¾‘

---

## âš¡ æ ¸å¿ƒç®—æ³•ä¸€å¥è¯æ€»ç»“

**é€šè¿‡å›ºå®šè®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ï¼Œç”¨æ¢¯åº¦ä¸‹é™ç›´æ¥ä¼˜åŒ– latent representationï¼Œä½¿å¾—ç›®æ ‡ä»»åŠ¡çš„è¾“å‡ºè¾¾åˆ°æå€¼ï¼Œç„¶åç”¨ AutoEncoder å°†ä¼˜åŒ–åçš„ latent é‡æ„å›å¯è§£é‡Šçš„ descriptor ç©ºé—´ã€‚**

---

## ğŸ”‘ æ ¸å¿ƒä»£ç ç‰‡æ®µ

### æœ€æ ¸å¿ƒçš„ 3 è¡Œä»£ç 

```python
latent = initial_latent.detach().clone().requires_grad_(True)  # åˆ›å»ºä¼˜åŒ–å˜é‡
optimizer = optim.Adam([latent], lr=lr)                        # åªä¼˜åŒ– latent

for step in range(steps):
    pred = task_head(latent)      # å‰å‘: latent â†’ property
    loss = -pred                  # æŸå¤±ï¼ˆæœ€å¤§åŒ–æ—¶å–è´Ÿï¼‰
    loss.backward()               # åå‘: è®¡ç®— âˆ‚loss/âˆ‚latent
    optimizer.step()              # æ›´æ–°: latent â† latent - lrÂ·âˆ‡loss
```

### å®Œæ•´æµç¨‹ï¼ˆ10è¡Œæ ¸å¿ƒé€»è¾‘ï¼‰

```python
# 1. åˆå§‹åŒ–ä¼˜åŒ–å˜é‡ï¼ˆä»æ¨¡å‹è®¡ç®—å›¾åˆ†ç¦»ï¼‰
latent = initial_latent.detach().clone().requires_grad_(True)
optimizer = optim.Adam([latent], lr=0.1)

# 2. ä¼˜åŒ–å¾ªç¯
for step in range(200):
    optimizer.zero_grad()
    pred = task_head(latent)           # latent â†’ property
    loss = -pred                       # æœ€å¤§åŒ–
    loss.backward()                    # âˆ‚loss/âˆ‚latent
    optimizer.step()                   # æ›´æ–° latent

# 3. é‡æ„åˆ°è¾“å…¥ç©ºé—´
reconstructed_input = decoder(optimized_latent)  # latent â†’ descriptor
```

---

## ğŸ“Š ç®—æ³•æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åˆå§‹åŒ–é˜¶æ®µ                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ initial_input (å¯é€‰)                  â”‚
      â”‚   æˆ–                                  â”‚
      â”‚ random sample ~ N(0,1)                â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Encoder (å›ºå®šå‚æ•°)                    â”‚
      â”‚    â†“                                  â”‚
      â”‚ initial_latent                        â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ latent = initial_latent               â”‚
      â”‚   .detach()    â† ä»è®¡ç®—å›¾åˆ†ç¦»         â”‚
      â”‚   .clone()     â† åˆ›å»ºå‰¯æœ¬             â”‚
      â”‚   .requires_grad_(True) â† è®¾ä¸ºå˜é‡    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ä¼˜åŒ–å¾ªç¯ (æ­¥éª¤ 1..N)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â”‚             â”‚
                    â–¼             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚ optimizer.zero_grad()   â”‚ â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â”‚             â”‚
                    â–¼             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚ pred = task_head(latent)â”‚ â”‚ â† å‰å‘ä¼ æ’­
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   (åªé€šè¿‡ task_head)
                    â”‚             â”‚
                    â–¼             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚ loss = -sign * pred     â”‚ â”‚ â† è®¡ç®—æŸå¤±
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   (æœ€å¤§åŒ–æ—¶å–è´Ÿ)
                    â”‚             â”‚
                    â–¼             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚ loss.backward()         â”‚ â”‚ â† åå‘ä¼ æ’­
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   è®¡ç®— âˆ‚loss/âˆ‚latent
                    â”‚             â”‚
                    â–¼             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚ optimizer.step()        â”‚ â”‚ â† æ›´æ–° latent
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   latent â† latent - lrÂ·âˆ‡
                    â”‚             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ é‡å¤ç›´åˆ°æ”¶æ•›
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    é‡æ„é˜¶æ®µ                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ optimized_latent = latent.detach()    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                    â”‚             â”‚
                    â–¼             â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ task_head(latent)   â”‚  â”‚ decoder(latent)      â”‚
      â”‚         â†“           â”‚  â”‚         â†“            â”‚
      â”‚ optimized_score     â”‚  â”‚ reconstructed_input  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              (æ€§è´¨å€¼)              (ææ–™æè¿°ç¬¦)
```

---

## ğŸ§® æ•°å­¦è¡¨è¿°

### ä¼˜åŒ–é—®é¢˜

$$
\begin{aligned}
& \text{ç»™å®š:} && x \xrightarrow{f_{enc}} z \xrightarrow{f_{task}} y \\
& \text{ä¼˜åŒ–:} && z^* = \underset{z}{\arg\max} \; f_{task}(z) \\
& \text{é‡æ„:} && x^* = f_{dec}(z^*)
\end{aligned}
$$

### æ¢¯åº¦æ›´æ–°

$$
z_{t+1} = z_t - \alpha \nabla_z \mathcal{L}(z_t)
$$

å…¶ä¸­ï¼š
- $\mathcal{L}(z) = -f_{task}(z)$ ï¼ˆæœ€å¤§åŒ–æ—¶ï¼‰
- $\alpha$ = å­¦ä¹ ç‡
- $\nabla_z$ ç”± PyTorch è‡ªåŠ¨è®¡ç®—

### Adam ä¼˜åŒ–å™¨

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1-\beta_1) \nabla_z \mathcal{L} \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2) (\nabla_z \mathcal{L})^2 \\
z_{t+1} &= z_t - \alpha \frac{m_t}{\sqrt{v_t} + \epsilon}
\end{aligned}
$$

---

## ğŸ’¡ å…³é”®æŠ€æœ¯ç‚¹

### 1. ä¸ºä»€ä¹ˆç”¨ `detach().clone().requires_grad_(True)`ï¼Ÿ

```python
latent = initial_latent.detach().clone().requires_grad_(True)
```

- **`detach()`**: ä» encoder çš„è®¡ç®—å›¾åˆ†ç¦»
  - é¿å…æ¢¯åº¦åå‘ä¼ æ’­åˆ° encoder
  - ä¿æŠ¤è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°

- **`clone()`**: åˆ›å»ºå‰¯æœ¬
  - é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
  - å…è®¸ç‹¬ç«‹ä¼˜åŒ–

- **`requires_grad_(True)`**: è®¾ä¸ºä¼˜åŒ–å˜é‡
  - å¯ç”¨æ¢¯åº¦è®¡ç®—
  - ä½¿å…¶å¯è¢«ä¼˜åŒ–å™¨æ›´æ–°

### 2. ä¸ºä»€ä¹ˆåªä¼  `[latent]` ç»™ä¼˜åŒ–å™¨ï¼Ÿ

```python
optimizer = optim.Adam([latent], lr=lr)  # åªä¼˜åŒ– latent
```

**å¯¹æ¯”**ï¼š
```python
# è®­ç»ƒæ¨¡å‹æ—¶
optimizer = optim.Adam(model.parameters())  # ä¼˜åŒ–æ‰€æœ‰å‚æ•°

# ä¼˜åŒ– latent æ—¶
optimizer = optim.Adam([latent])  # åªä¼˜åŒ– latent å˜é‡
```

è¿™æ˜¯ **å˜é‡ä¼˜åŒ–** è€Œé **å‚æ•°ä¼˜åŒ–**ï¼

### 3. ä¸ºä»€ä¹ˆç”¨ `loss = -sign * pred`ï¼Ÿ

```python
sign = 1.0 if mode == "max" else -1.0
loss = -sign * pred
```

| ç›®æ ‡ | sign | loss | æ¢¯åº¦ä¸‹é™ç»“æœ |
|------|------|------|--------------|
| æœ€å¤§åŒ– y | +1.0 | -y | min(-y) = max(y) âœ… |
| æœ€å°åŒ– y | -1.0 | +y | min(+y) = min(y) âœ… |

PyTorch ä¼˜åŒ–å™¨é»˜è®¤æ˜¯**æ¢¯åº¦ä¸‹é™**ï¼ˆæœ€å°åŒ–ï¼‰ï¼Œæ‰€ä»¥æœ€å¤§åŒ–æ—¶è¦å–è´Ÿå·ã€‚

---

## ğŸ¨ å¯è§†åŒ–ç¤ºä¾‹

### ä¼˜åŒ–è½¨è¿¹ï¼ˆ2D latent spaceï¼‰

```
Property Landscape:

    3.0 â”‚                    â•”â•â•â•â•â•â•â•—  â† Global Maximum
        â”‚                    â•‘  â˜…   â•‘
    2.5 â”‚           â•”â•â•â•â•â•â•â•â•â•      â•šâ•â•â•â•—
        â”‚           â•‘                   â•‘
    2.0 â”‚     â•”â•â•â•â•â•â•  â€¢               â•‘
        â”‚     â•‘         â†‘               â•‘
    1.5 â”‚  â•”â•â•â•         â”‚ Optimization  â•‘
        â”‚  â•‘            â”‚ Path          â•‘
    1.0 â”‚â•â•â•            â€¢               â•šâ•â•â•—
        â”‚               â†‘                  â•‘
    0.5 â”‚               â€¢ â† Start          â•‘
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          zâ‚€            zâ‚               zâ‚‚

â€¢ Initial point (random)
â˜… Optimized point (maximum)
```

### Multi-Restart æ•ˆæœ

```
Different starting points lead to different local maxima:

Restart 1:  â€¢â”€â”€â”€â”€â”€â†’ â€¢ (score=2.1, local max)
Restart 2:  â€¢â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â˜… (score=2.8, GLOBAL max) âœ“
Restart 3:  â€¢â”€â”€â”€â†’ â€¢ (score=1.9, local max)
Restart 4:  â€¢â”€â”€â”€â”€â”€â†’ â€¢ (score=2.3, local max)
Restart 5:  â€¢â”€â”€â†’ â€¢ (score=1.7, local max)

Best result: Restart 2 with score 2.8
```

---

## ğŸ”¬ å®é™…åº”ç”¨ç¤ºä¾‹

### å¯»æ‰¾é«˜å¯†åº¦ææ–™

```python
# è®­ç»ƒå¥½çš„æ¨¡å‹
model = FlexibleMultiTaskModel.load_from_checkpoint("model.ckpt")

# ä¼˜åŒ–å¯»æ‰¾æœ€é«˜å¯†åº¦çš„ææ–™
result = model.optimize_latent(
    task_name="density",
    initial_input=None,      # éšæœºæœç´¢
    mode="max",              # æœ€å¤§åŒ–å¯†åº¦
    num_restarts=50,         # 50ä¸ªèµ·ç‚¹
    steps=500,               # æ¯ä¸ªèµ·ç‚¹ä¼˜åŒ–500æ­¥
    ae_task_name="reconstruction"
)

# è·å–ä¼˜åŒ–åçš„ææ–™æè¿°ç¬¦
optimized_descriptor = result['reconstructed_input']
print(f"Predicted density: {result['optimized_score'].item():.2f} g/cmÂ³")

# å¯ä»¥ç”¨è¿™ä¸ª descriptor æŒ‡å¯¼å®éªŒåˆæˆ
```

---

## ğŸ“ˆ æ€§èƒ½ç‰¹ç‚¹

### ä¼˜ç‚¹
âœ… **åˆ©ç”¨æ¢¯åº¦ä¿¡æ¯** - æ¯”é»‘ç›’ä¼˜åŒ–ï¼ˆå¦‚é—ä¼ ç®—æ³•ï¼‰æ›´é«˜æ•ˆ
âœ… **è‡ªåŠ¨å¾®åˆ†** - æ— éœ€æ‰‹åŠ¨æ¨å¯¼æ¢¯åº¦
âœ… **å¯å¤„ç†é«˜ç»´ç©ºé—´** - latent dim å¯è¾¾æ•°ç™¾ç»´
âœ… **æ”¯æŒä»»æ„ç½‘ç»œç»“æ„** - åªè¦å¯å¾®åˆ†å³å¯

### é™åˆ¶
âš ï¸ **å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜** - éœ€è¦ multi-restart
âš ï¸ **ä¾èµ– AutoEncoder è´¨é‡** - é‡æ„è¯¯å·®ä¼šå½±å“ç»“æœ
âš ï¸ **è®¡ç®—æˆæœ¬** - ä¸ä¼˜åŒ–æ­¥æ•°ã€é‡å¯æ¬¡æ•°æˆæ­£æ¯”

---

## ğŸš€ è¿›é˜¶æŠ€å·§

### 1. æ·»åŠ æ­£åˆ™åŒ–çº¦æŸ

```python
# åœ¨ latent ä¸Šæ·»åŠ  L2 æ­£åˆ™åŒ–ï¼Œé¿å…è¿‡åº¦åç¦»è®­ç»ƒåˆ†å¸ƒ
for step in range(steps):
    pred = task_head(latent)
    reg = 0.01 * (latent ** 2).sum()  # L2 penalty
    loss = -pred + reg
    loss.backward()
    optimizer.step()
```

### 2. ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦

```python
optimizer = optim.Adam([latent], lr=0.1)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='max', patience=20
)

for step in range(steps):
    # ... optimization ...
    scheduler.step(pred.item())
```

### 3. æ—©åœæœºåˆ¶

```python
best_score = -float('inf')
patience = 50
no_improve = 0

for step in range(steps):
    pred = task_head(latent)
    if pred.item() > best_score:
        best_score = pred.item()
        no_improve = 0
    else:
        no_improve += 1

    if no_improve >= patience:
        print(f"Early stopping at step {step}")
        break
```

---

## ğŸ“– ç›¸å…³è®ºæ–‡å’ŒæŠ€æœ¯

1. **Gradient-based Optimization in Input Space**
   - å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ (Adversarial Examples)
   - DeepDream, Neural Style Transfer

2. **Latent Space Manipulation**
   - VAE latent space interpolation
   - GAN latent space editing

3. **Inverse Design**
   - Materials discovery via generative models
   - Protein design with AlphaFold

---

## ğŸ“ å»¶ä¼¸é˜…è¯»

- [OPTIMIZATION_ALGORITHM_CORE.md](OPTIMIZATION_ALGORITHM_CORE.md) - å®Œæ•´ç®—æ³•è¯¦è§£
- [OPTIMIZATION_IMPROVEMENTS.md](OPTIMIZATION_IMPROVEMENTS.md) - æ”¹è¿›ç­–ç•¥è¯´æ˜
- [examples/minimal_latent_optimization.py](examples/minimal_latent_optimization.py) - å¯è¿è¡Œç¤ºä¾‹

---

## â“ å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆä¸ç›´æ¥ä¼˜åŒ–è¾“å…¥ x è€Œæ˜¯ä¼˜åŒ– latent zï¼Ÿ**

A:
1. Latent space æ›´å¹³æ»‘ã€ç»“æ„æ›´å¥½
2. ç»´åº¦æ›´ä½ï¼ˆ128 vs 190ï¼‰ï¼Œä¼˜åŒ–æ›´å¿«
3. AutoEncoder ä¿è¯é‡æ„åçš„ x æ›´åˆç†

**Q: å¦‚ä½•ä¿è¯ä¼˜åŒ–åçš„ææ–™æ˜¯å¯è¡Œçš„ï¼Ÿ**

A:
1. AutoEncoder çº¦æŸäº†åˆç†æ€§
2. å¯æ·»åŠ ç‰©ç†çº¦æŸï¼ˆæ­£åˆ™åŒ–ï¼‰
3. å¤šæ¬¡éªŒè¯ï¼ˆå¤šé‡å¯åŠ¨ï¼‰

**Q: ä¼˜åŒ–éœ€è¦å¤šä¹…ï¼Ÿ**

A: å•æ¬¡è¿è¡Œé€šå¸¸ 1-5 ç§’ï¼ˆ200æ­¥ï¼‰ï¼Œ50æ¬¡é‡å¯çº¦ 1-2 åˆ†é’Ÿ

---

**æ ¸å¿ƒç®—æ³•å°±æ˜¯è¿™äº›ï¼ç®€æ´ä½†å¼ºå¤§ ğŸš€**
