# ç®€åŒ–æ¶æ„æ¸…ç†æ€»ç»“

## ğŸ¯ ç›®æ ‡

ç§»é™¤ deposit layer åï¼Œæ¸…ç†ä»£ç ä¸­æ‰€æœ‰è¿‡æ—¶çš„å¼•ç”¨å’Œæ–‡æ¡£ã€‚

## âœ… å®Œæˆçš„ä¿®å¤

### 1. ç§»é™¤ `encoder.deposit` çš„ä»£ç å¼•ç”¨

**æ–‡ä»¶**: `src/foundation_model/models/flexible_multi_task_model.py`

**ä½ç½®**: Line 525

**ä¿®å¤å‰**:
```python
if self.freeze_shared_encoder:
    for p in self.encoder.shared.parameters():
        p.requires_grad_(False)
    for p in self.encoder.deposit.parameters():  # â† é”™è¯¯ï¼encoder å·²æ—  deposit
        p.requires_grad_(False)
```

**ä¿®å¤å**:
```python
if self.freeze_shared_encoder:
    for p in self.encoder.shared.parameters():
        p.requires_grad_(False)
    # deposit layer removed in simplified architecture
```

**åŸå› **: `FoundationEncoder` å·²ç»ç§»é™¤äº† `self.deposit` å±æ€§ï¼Œå¼•ç”¨ä¼šå¯¼è‡´ `AttributeError`ã€‚

---

### 2. æ›´æ–° `_TransformerBackbone` æ–‡æ¡£å­—ç¬¦ä¸²

**æ–‡ä»¶**: `src/foundation_model/models/components/foundation_encoder.py`

#### ä¿®å¤ 2.1: Class docstring (Lines 34-41)

**ä¿®å¤å‰**:
```python
When ``use_cls_token`` is enabled the downstream ``deposit`` layer only sees
the hidden state of the classifier token.
...
Disabling the ``[CLS]`` token switches to mean pooling, which exposes the
aggregated hidden states of all tokens directly to the deposit layer and
distributes gradients evenly across the sequence.
```

**ä¿®å¤å**:
```python
When ``use_cls_token`` is enabled the downstream task heads only see
the hidden state of the classifier token.
...
Disabling the ``[CLS]`` token switches to mean pooling, which exposes the
aggregated hidden states of all tokens directly to the task heads and
distributes gradients evenly across the sequence.
```

#### ä¿®å¤ 2.2: Forward method comments (Lines 133-140)

**ä¿®å¤å‰**:
```python
# Gradients from the downstream deposit layer flow into the `[CLS]` token
...
# Mean pooling exposes every contextualised feature token to the deposit layer
```

**ä¿®å¤å**:
```python
# Gradients from the downstream task heads flow into the `[CLS]` token
...
# Mean pooling exposes every contextualised feature token to the task heads
```

---

### 3. æ›´æ–° `FlexibleMultiTaskModel` æ–‡æ¡£å­—ç¬¦ä¸²

**æ–‡ä»¶**: `src/foundation_model/models/flexible_multi_task_model.py`

#### ä¿®å¤ 3.1: Usage scenarios (Line 84)

**ä¿®å¤å‰**:
```python
4. Continual Learning: Support model updates via deposit layer design
```

**ä¿®å¤å**:
```python
4. Continual Learning: Support model updates via modular architecture
```

#### ä¿®å¤ 3.2: Parameter documentation (Lines 90-91, 97)

**ä¿®å¤å‰**:
```python
task_configs : list[...]
    ...Regression and classification task heads receive the deposit
    layer output, while KernelRegression task heads receive both
    deposit layer output and sequence points.

shared_block_optimizer : OptimizerConfig | None
    Optimizer configuration for the shared foundation encoder and deposit layer.
```

**ä¿®å¤å**:
```python
task_configs : list[...]
    ...Regression and classification task heads receive Tanh-activated
    latent representations, while KernelRegression task heads receive both
    latent representations and sequence points.

shared_block_optimizer : OptimizerConfig | None
    Optimizer configuration for the shared foundation encoder.
```

#### ä¿®å¤ 3.3: Method parameter documentation (Line 1144)

**ä¿®å¤å‰**:
```python
h_task : torch.Tensor
    Task representations from deposit layer, shape (B, D)
```

**ä¿®å¤å**:
```python
h_task : torch.Tensor
    Tanh-activated latent representations, shape (B, D)
```

---

### 4. æ·»åŠ  `self.deposit_dim` æ³¨é‡Šè¯´æ˜

**æ–‡ä»¶**: `src/foundation_model/models/flexible_multi_task_model.py`

**ä½ç½®**: Lines 134-136

**æ·»åŠ **:
```python
# Note: deposit_dim retained for backward compatibility, equals latent_dim in simplified architecture
# Task heads receive Tanh(latent) with dimension = latent_dim
self.deposit_dim = self.encoder_config.latent_dim
```

**åŸå› **:
- å˜é‡å `deposit_dim` å¯èƒ½å¼•èµ·æ··æ·†
- ä½†ä¸ºäº†å‘åå…¼å®¹æ€§ä¿ç•™ï¼ˆå¯èƒ½æœ‰å¤–éƒ¨ä»£ç å¼•ç”¨ï¼‰
- æ·»åŠ æ³¨é‡Šæ˜ç¡®è¯´æ˜å…¶å«ä¹‰

---

## ğŸ“Š æ¶æ„å¯¹æ¯”

### æ—§æ¶æ„ï¼ˆæœ‰ deposit layerï¼‰
```
X â†’ encoder.shared â†’ latent â†’ encoder.deposit(Linear + Tanh) â†’ task_heads
                                       â†‘
                                 å¯å­¦ä¹ çš„ Linear å˜æ¢
```

### æ–°æ¶æ„ï¼ˆç®€åŒ–ï¼‰
```
X â†’ encoder.shared â†’ latent â†’ torch.tanh() â†’ task_heads
                                  â†‘
                    åœ¨ FlexibleMultiTaskModel.forward() ä¸­ç»Ÿä¸€åº”ç”¨
```

---

## ğŸ” éªŒè¯æ¸…å•

- [x] ç§»é™¤ä»£ç ä¸­å¯¹ `encoder.deposit` çš„å¼•ç”¨
- [x] æ›´æ–° `_TransformerBackbone` æ–‡æ¡£å­—ç¬¦ä¸²
- [x] æ›´æ–° `FlexibleMultiTaskModel` æ–‡æ¡£å­—ç¬¦ä¸²
- [x] æ·»åŠ  `deposit_dim` æ³¨é‡Šè¯´æ˜
- [x] éªŒè¯æ²¡æœ‰æ®‹ç•™çš„ "deposit layer" å¼•ç”¨

---

## ğŸ“ å…³é”®æ”¶è·

### 1. ä¸ºä»€ä¹ˆç®€åŒ–æ¶æ„æ€§èƒ½æ›´å¥½ï¼Ÿ

**è§‚å¯Ÿ**: ä¼˜åŒ–åˆ†æ•°ä» 2.5 æå‡åˆ° 5.0ï¼ˆ2å€æå‡ï¼‰

**åŸå› **:
1. **æ›´å¼ºçš„æ¢¯åº¦æµ**: ç§»é™¤ deposit Linear å±‚ï¼Œæ¢¯åº¦ç›´æ¥é€šè¿‡ Tanh åå‘ä¼ æ’­
2. **æ›´è‡ªç”±çš„ä¼˜åŒ–ç©ºé—´**: æ—  Linear å˜æ¢çš„çº¦æŸ
3. **æ›´å…‰æ»‘çš„ä¼˜åŒ–æ›²çº¿**: Tanh å‡½æ•°æœ¬èº«æ˜¯å¹³æ»‘å¯å¯¼çš„

### 2. è¿™æ˜¯ bug è¿˜æ˜¯é¢„æœŸè¡Œä¸ºï¼Ÿ

**ç»“è®º**: **é¢„æœŸè¡Œä¸ºï¼Œæ˜¯æœ‰æ„çš„è®¾è®¡æ”¹è¿›**

ä¸¤ç§æ¶æ„çš„å¯¹æ¯”ï¼š

| ç»´åº¦ | æ—§æ¶æ„ï¼ˆæœ‰ deposit Linearï¼‰ | æ–°æ¶æ„ï¼ˆæ—  deposit Linearï¼‰ |
|------|---------------------------|---------------------------|
| **æ¢¯åº¦æµ** | é€šè¿‡ Linear å±‚è¡°å‡ | ç›´æ¥ä¼ æ’­ |
| **å‚æ•°æ•°é‡** | æ›´å¤šï¼ˆLinear å±‚ï¼‰ | æ›´å°‘ |
| **ä¼˜åŒ–éš¾åº¦** | å— Linear çº¦æŸ | æ›´è‡ªç”± |
| **æ€§èƒ½** | 2.5 | 5.0 |
| **æ›²çº¿å…‰æ»‘åº¦** | å¯èƒ½ä¸è¿ç»­ | å…‰æ»‘ |

### 3. å‘åå…¼å®¹æ€§

**ä¿ç•™çš„åç§°**:
- `self.deposit_dim`: ä¿ç•™å˜é‡åä½†æ·»åŠ æ³¨é‡Šè¯´æ˜

**ç§»é™¤çš„åŠŸèƒ½**:
- `encoder.deposit`: å®Œå…¨ç§»é™¤ï¼Œä»£ç å¼•ç”¨å·²æ¸…ç†

---

## ğŸ“ ç›¸å…³æ–‡æ¡£

- [UNIFIED_TANH_ARCHITECTURE.md](UNIFIED_TANH_ARCHITECTURE.md) - ç»Ÿä¸€ Tanh æ¶æ„è¯´æ˜
- [FIX_SUMMARY.md](FIX_SUMMARY.md) - Latent ä¼˜åŒ–ä¿®å¤æ€»ç»“
- [verify_current_architecture.py](verify_current_architecture.py) - æ¶æ„éªŒè¯è„šæœ¬

---

**æ—¥æœŸ**: 2025-11-25
**ä¿®å¤äºº**: Claude Code Assistant
