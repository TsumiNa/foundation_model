# ç»Ÿä¸€ Tanh æ¶æ„ - æœ€ç»ˆæ–¹æ¡ˆ

## ğŸ¯ æ ¸å¿ƒæ¶æ„

```
X â†’ encoder â†’ latent â†’ Tanh â†’ (æ‰€æœ‰ task headsï¼ŒåŒ…æ‹¬ AE)
```

**å…³é”®åŸåˆ™**ï¼š**æ‰€æœ‰ task headsï¼ˆåŒ…æ‹¬ AEï¼‰éƒ½æ¥æ”¶ `torch.tanh(latent)`**

## âŒ ä¹‹å‰æ–¹æ¡ˆçš„é—®é¢˜

### é—®é¢˜ 1ï¼šåŒ Tanh é—®é¢˜ï¼ˆç”¨æˆ·å‘ç°ï¼‰

å¦‚æœåœ¨ latent ä¸Šç›´æ¥å¥— `torch.tanh()`ï¼š
- è®­ç»ƒæ—¶ AE çœ‹åˆ°ï¼š`latent â†’ deposit(Linear + Tanh) â†’ task_repr`
- ä¼˜åŒ–æ—¶ä½¿ç”¨ï¼š`latent â†’ torch.tanh() â†’ task_head`ï¼ˆç¬¬äºŒæ¬¡ Tanhï¼ï¼‰
- ç»“æœï¼šä¼˜åŒ–å‡ºçš„ latent ä¸ AE è®­ç»ƒæ—¶çš„åˆ†å¸ƒä¸ä¸€è‡´

### é—®é¢˜ 2ï¼šdeposit layer çš„ Linear å±‚ï¼ˆç”¨æˆ·å‘ç°ï¼‰

å½“ `use_deposit_layer=True` æ—¶ï¼š
```python
self.deposit = nn.Sequential(
    nn.Linear(latent_dim, deposit_dim),  # â† å¯å­¦ä¹ å‚æ•°
    nn.Tanh(),
)
```

ä¹‹å‰çš„ä¿®å¤å°è¯•ï¼š
- ä¼˜åŒ– `initial_latent`ï¼ˆshared çš„è¾“å‡ºï¼‰
- é€šè¿‡ `encoder.deposit(optim_latent)` åº”ç”¨ Linear + Tanh
- **é—®é¢˜**ï¼šæ¢¯åº¦ä¼šå½±å“ `optim_latent` çš„åˆ†å¸ƒï¼Œä½¿å…¶åç¦» AE è®­ç»ƒæ—¶çš„åˆ†å¸ƒ
- AE é‡æ„æ—¶ï¼š`optim_latent â†’ AE_head`ï¼Œä½†è¿™ä¸ª latent å·²ç»è¢« deposit çš„ Linear å½±å“äº†ï¼

## âœ… æœ€ç»ˆæ­£ç¡®æ–¹æ¡ˆ

### æ¶æ„æ”¹è¿›

**å°† Tanh ç§»åˆ° FlexibleMultiTaskModel å±‚é¢ç»Ÿä¸€ç®¡ç†**

ä¿®æ”¹ [`flexible_multi_task_model.py:568-572`](src/foundation_model/models/flexible_multi_task_model.py#L568-L572)ï¼š

```python
def forward(self, x, t_sequences=None):
    # Get latent representation from encoder
    latent, _ = self.encoder(x)

    # Apply Tanh activation - ALL task heads (including AE) receive Tanh(latent)
    # This ensures architectural consistency between training and optimization
    h_task = torch.tanh(latent)

    # Apply task heads - all task heads use h_task
    outputs = {}
    for name, head in self.task_heads.items():
        outputs[name] = head(h_task)

    return outputs
```

### ä¼˜åŒ–æ–¹æ³•ä¸­çš„åº”ç”¨

ä¿®æ”¹ [`flexible_multi_task_model.py:1777-1856`](src/foundation_model/models/flexible_multi_task_model.py#L1777-L1856)ï¼š

```python
# Latent space optimization
initial_latent, _ = self.encoder(input_tensor)
optim_latent = initial_latent.clone().detach().requires_grad_(True)

for step in range(steps):
    optimizer.zero_grad()

    # Apply Tanh to get task representation (consistent with forward())
    h_task = torch.tanh(optim_latent)

    # Forward through task heads using h_task
    per_task_values = []
    for name in tasks_for_optimization:
        pred = self.task_heads[name](h_task)
        per_task_values.append(_reduce_pred(pred))

    # Compute loss and optimize
    aggregate = torch.stack(per_task_values, dim=-1).mean(dim=-1)
    loss = -sign * aggregate.mean()
    loss.backward()
    optimizer.step()

# Reconstruction via AE
with torch.no_grad():
    final_h_task = torch.tanh(optim_latent)
    # AE also receives Tanh(latent) for consistency with training
    reconstructed_input = self.task_heads[ae_task_name](final_h_task)
```

## ğŸ“Š éªŒè¯ç»“æœ

è¿è¡Œ [`test_unified_tanh.py`](test_unified_tanh.py) çš„ç»“æœï¼š

```
âœ“ Tanh applied correctly in forward()
âœ“ Tanh bounds respected (max |Tanh(latent)| = 1.0)
âœ“ Perfect architectural consistency
  - Training path and optimization path produce identical results
âœ“ AE reconstruction works (error: 0.399)
âœ“ All task heads (including AE) receive Tanh(latent)
```

### å…³é”®æŒ‡æ ‡

| æ£€æŸ¥é¡¹ | ç»“æœ |
|--------|------|
| Tanh åº”ç”¨æ­£ç¡® | âœ“ |
| Tanh è¾¹ç•Œ [âˆ’1, 1] | âœ“ |
| è®­ç»ƒè·¯å¾„ = ä¼˜åŒ–è·¯å¾„ | âœ“ |
| AE é‡æ„å¯ç”¨ | âœ“ |
| æ¶æ„ä¸€è‡´æ€§ | âœ“ |

## ğŸ” æ¶æ„å¯¹æ¯”

### è®­ç»ƒæ—¶

```
X â†’ encoder â†’ latent â†’ torch.tanh() â†’ h_task â†’ task_heads
                                          â†“
                                     (density, AE, ...)
```

### è¾“å…¥ç©ºé—´ä¼˜åŒ–

```
optim_X â†’ encoder â†’ latent â†’ torch.tanh() â†’ h_task â†’ task_heads
```

### æ½œåœ¨ç©ºé—´ä¼˜åŒ–

```
optim_latent â†’ torch.tanh() â†’ h_task â†’ task_heads
         â†“                         â†“
     (ä¼˜åŒ–å˜é‡)                 (æ‰€æœ‰headsï¼ŒåŒ…æ‹¬AE)
```

**å®Œå…¨ä¸€è‡´ï¼** æ‰€æœ‰è·¯å¾„éƒ½ç»è¿‡ç›¸åŒçš„ `torch.tanh()` æ¿€æ´»ã€‚

## ğŸ’¡ å…³é”®æ”¶è·

### 1. ç”¨æˆ·çš„ä¸¤ä¸ªå…³é”®æ´å¯Ÿ

#### æ´å¯Ÿ 1ï¼š"åŒ Tanh"é—®é¢˜
ç›´æ¥åœ¨ latent ä¸Šå¥— `torch.tanh()` ä¼šå¯¼è‡´ï¼š
- è®­ç»ƒæ—¶ï¼šdeposit layer çš„ Tanh
- ä¼˜åŒ–æ—¶ï¼šæ‰‹åŠ¨çš„ Tanhï¼ˆç¬¬äºŒæ¬¡ï¼ï¼‰
- ç»“æœï¼šAE æ— æ³•æ­£ç¡®é‡æ„

#### æ´å¯Ÿ 2ï¼šdeposit layer çš„ Linear å±‚é—®é¢˜
`use_deposit_layer=True` æ—¶çš„ Linear å±‚ä¼šï¼š
- å¼•å…¥å¯å­¦ä¹ å‚æ•°
- ä¼˜åŒ–æ—¶ä¼šå½±å“ latent çš„åˆ†å¸ƒ
- å¯¼è‡´ä¸ AE è®­ç»ƒæ—¶çš„åˆ†å¸ƒä¸ä¸€è‡´

### 2. æ­£ç¡®çš„æ¶æ„åŸåˆ™

**Tanh åº”è¯¥åœ¨æ¨¡å‹å±‚é¢ç»Ÿä¸€ç®¡ç†ï¼Œè€Œä¸æ˜¯åœ¨ encoder å†…éƒ¨**

åŸå› ï¼š
1. **æ¶æ„æ¸…æ™°**ï¼šæ‰€æœ‰ task heads çš„è¾“å…¥çº¦æŸåœ¨ä¸€ä¸ªåœ°æ–¹æ§åˆ¶
2. **ä¼˜åŒ–ä¸€è‡´**ï¼šè®­ç»ƒå’Œä¼˜åŒ–ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è·¯å¾„
3. **æ— é¢å¤–å‚æ•°**ï¼š`torch.tanh()` åªæ˜¯æ¿€æ´»å‡½æ•°ï¼Œæ²¡æœ‰å¯å­¦ä¹ å‚æ•°
4. **AE å…¼å®¹**ï¼šAE åœ¨è®­ç»ƒå’Œé‡æ„æ—¶éƒ½çœ‹åˆ°ç›¸åŒçš„ `Tanh(latent)`

### 3. deposit layer çš„è§’è‰²

å½“å‰ deposit layer ä»åŒ…å« `Linear + Tanh`ï¼Œä½†ï¼š
- æˆ‘ä»¬åœ¨ `FlexibleMultiTaskModel.forward()` ä¸­**å¿½ç•¥** deposit layer çš„è¾“å‡º
- ç›´æ¥ä½¿ç”¨ `torch.tanh(latent)`
- **æœªæ¥å¯ä»¥è€ƒè™‘ç®€åŒ– deposit layer**ï¼Œç§»é™¤ Linear å±‚

### 4. ä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§

ç”±äºæˆ‘ä»¬åœ¨ `FlexibleMultiTaskModel` ä¸­é‡æ–°åº”ç”¨ Tanhï¼š
- ä¸éœ€è¦ä¿®æ”¹ `FoundationEncoder` çš„ç°æœ‰é€»è¾‘
- ä¸ä¼šç ´åç°æœ‰æ¨¡å‹çš„åŠ è½½
- å‘åå…¼å®¹

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### Tanh çš„ä½œç”¨

```python
h_task = torch.tanh(latent)
```

- **è¾“å…¥**ï¼šlatent âˆˆ â„^dï¼ˆæ— ç•Œï¼‰
- **è¾“å‡º**ï¼šh_task âˆˆ [-1, 1]^dï¼ˆæœ‰ç•Œï¼‰
- **æ¢¯åº¦**ï¼šå¹³æ»‘ï¼Œå¯ä»¥åå‘ä¼ æ’­åˆ° latent

### ä¸ºä»€ä¹ˆæ‰€æœ‰ task headsï¼ˆåŒ…æ‹¬ AEï¼‰éƒ½éœ€è¦ Tanhï¼Ÿ

1. **ä¸€è‡´æ€§**ï¼šæ‰€æœ‰ heads çœ‹åˆ°ç›¸åŒçš„è¾“å…¥åˆ†å¸ƒ
2. **çº¦æŸæ€§**ï¼šé˜²æ­¢ latent space optimization æ— ç•Œå¢é•¿
3. **è®­ç»ƒæ•ˆæœ**ï¼šTanh æä¾›éçº¿æ€§å’Œå½’ä¸€åŒ–æ•ˆæœ

### AE å­¦ä¹ ä»€ä¹ˆï¼Ÿ

è®­ç»ƒæ—¶ï¼š
```
X â†’ encoder â†’ latent â†’ Tanh â†’ AE_head â†’ reconstructed_X
```

AE å­¦ä¹ çš„æ˜ å°„ï¼š
```
Tanh(latent) â†’ X
```

ä¼˜åŒ–æ—¶é‡æ„ï¼š
```
optim_latent â†’ Tanh â†’ AE_head â†’ reconstructed_X
```

ä½¿ç”¨çš„ä¹Ÿæ˜¯ï¼š
```
Tanh(optim_latent) â†’ X
```

**å®Œå…¨ä¸€è‡´ï¼**

## ğŸ“ ç›¸å…³ä¿®æ”¹

### æ ¸å¿ƒæ–‡ä»¶

1. [`flexible_multi_task_model.py:568-572`](src/foundation_model/models/flexible_multi_task_model.py#L568-L572)
   - `forward()` æ–¹æ³•ä¸­ç»Ÿä¸€åº”ç”¨ Tanh

2. [`flexible_multi_task_model.py:1777-1856`](src/foundation_model/models/flexible_multi_task_model.py#L1777-L1856)
   - `optimize_latent()` ä¸­ latent space optimization éƒ¨åˆ†

### æµ‹è¯•æ–‡ä»¶

- [`test_unified_tanh.py`](test_unified_tanh.py) - ç»Ÿä¸€æ¶æ„éªŒè¯

### æ–‡æ¡£

- æœ¬æ–‡æ¡£ - ç»Ÿä¸€ Tanh æ¶æ„è¯´æ˜

## ğŸ“ æœ€ä½³å®è·µ

### åˆ›å»ºæ–°æ¨¡å‹æ—¶

æ¨èé…ç½®ï¼š
```python
encoder_config = MLPEncoderConfig(
    hidden_dims=[input_dim, hidden, latent_dim],
    norm=True,
    use_deposit_layer=True,  # å¯ä»¥ä¿ç•™ï¼Œä½†ä¼šè¢« forward() ä¸­çš„ Tanh è¦†ç›–
)
```

### ä½¿ç”¨ optimize_latent æ—¶

```python
# Input space optimizationï¼ˆæ¨èï¼‰
result = model.optimize_latent(
    task_name="your_task",
    initial_input=X_seed,
    mode="max",
    steps=200,
    optimize_space="input",
)

# Latent space optimizationï¼ˆéœ€è¦ AEï¼‰
result = model.optimize_latent(
    task_name="your_task",
    initial_input=X_seed,
    mode="max",
    steps=200,
    ae_task_name="reconstruction",
    optimize_space="latent",
)
```

ä¸¤ç§æ–¹æ³•ç°åœ¨éƒ½ï¼š
- âœ… æ¶æ„ä¸€è‡´
- âœ… è‡ªåŠ¨åº”ç”¨ Tanh çº¦æŸ
- âœ… AE å…¼å®¹
- âœ… ä¸éœ€è¦äººå·¥æ·»åŠ é¢å¤–çº¦æŸ

## âœ… ç»“è®º

é€šè¿‡å°† Tanh ç§»åˆ° `FlexibleMultiTaskModel` å±‚é¢ç»Ÿä¸€ç®¡ç†ï¼š

1. âœ… è§£å†³äº†"åŒ Tanh"é—®é¢˜
2. âœ… è§£å†³äº† deposit layer Linear å±‚çš„å‚æ•°å¹²æ‰°é—®é¢˜
3. âœ… æ‰€æœ‰ task headsï¼ˆåŒ…æ‹¬ AEï¼‰éƒ½æ¥æ”¶ä¸€è‡´çš„è¾“å…¥
4. âœ… è®­ç»ƒå’Œä¼˜åŒ–è·¯å¾„å®Œå…¨ä¸€è‡´
5. âœ… æ— éœ€ä»»ä½•é¢å¤–çº¦æŸæˆ–è¶…å‚æ•°

**è¿™æ˜¯æœ€ç®€æ´ã€æœ€ç¬¦åˆæ¶æ„è®¾è®¡åŸåˆ™çš„è§£å†³æ–¹æ¡ˆï¼**

---

**ç‰¹åˆ«æ„Ÿè°¢ç”¨æˆ·çš„ä¸¤ä¸ªå…³é”®æ´å¯Ÿï¼Œå¼•å¯¼æ‰¾åˆ°äº†æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆï¼** ğŸ™
